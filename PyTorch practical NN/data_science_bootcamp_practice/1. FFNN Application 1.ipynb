{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66cd85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3324ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"C:/Users/Mi/jupyter_lab_projs/The-Complete-Neural-Networks-Bootcamp-Theory-Applications-master/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ce7102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>Plasma glucose concentration</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Triceps skin fold thickness</th>\n",
       "      <th>2-Hour serum insulin</th>\n",
       "      <th>Body mass index</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>50</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>31</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>32</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>21</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>33</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>63</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>27</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>30</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>47</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>23</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Number of times pregnant  Plasma glucose concentration  \\\n",
       "0                           6                           148   \n",
       "1                           1                            85   \n",
       "2                           8                           183   \n",
       "3                           1                            89   \n",
       "4                           0                           137   \n",
       "..                        ...                           ...   \n",
       "763                        10                           101   \n",
       "764                         2                           122   \n",
       "765                         5                           121   \n",
       "766                         1                           126   \n",
       "767                         1                            93   \n",
       "\n",
       "     Diastolic blood pressure  Triceps skin fold thickness  \\\n",
       "0                          72                           35   \n",
       "1                          66                           29   \n",
       "2                          64                            0   \n",
       "3                          66                           23   \n",
       "4                          40                           35   \n",
       "..                        ...                          ...   \n",
       "763                        76                           48   \n",
       "764                        70                           27   \n",
       "765                        72                           23   \n",
       "766                        60                            0   \n",
       "767                        70                           31   \n",
       "\n",
       "     2-Hour serum insulin  Body mass index  Age     Class  \n",
       "0                       0             33.6   50  positive  \n",
       "1                       0             26.6   31  negative  \n",
       "2                       0             23.3   32  positive  \n",
       "3                      94             28.1   21  negative  \n",
       "4                     168             43.1   33  positive  \n",
       "..                    ...              ...  ...       ...  \n",
       "763                   180             32.9   63  negative  \n",
       "764                     0             36.8   27  negative  \n",
       "765                   112             26.2   30  negative  \n",
       "766                     0             30.1   47  positive  \n",
       "767                     0             30.4   23  negative  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41153e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x: Extract out the dataset from all the rows (all samples and all columns except last column (last featrue))\n",
    "# For y: Extract out the last column (which is the label)\n",
    "# Convert both to numpy using the .values method\n",
    "x=data.iloc[:,0:-1].values\n",
    "y_string=list(data.iloc[:,-1])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5945ddd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250406a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our NN only understand numbers so convert string to labels \n",
    "y_int=[]\n",
    "for string in y_string:\n",
    "    if string== 'positive':\n",
    "        y_int.append(1)\n",
    "    else:\n",
    "        y_int.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39733366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dec51a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now convert to an array\n",
    "y = np.array(y_int,dtype='float64')\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95967bc",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faeecddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6. , 148. ,  72. , ...,   0. ,  33.6,  50. ],\n",
       "       [  1. ,  85. ,  66. , ...,   0. ,  26.6,  31. ],\n",
       "       [  8. , 183. ,  64. , ...,   0. ,  23.3,  32. ],\n",
       "       ...,\n",
       "       [  5. , 121. ,  72. , ..., 112. ,  26.2,  30. ],\n",
       "       [  1. , 126. ,  60. , ...,   0. ,  30.1,  47. ],\n",
       "       [  1. ,  93. ,  70. , ...,   0. ,  30.4,  23. ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba04d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Normalization. All features should have the same range of values(-1,1)\n",
    "sc=StandardScaler()\n",
    "x=sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9e56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(x)\n",
    "y=torch.tensor(y).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17ef95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 7])\n",
      "torch.Size([768, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba12342",
   "metadata": {},
   "source": [
    "# Creating and Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9744bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aaaeba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Dataset(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53c85fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2814c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to your dataloader for batch processing and shuffling\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                           batch_size= 32,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00650dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x19e0b157a60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8828c95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 24 batches in the dataset\n",
      "For one iteration (batch),there is :\n",
      "Data: torch.Size([32, 7])\n",
      "Labels: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "#lets have a look at the data Loader\n",
    "print('There is {} batches in the dataset'.format(len(train_loader)))\n",
    "for (x,y) in train_loader:\n",
    "    print('For one iteration (batch),there is :')\n",
    "    print(\"Data: {}\".format(x.shape))\n",
    "    print(\"Labels: {}\".format(y.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483e20a",
   "metadata": {},
   "source": [
    "# Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29b483a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not lets build the above network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,input_features,output_features):\n",
    "        super(Model,self).__init__()\n",
    "        self.fc1= nn.Linear(input_features,5)\n",
    "        self.fc2= nn.Linear(5,4)\n",
    "        self.fc3= nn.Linear(4,3)\n",
    "        self.fc4= nn.Linear(3,output_features)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.tanh=nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.fc1(x)\n",
    "        out= self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf5a4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the netrowk (an object of the Net class)\n",
    "net=Model(7,1)\n",
    "#in Binary Cross Entropy (EBP) : the input and output should have the same shape\n",
    "# size_average = True --> the losses are averaged over observations for each minibatch\n",
    "criterion=torch.nn.BCELoss(size_average =True)\n",
    "# We will use SGD with momentum with a learning rate of 0.1\n",
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf22de7",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "205aa39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.754, Accuracy:0.250\n",
      "Epoch 2/500, Loss: 0.718, Accuracy:0.375\n",
      "Epoch 3/500, Loss: 0.723, Accuracy:0.344\n",
      "Epoch 4/500, Loss: 0.690, Accuracy:0.500\n",
      "Epoch 5/500, Loss: 0.665, Accuracy:0.812\n",
      "Epoch 6/500, Loss: 0.679, Accuracy:0.719\n",
      "Epoch 7/500, Loss: 0.654, Accuracy:0.750\n",
      "Epoch 8/500, Loss: 0.684, Accuracy:0.594\n",
      "Epoch 9/500, Loss: 0.661, Accuracy:0.688\n",
      "Epoch 10/500, Loss: 0.619, Accuracy:0.781\n",
      "Epoch 11/500, Loss: 0.637, Accuracy:0.688\n",
      "Epoch 12/500, Loss: 0.713, Accuracy:0.500\n",
      "Epoch 13/500, Loss: 0.600, Accuracy:0.688\n",
      "Epoch 14/500, Loss: 0.629, Accuracy:0.656\n",
      "Epoch 15/500, Loss: 0.609, Accuracy:0.719\n",
      "Epoch 16/500, Loss: 0.519, Accuracy:0.844\n",
      "Epoch 17/500, Loss: 0.591, Accuracy:0.719\n",
      "Epoch 18/500, Loss: 0.634, Accuracy:0.656\n",
      "Epoch 19/500, Loss: 0.589, Accuracy:0.750\n",
      "Epoch 20/500, Loss: 0.626, Accuracy:0.625\n",
      "Epoch 21/500, Loss: 0.610, Accuracy:0.625\n",
      "Epoch 22/500, Loss: 0.562, Accuracy:0.812\n",
      "Epoch 23/500, Loss: 0.580, Accuracy:0.688\n",
      "Epoch 24/500, Loss: 0.533, Accuracy:0.781\n",
      "Epoch 25/500, Loss: 0.538, Accuracy:0.750\n",
      "Epoch 26/500, Loss: 0.556, Accuracy:0.719\n",
      "Epoch 27/500, Loss: 0.506, Accuracy:0.844\n",
      "Epoch 28/500, Loss: 0.616, Accuracy:0.656\n",
      "Epoch 29/500, Loss: 0.565, Accuracy:0.719\n",
      "Epoch 30/500, Loss: 0.560, Accuracy:0.750\n",
      "Epoch 31/500, Loss: 0.531, Accuracy:0.719\n",
      "Epoch 32/500, Loss: 0.485, Accuracy:0.812\n",
      "Epoch 33/500, Loss: 0.492, Accuracy:0.781\n",
      "Epoch 34/500, Loss: 0.454, Accuracy:0.844\n",
      "Epoch 35/500, Loss: 0.559, Accuracy:0.688\n",
      "Epoch 36/500, Loss: 0.461, Accuracy:0.844\n",
      "Epoch 37/500, Loss: 0.578, Accuracy:0.719\n",
      "Epoch 38/500, Loss: 0.481, Accuracy:0.844\n",
      "Epoch 39/500, Loss: 0.564, Accuracy:0.688\n",
      "Epoch 40/500, Loss: 0.454, Accuracy:0.844\n",
      "Epoch 41/500, Loss: 0.535, Accuracy:0.750\n",
      "Epoch 42/500, Loss: 0.433, Accuracy:0.875\n",
      "Epoch 43/500, Loss: 0.462, Accuracy:0.844\n",
      "Epoch 44/500, Loss: 0.629, Accuracy:0.656\n",
      "Epoch 45/500, Loss: 0.491, Accuracy:0.750\n",
      "Epoch 46/500, Loss: 0.437, Accuracy:0.844\n",
      "Epoch 47/500, Loss: 0.450, Accuracy:0.875\n",
      "Epoch 48/500, Loss: 0.396, Accuracy:0.844\n",
      "Epoch 49/500, Loss: 0.520, Accuracy:0.719\n",
      "Epoch 50/500, Loss: 0.544, Accuracy:0.719\n",
      "Epoch 51/500, Loss: 0.444, Accuracy:0.875\n",
      "Epoch 52/500, Loss: 0.555, Accuracy:0.719\n",
      "Epoch 53/500, Loss: 0.592, Accuracy:0.719\n",
      "Epoch 54/500, Loss: 0.534, Accuracy:0.750\n",
      "Epoch 55/500, Loss: 0.497, Accuracy:0.750\n",
      "Epoch 56/500, Loss: 0.628, Accuracy:0.688\n",
      "Epoch 57/500, Loss: 0.542, Accuracy:0.750\n",
      "Epoch 58/500, Loss: 0.507, Accuracy:0.812\n",
      "Epoch 59/500, Loss: 0.435, Accuracy:0.844\n",
      "Epoch 60/500, Loss: 0.675, Accuracy:0.625\n",
      "Epoch 61/500, Loss: 0.574, Accuracy:0.750\n",
      "Epoch 62/500, Loss: 0.444, Accuracy:0.812\n",
      "Epoch 63/500, Loss: 0.510, Accuracy:0.688\n",
      "Epoch 64/500, Loss: 0.381, Accuracy:0.875\n",
      "Epoch 65/500, Loss: 0.425, Accuracy:0.844\n",
      "Epoch 66/500, Loss: 0.509, Accuracy:0.812\n",
      "Epoch 67/500, Loss: 0.631, Accuracy:0.656\n",
      "Epoch 68/500, Loss: 0.509, Accuracy:0.688\n",
      "Epoch 69/500, Loss: 0.464, Accuracy:0.844\n",
      "Epoch 70/500, Loss: 0.463, Accuracy:0.812\n",
      "Epoch 71/500, Loss: 0.401, Accuracy:0.844\n",
      "Epoch 72/500, Loss: 0.545, Accuracy:0.719\n",
      "Epoch 73/500, Loss: 0.387, Accuracy:0.875\n",
      "Epoch 74/500, Loss: 0.430, Accuracy:0.844\n",
      "Epoch 75/500, Loss: 0.430, Accuracy:0.844\n",
      "Epoch 76/500, Loss: 0.620, Accuracy:0.656\n",
      "Epoch 77/500, Loss: 0.544, Accuracy:0.719\n",
      "Epoch 78/500, Loss: 0.470, Accuracy:0.812\n",
      "Epoch 79/500, Loss: 0.551, Accuracy:0.719\n",
      "Epoch 80/500, Loss: 0.440, Accuracy:0.781\n",
      "Epoch 81/500, Loss: 0.430, Accuracy:0.844\n",
      "Epoch 82/500, Loss: 0.462, Accuracy:0.781\n",
      "Epoch 83/500, Loss: 0.719, Accuracy:0.625\n",
      "Epoch 84/500, Loss: 0.458, Accuracy:0.781\n",
      "Epoch 85/500, Loss: 0.573, Accuracy:0.688\n",
      "Epoch 86/500, Loss: 0.505, Accuracy:0.719\n",
      "Epoch 87/500, Loss: 0.280, Accuracy:0.938\n",
      "Epoch 88/500, Loss: 0.472, Accuracy:0.750\n",
      "Epoch 89/500, Loss: 0.341, Accuracy:0.906\n",
      "Epoch 90/500, Loss: 0.583, Accuracy:0.625\n",
      "Epoch 91/500, Loss: 0.561, Accuracy:0.688\n",
      "Epoch 92/500, Loss: 0.584, Accuracy:0.688\n",
      "Epoch 93/500, Loss: 0.480, Accuracy:0.719\n",
      "Epoch 94/500, Loss: 0.514, Accuracy:0.750\n",
      "Epoch 95/500, Loss: 0.540, Accuracy:0.750\n",
      "Epoch 96/500, Loss: 0.433, Accuracy:0.781\n",
      "Epoch 97/500, Loss: 0.703, Accuracy:0.656\n",
      "Epoch 98/500, Loss: 0.314, Accuracy:0.938\n",
      "Epoch 99/500, Loss: 0.449, Accuracy:0.844\n",
      "Epoch 100/500, Loss: 0.569, Accuracy:0.688\n",
      "Epoch 101/500, Loss: 0.695, Accuracy:0.625\n",
      "Epoch 102/500, Loss: 0.511, Accuracy:0.719\n",
      "Epoch 103/500, Loss: 0.488, Accuracy:0.719\n",
      "Epoch 104/500, Loss: 0.410, Accuracy:0.812\n",
      "Epoch 105/500, Loss: 0.477, Accuracy:0.750\n",
      "Epoch 106/500, Loss: 0.432, Accuracy:0.781\n",
      "Epoch 107/500, Loss: 0.576, Accuracy:0.656\n",
      "Epoch 108/500, Loss: 0.413, Accuracy:0.812\n",
      "Epoch 109/500, Loss: 0.590, Accuracy:0.656\n",
      "Epoch 110/500, Loss: 0.424, Accuracy:0.812\n",
      "Epoch 111/500, Loss: 0.488, Accuracy:0.750\n",
      "Epoch 112/500, Loss: 0.409, Accuracy:0.812\n",
      "Epoch 113/500, Loss: 0.453, Accuracy:0.781\n",
      "Epoch 114/500, Loss: 0.564, Accuracy:0.656\n",
      "Epoch 115/500, Loss: 0.618, Accuracy:0.719\n",
      "Epoch 116/500, Loss: 0.492, Accuracy:0.750\n",
      "Epoch 117/500, Loss: 0.389, Accuracy:0.844\n",
      "Epoch 118/500, Loss: 0.539, Accuracy:0.750\n",
      "Epoch 119/500, Loss: 0.374, Accuracy:0.875\n",
      "Epoch 120/500, Loss: 0.430, Accuracy:0.812\n",
      "Epoch 121/500, Loss: 0.484, Accuracy:0.781\n",
      "Epoch 122/500, Loss: 0.475, Accuracy:0.812\n",
      "Epoch 123/500, Loss: 0.553, Accuracy:0.688\n",
      "Epoch 124/500, Loss: 0.520, Accuracy:0.688\n",
      "Epoch 125/500, Loss: 0.429, Accuracy:0.812\n",
      "Epoch 126/500, Loss: 0.407, Accuracy:0.812\n",
      "Epoch 127/500, Loss: 0.373, Accuracy:0.875\n",
      "Epoch 128/500, Loss: 0.445, Accuracy:0.750\n",
      "Epoch 129/500, Loss: 0.485, Accuracy:0.781\n",
      "Epoch 130/500, Loss: 0.535, Accuracy:0.656\n",
      "Epoch 131/500, Loss: 0.549, Accuracy:0.719\n",
      "Epoch 132/500, Loss: 0.481, Accuracy:0.719\n",
      "Epoch 133/500, Loss: 0.508, Accuracy:0.812\n",
      "Epoch 134/500, Loss: 0.508, Accuracy:0.750\n",
      "Epoch 135/500, Loss: 0.375, Accuracy:0.844\n",
      "Epoch 136/500, Loss: 0.620, Accuracy:0.625\n",
      "Epoch 137/500, Loss: 0.587, Accuracy:0.656\n",
      "Epoch 138/500, Loss: 0.456, Accuracy:0.750\n",
      "Epoch 139/500, Loss: 0.527, Accuracy:0.781\n",
      "Epoch 140/500, Loss: 0.539, Accuracy:0.750\n",
      "Epoch 141/500, Loss: 0.406, Accuracy:0.812\n",
      "Epoch 142/500, Loss: 0.412, Accuracy:0.812\n",
      "Epoch 143/500, Loss: 0.592, Accuracy:0.656\n",
      "Epoch 144/500, Loss: 0.626, Accuracy:0.719\n",
      "Epoch 145/500, Loss: 0.417, Accuracy:0.812\n",
      "Epoch 146/500, Loss: 0.513, Accuracy:0.750\n",
      "Epoch 147/500, Loss: 0.438, Accuracy:0.812\n",
      "Epoch 148/500, Loss: 0.510, Accuracy:0.750\n",
      "Epoch 149/500, Loss: 0.559, Accuracy:0.656\n",
      "Epoch 150/500, Loss: 0.455, Accuracy:0.781\n",
      "Epoch 151/500, Loss: 0.527, Accuracy:0.750\n",
      "Epoch 152/500, Loss: 0.596, Accuracy:0.688\n",
      "Epoch 153/500, Loss: 0.609, Accuracy:0.688\n",
      "Epoch 154/500, Loss: 0.352, Accuracy:0.875\n",
      "Epoch 155/500, Loss: 0.427, Accuracy:0.781\n",
      "Epoch 156/500, Loss: 0.381, Accuracy:0.812\n",
      "Epoch 157/500, Loss: 0.456, Accuracy:0.719\n",
      "Epoch 158/500, Loss: 0.452, Accuracy:0.812\n",
      "Epoch 159/500, Loss: 0.546, Accuracy:0.719\n",
      "Epoch 160/500, Loss: 0.454, Accuracy:0.812\n",
      "Epoch 161/500, Loss: 0.461, Accuracy:0.781\n",
      "Epoch 162/500, Loss: 0.624, Accuracy:0.625\n",
      "Epoch 163/500, Loss: 0.439, Accuracy:0.750\n",
      "Epoch 164/500, Loss: 0.531, Accuracy:0.688\n",
      "Epoch 165/500, Loss: 0.511, Accuracy:0.719\n",
      "Epoch 166/500, Loss: 0.542, Accuracy:0.688\n",
      "Epoch 167/500, Loss: 0.535, Accuracy:0.688\n",
      "Epoch 168/500, Loss: 0.421, Accuracy:0.781\n",
      "Epoch 169/500, Loss: 0.300, Accuracy:0.875\n",
      "Epoch 170/500, Loss: 0.409, Accuracy:0.812\n",
      "Epoch 171/500, Loss: 0.427, Accuracy:0.750\n",
      "Epoch 172/500, Loss: 0.560, Accuracy:0.656\n",
      "Epoch 173/500, Loss: 0.449, Accuracy:0.719\n",
      "Epoch 174/500, Loss: 0.674, Accuracy:0.656\n",
      "Epoch 175/500, Loss: 0.403, Accuracy:0.844\n",
      "Epoch 176/500, Loss: 0.352, Accuracy:0.875\n",
      "Epoch 177/500, Loss: 0.471, Accuracy:0.719\n",
      "Epoch 178/500, Loss: 0.481, Accuracy:0.719\n",
      "Epoch 179/500, Loss: 0.411, Accuracy:0.875\n",
      "Epoch 180/500, Loss: 0.491, Accuracy:0.719\n",
      "Epoch 181/500, Loss: 0.481, Accuracy:0.750\n",
      "Epoch 182/500, Loss: 0.520, Accuracy:0.750\n",
      "Epoch 183/500, Loss: 0.529, Accuracy:0.812\n",
      "Epoch 184/500, Loss: 0.325, Accuracy:0.844\n",
      "Epoch 185/500, Loss: 0.512, Accuracy:0.781\n",
      "Epoch 186/500, Loss: 0.378, Accuracy:0.844\n",
      "Epoch 187/500, Loss: 0.417, Accuracy:0.812\n",
      "Epoch 188/500, Loss: 0.529, Accuracy:0.750\n",
      "Epoch 189/500, Loss: 0.578, Accuracy:0.688\n",
      "Epoch 190/500, Loss: 0.437, Accuracy:0.844\n",
      "Epoch 191/500, Loss: 0.416, Accuracy:0.781\n",
      "Epoch 192/500, Loss: 0.569, Accuracy:0.625\n",
      "Epoch 193/500, Loss: 0.521, Accuracy:0.719\n",
      "Epoch 194/500, Loss: 0.439, Accuracy:0.750\n",
      "Epoch 195/500, Loss: 0.528, Accuracy:0.719\n",
      "Epoch 196/500, Loss: 0.366, Accuracy:0.875\n",
      "Epoch 197/500, Loss: 0.357, Accuracy:0.875\n",
      "Epoch 198/500, Loss: 0.434, Accuracy:0.812\n",
      "Epoch 199/500, Loss: 0.372, Accuracy:0.844\n",
      "Epoch 200/500, Loss: 0.437, Accuracy:0.844\n",
      "Epoch 201/500, Loss: 0.407, Accuracy:0.781\n",
      "Epoch 202/500, Loss: 0.458, Accuracy:0.781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/500, Loss: 0.428, Accuracy:0.781\n",
      "Epoch 204/500, Loss: 0.522, Accuracy:0.688\n",
      "Epoch 205/500, Loss: 0.508, Accuracy:0.656\n",
      "Epoch 206/500, Loss: 0.553, Accuracy:0.688\n",
      "Epoch 207/500, Loss: 0.337, Accuracy:0.844\n",
      "Epoch 208/500, Loss: 0.358, Accuracy:0.812\n",
      "Epoch 209/500, Loss: 0.394, Accuracy:0.750\n",
      "Epoch 210/500, Loss: 0.595, Accuracy:0.781\n",
      "Epoch 211/500, Loss: 0.376, Accuracy:0.812\n",
      "Epoch 212/500, Loss: 0.446, Accuracy:0.750\n",
      "Epoch 213/500, Loss: 0.533, Accuracy:0.719\n",
      "Epoch 214/500, Loss: 0.418, Accuracy:0.844\n",
      "Epoch 215/500, Loss: 0.532, Accuracy:0.750\n",
      "Epoch 216/500, Loss: 0.494, Accuracy:0.750\n",
      "Epoch 217/500, Loss: 0.612, Accuracy:0.750\n",
      "Epoch 218/500, Loss: 0.444, Accuracy:0.750\n",
      "Epoch 219/500, Loss: 0.621, Accuracy:0.656\n",
      "Epoch 220/500, Loss: 0.396, Accuracy:0.844\n",
      "Epoch 221/500, Loss: 0.401, Accuracy:0.750\n",
      "Epoch 222/500, Loss: 0.554, Accuracy:0.688\n",
      "Epoch 223/500, Loss: 0.519, Accuracy:0.688\n",
      "Epoch 224/500, Loss: 0.431, Accuracy:0.812\n",
      "Epoch 225/500, Loss: 0.409, Accuracy:0.781\n",
      "Epoch 226/500, Loss: 0.482, Accuracy:0.781\n",
      "Epoch 227/500, Loss: 0.361, Accuracy:0.812\n",
      "Epoch 228/500, Loss: 0.429, Accuracy:0.875\n",
      "Epoch 229/500, Loss: 0.522, Accuracy:0.719\n",
      "Epoch 230/500, Loss: 0.390, Accuracy:0.750\n",
      "Epoch 231/500, Loss: 0.526, Accuracy:0.750\n",
      "Epoch 232/500, Loss: 0.515, Accuracy:0.688\n",
      "Epoch 233/500, Loss: 0.370, Accuracy:0.875\n",
      "Epoch 234/500, Loss: 0.443, Accuracy:0.781\n",
      "Epoch 235/500, Loss: 0.488, Accuracy:0.719\n",
      "Epoch 236/500, Loss: 0.463, Accuracy:0.812\n",
      "Epoch 237/500, Loss: 0.417, Accuracy:0.719\n",
      "Epoch 238/500, Loss: 0.421, Accuracy:0.781\n",
      "Epoch 239/500, Loss: 0.428, Accuracy:0.750\n",
      "Epoch 240/500, Loss: 0.601, Accuracy:0.750\n",
      "Epoch 241/500, Loss: 0.568, Accuracy:0.688\n",
      "Epoch 242/500, Loss: 0.479, Accuracy:0.781\n",
      "Epoch 243/500, Loss: 0.470, Accuracy:0.750\n",
      "Epoch 244/500, Loss: 0.545, Accuracy:0.688\n",
      "Epoch 245/500, Loss: 0.444, Accuracy:0.812\n",
      "Epoch 246/500, Loss: 0.508, Accuracy:0.719\n",
      "Epoch 247/500, Loss: 0.519, Accuracy:0.719\n",
      "Epoch 248/500, Loss: 0.358, Accuracy:0.875\n",
      "Epoch 249/500, Loss: 0.412, Accuracy:0.812\n",
      "Epoch 250/500, Loss: 0.276, Accuracy:0.938\n",
      "Epoch 251/500, Loss: 0.438, Accuracy:0.719\n",
      "Epoch 252/500, Loss: 0.482, Accuracy:0.719\n",
      "Epoch 253/500, Loss: 0.494, Accuracy:0.750\n",
      "Epoch 254/500, Loss: 0.411, Accuracy:0.844\n",
      "Epoch 255/500, Loss: 0.461, Accuracy:0.812\n",
      "Epoch 256/500, Loss: 0.466, Accuracy:0.812\n",
      "Epoch 257/500, Loss: 0.297, Accuracy:0.875\n",
      "Epoch 258/500, Loss: 0.441, Accuracy:0.812\n",
      "Epoch 259/500, Loss: 0.496, Accuracy:0.844\n",
      "Epoch 260/500, Loss: 0.394, Accuracy:0.875\n",
      "Epoch 261/500, Loss: 0.685, Accuracy:0.562\n",
      "Epoch 262/500, Loss: 0.396, Accuracy:0.812\n",
      "Epoch 263/500, Loss: 0.404, Accuracy:0.844\n",
      "Epoch 264/500, Loss: 0.280, Accuracy:0.938\n",
      "Epoch 265/500, Loss: 0.550, Accuracy:0.688\n",
      "Epoch 266/500, Loss: 0.402, Accuracy:0.844\n",
      "Epoch 267/500, Loss: 0.347, Accuracy:0.844\n",
      "Epoch 268/500, Loss: 0.566, Accuracy:0.688\n",
      "Epoch 269/500, Loss: 0.451, Accuracy:0.812\n",
      "Epoch 270/500, Loss: 0.328, Accuracy:0.844\n",
      "Epoch 271/500, Loss: 0.573, Accuracy:0.688\n",
      "Epoch 272/500, Loss: 0.510, Accuracy:0.781\n",
      "Epoch 273/500, Loss: 0.414, Accuracy:0.844\n",
      "Epoch 274/500, Loss: 0.537, Accuracy:0.750\n",
      "Epoch 275/500, Loss: 0.364, Accuracy:0.781\n",
      "Epoch 276/500, Loss: 0.474, Accuracy:0.781\n",
      "Epoch 277/500, Loss: 0.383, Accuracy:0.812\n",
      "Epoch 278/500, Loss: 0.568, Accuracy:0.719\n",
      "Epoch 279/500, Loss: 0.502, Accuracy:0.750\n",
      "Epoch 280/500, Loss: 0.368, Accuracy:0.875\n",
      "Epoch 281/500, Loss: 0.555, Accuracy:0.719\n",
      "Epoch 282/500, Loss: 0.490, Accuracy:0.750\n",
      "Epoch 283/500, Loss: 0.493, Accuracy:0.812\n",
      "Epoch 284/500, Loss: 0.415, Accuracy:0.812\n",
      "Epoch 285/500, Loss: 0.460, Accuracy:0.812\n",
      "Epoch 286/500, Loss: 0.376, Accuracy:0.781\n",
      "Epoch 287/500, Loss: 0.528, Accuracy:0.750\n",
      "Epoch 288/500, Loss: 0.343, Accuracy:0.844\n",
      "Epoch 289/500, Loss: 0.482, Accuracy:0.688\n",
      "Epoch 290/500, Loss: 0.401, Accuracy:0.875\n",
      "Epoch 291/500, Loss: 0.412, Accuracy:0.812\n",
      "Epoch 292/500, Loss: 0.608, Accuracy:0.625\n",
      "Epoch 293/500, Loss: 0.597, Accuracy:0.750\n",
      "Epoch 294/500, Loss: 0.466, Accuracy:0.750\n",
      "Epoch 295/500, Loss: 0.512, Accuracy:0.781\n",
      "Epoch 296/500, Loss: 0.554, Accuracy:0.594\n",
      "Epoch 297/500, Loss: 0.643, Accuracy:0.625\n",
      "Epoch 298/500, Loss: 0.407, Accuracy:0.750\n",
      "Epoch 299/500, Loss: 0.452, Accuracy:0.844\n",
      "Epoch 300/500, Loss: 0.372, Accuracy:0.875\n",
      "Epoch 301/500, Loss: 0.590, Accuracy:0.625\n",
      "Epoch 302/500, Loss: 0.500, Accuracy:0.844\n",
      "Epoch 303/500, Loss: 0.438, Accuracy:0.812\n",
      "Epoch 304/500, Loss: 0.374, Accuracy:0.844\n",
      "Epoch 305/500, Loss: 0.646, Accuracy:0.688\n",
      "Epoch 306/500, Loss: 0.410, Accuracy:0.812\n",
      "Epoch 307/500, Loss: 0.423, Accuracy:0.844\n",
      "Epoch 308/500, Loss: 0.445, Accuracy:0.781\n",
      "Epoch 309/500, Loss: 0.308, Accuracy:0.844\n",
      "Epoch 310/500, Loss: 0.310, Accuracy:0.844\n",
      "Epoch 311/500, Loss: 0.494, Accuracy:0.812\n",
      "Epoch 312/500, Loss: 0.441, Accuracy:0.844\n",
      "Epoch 313/500, Loss: 0.388, Accuracy:0.875\n",
      "Epoch 314/500, Loss: 0.438, Accuracy:0.844\n",
      "Epoch 315/500, Loss: 0.396, Accuracy:0.875\n",
      "Epoch 316/500, Loss: 0.288, Accuracy:0.906\n",
      "Epoch 317/500, Loss: 0.541, Accuracy:0.781\n",
      "Epoch 318/500, Loss: 0.459, Accuracy:0.844\n",
      "Epoch 319/500, Loss: 0.446, Accuracy:0.750\n",
      "Epoch 320/500, Loss: 0.296, Accuracy:0.938\n",
      "Epoch 321/500, Loss: 0.497, Accuracy:0.750\n",
      "Epoch 322/500, Loss: 0.401, Accuracy:0.812\n",
      "Epoch 323/500, Loss: 0.618, Accuracy:0.719\n",
      "Epoch 324/500, Loss: 0.312, Accuracy:0.844\n",
      "Epoch 325/500, Loss: 0.486, Accuracy:0.781\n",
      "Epoch 326/500, Loss: 0.379, Accuracy:0.781\n",
      "Epoch 327/500, Loss: 0.467, Accuracy:0.719\n",
      "Epoch 328/500, Loss: 0.378, Accuracy:0.906\n",
      "Epoch 329/500, Loss: 0.390, Accuracy:0.812\n",
      "Epoch 330/500, Loss: 0.660, Accuracy:0.750\n",
      "Epoch 331/500, Loss: 0.444, Accuracy:0.844\n",
      "Epoch 332/500, Loss: 0.411, Accuracy:0.781\n",
      "Epoch 333/500, Loss: 0.595, Accuracy:0.688\n",
      "Epoch 334/500, Loss: 0.453, Accuracy:0.781\n",
      "Epoch 335/500, Loss: 0.373, Accuracy:0.844\n",
      "Epoch 336/500, Loss: 0.515, Accuracy:0.719\n",
      "Epoch 337/500, Loss: 0.427, Accuracy:0.875\n",
      "Epoch 338/500, Loss: 0.603, Accuracy:0.656\n",
      "Epoch 339/500, Loss: 0.474, Accuracy:0.750\n",
      "Epoch 340/500, Loss: 0.409, Accuracy:0.812\n",
      "Epoch 341/500, Loss: 0.380, Accuracy:0.781\n",
      "Epoch 342/500, Loss: 0.505, Accuracy:0.719\n",
      "Epoch 343/500, Loss: 0.480, Accuracy:0.750\n",
      "Epoch 344/500, Loss: 0.459, Accuracy:0.812\n",
      "Epoch 345/500, Loss: 0.380, Accuracy:0.875\n",
      "Epoch 346/500, Loss: 0.636, Accuracy:0.719\n",
      "Epoch 347/500, Loss: 0.593, Accuracy:0.688\n",
      "Epoch 348/500, Loss: 0.444, Accuracy:0.812\n",
      "Epoch 349/500, Loss: 0.538, Accuracy:0.719\n",
      "Epoch 350/500, Loss: 0.360, Accuracy:0.875\n",
      "Epoch 351/500, Loss: 0.457, Accuracy:0.781\n",
      "Epoch 352/500, Loss: 0.461, Accuracy:0.812\n",
      "Epoch 353/500, Loss: 0.582, Accuracy:0.719\n",
      "Epoch 354/500, Loss: 0.455, Accuracy:0.781\n",
      "Epoch 355/500, Loss: 0.388, Accuracy:0.812\n",
      "Epoch 356/500, Loss: 0.477, Accuracy:0.781\n",
      "Epoch 357/500, Loss: 0.478, Accuracy:0.781\n",
      "Epoch 358/500, Loss: 0.415, Accuracy:0.688\n",
      "Epoch 359/500, Loss: 0.636, Accuracy:0.625\n",
      "Epoch 360/500, Loss: 0.386, Accuracy:0.812\n",
      "Epoch 361/500, Loss: 0.466, Accuracy:0.781\n",
      "Epoch 362/500, Loss: 0.450, Accuracy:0.844\n",
      "Epoch 363/500, Loss: 0.289, Accuracy:0.938\n",
      "Epoch 364/500, Loss: 0.509, Accuracy:0.750\n",
      "Epoch 365/500, Loss: 0.363, Accuracy:0.875\n",
      "Epoch 366/500, Loss: 0.511, Accuracy:0.781\n",
      "Epoch 367/500, Loss: 0.377, Accuracy:0.844\n",
      "Epoch 368/500, Loss: 0.403, Accuracy:0.781\n",
      "Epoch 369/500, Loss: 0.438, Accuracy:0.781\n",
      "Epoch 370/500, Loss: 0.376, Accuracy:0.875\n",
      "Epoch 371/500, Loss: 0.452, Accuracy:0.781\n",
      "Epoch 372/500, Loss: 0.362, Accuracy:0.875\n",
      "Epoch 373/500, Loss: 0.576, Accuracy:0.688\n",
      "Epoch 374/500, Loss: 0.405, Accuracy:0.844\n",
      "Epoch 375/500, Loss: 0.555, Accuracy:0.688\n",
      "Epoch 376/500, Loss: 0.580, Accuracy:0.781\n",
      "Epoch 377/500, Loss: 0.494, Accuracy:0.750\n",
      "Epoch 378/500, Loss: 0.418, Accuracy:0.781\n",
      "Epoch 379/500, Loss: 0.495, Accuracy:0.719\n",
      "Epoch 380/500, Loss: 0.436, Accuracy:0.812\n",
      "Epoch 381/500, Loss: 0.529, Accuracy:0.781\n",
      "Epoch 382/500, Loss: 0.301, Accuracy:0.938\n",
      "Epoch 383/500, Loss: 0.314, Accuracy:0.844\n",
      "Epoch 384/500, Loss: 0.324, Accuracy:0.906\n",
      "Epoch 385/500, Loss: 0.419, Accuracy:0.844\n",
      "Epoch 386/500, Loss: 0.465, Accuracy:0.844\n",
      "Epoch 387/500, Loss: 0.488, Accuracy:0.719\n",
      "Epoch 388/500, Loss: 0.341, Accuracy:0.875\n",
      "Epoch 389/500, Loss: 0.392, Accuracy:0.812\n",
      "Epoch 390/500, Loss: 0.677, Accuracy:0.656\n",
      "Epoch 391/500, Loss: 0.440, Accuracy:0.812\n",
      "Epoch 392/500, Loss: 0.461, Accuracy:0.812\n",
      "Epoch 393/500, Loss: 0.436, Accuracy:0.812\n",
      "Epoch 394/500, Loss: 0.496, Accuracy:0.844\n",
      "Epoch 395/500, Loss: 0.454, Accuracy:0.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/500, Loss: 0.637, Accuracy:0.656\n",
      "Epoch 397/500, Loss: 0.603, Accuracy:0.719\n",
      "Epoch 398/500, Loss: 0.741, Accuracy:0.656\n",
      "Epoch 399/500, Loss: 0.488, Accuracy:0.719\n",
      "Epoch 400/500, Loss: 0.434, Accuracy:0.844\n",
      "Epoch 401/500, Loss: 0.446, Accuracy:0.781\n",
      "Epoch 402/500, Loss: 0.540, Accuracy:0.688\n",
      "Epoch 403/500, Loss: 0.479, Accuracy:0.750\n",
      "Epoch 404/500, Loss: 0.364, Accuracy:0.875\n",
      "Epoch 405/500, Loss: 0.482, Accuracy:0.812\n",
      "Epoch 406/500, Loss: 0.342, Accuracy:0.875\n",
      "Epoch 407/500, Loss: 0.365, Accuracy:0.812\n",
      "Epoch 408/500, Loss: 0.517, Accuracy:0.750\n",
      "Epoch 409/500, Loss: 0.456, Accuracy:0.750\n",
      "Epoch 410/500, Loss: 0.425, Accuracy:0.750\n",
      "Epoch 411/500, Loss: 0.445, Accuracy:0.812\n",
      "Epoch 412/500, Loss: 0.632, Accuracy:0.719\n",
      "Epoch 413/500, Loss: 0.584, Accuracy:0.719\n",
      "Epoch 414/500, Loss: 0.371, Accuracy:0.812\n",
      "Epoch 415/500, Loss: 0.657, Accuracy:0.688\n",
      "Epoch 416/500, Loss: 0.486, Accuracy:0.812\n",
      "Epoch 417/500, Loss: 0.378, Accuracy:0.812\n",
      "Epoch 418/500, Loss: 0.429, Accuracy:0.750\n",
      "Epoch 419/500, Loss: 0.437, Accuracy:0.812\n",
      "Epoch 420/500, Loss: 0.470, Accuracy:0.750\n",
      "Epoch 421/500, Loss: 0.489, Accuracy:0.688\n",
      "Epoch 422/500, Loss: 0.437, Accuracy:0.750\n",
      "Epoch 423/500, Loss: 0.341, Accuracy:0.906\n",
      "Epoch 424/500, Loss: 0.533, Accuracy:0.750\n",
      "Epoch 425/500, Loss: 0.417, Accuracy:0.812\n",
      "Epoch 426/500, Loss: 0.745, Accuracy:0.688\n",
      "Epoch 427/500, Loss: 0.423, Accuracy:0.750\n",
      "Epoch 428/500, Loss: 0.549, Accuracy:0.719\n",
      "Epoch 429/500, Loss: 0.373, Accuracy:0.844\n",
      "Epoch 430/500, Loss: 0.410, Accuracy:0.750\n",
      "Epoch 431/500, Loss: 0.508, Accuracy:0.688\n",
      "Epoch 432/500, Loss: 0.478, Accuracy:0.781\n",
      "Epoch 433/500, Loss: 0.435, Accuracy:0.750\n",
      "Epoch 434/500, Loss: 0.500, Accuracy:0.750\n",
      "Epoch 435/500, Loss: 0.490, Accuracy:0.719\n",
      "Epoch 436/500, Loss: 0.313, Accuracy:0.875\n",
      "Epoch 437/500, Loss: 0.309, Accuracy:0.906\n",
      "Epoch 438/500, Loss: 0.542, Accuracy:0.688\n",
      "Epoch 439/500, Loss: 0.522, Accuracy:0.781\n",
      "Epoch 440/500, Loss: 0.464, Accuracy:0.750\n",
      "Epoch 441/500, Loss: 0.408, Accuracy:0.812\n",
      "Epoch 442/500, Loss: 0.471, Accuracy:0.781\n",
      "Epoch 443/500, Loss: 0.456, Accuracy:0.812\n",
      "Epoch 444/500, Loss: 0.566, Accuracy:0.750\n",
      "Epoch 445/500, Loss: 0.587, Accuracy:0.688\n",
      "Epoch 446/500, Loss: 0.362, Accuracy:0.844\n",
      "Epoch 447/500, Loss: 0.393, Accuracy:0.781\n",
      "Epoch 448/500, Loss: 0.472, Accuracy:0.812\n",
      "Epoch 449/500, Loss: 0.336, Accuracy:0.875\n",
      "Epoch 450/500, Loss: 0.512, Accuracy:0.719\n",
      "Epoch 451/500, Loss: 0.541, Accuracy:0.688\n",
      "Epoch 452/500, Loss: 0.467, Accuracy:0.750\n",
      "Epoch 453/500, Loss: 0.442, Accuracy:0.781\n",
      "Epoch 454/500, Loss: 0.514, Accuracy:0.750\n",
      "Epoch 455/500, Loss: 0.422, Accuracy:0.844\n",
      "Epoch 456/500, Loss: 0.535, Accuracy:0.719\n",
      "Epoch 457/500, Loss: 0.656, Accuracy:0.719\n",
      "Epoch 458/500, Loss: 0.394, Accuracy:0.875\n",
      "Epoch 459/500, Loss: 0.407, Accuracy:0.844\n",
      "Epoch 460/500, Loss: 0.469, Accuracy:0.750\n",
      "Epoch 461/500, Loss: 0.587, Accuracy:0.688\n",
      "Epoch 462/500, Loss: 0.396, Accuracy:0.812\n",
      "Epoch 463/500, Loss: 0.433, Accuracy:0.719\n",
      "Epoch 464/500, Loss: 0.330, Accuracy:0.844\n",
      "Epoch 465/500, Loss: 0.302, Accuracy:0.875\n",
      "Epoch 466/500, Loss: 0.566, Accuracy:0.625\n",
      "Epoch 467/500, Loss: 0.489, Accuracy:0.750\n",
      "Epoch 468/500, Loss: 0.384, Accuracy:0.844\n",
      "Epoch 469/500, Loss: 0.347, Accuracy:0.875\n",
      "Epoch 470/500, Loss: 0.338, Accuracy:0.938\n",
      "Epoch 471/500, Loss: 0.553, Accuracy:0.750\n",
      "Epoch 472/500, Loss: 0.542, Accuracy:0.719\n",
      "Epoch 473/500, Loss: 0.414, Accuracy:0.750\n",
      "Epoch 474/500, Loss: 0.532, Accuracy:0.719\n",
      "Epoch 475/500, Loss: 0.426, Accuracy:0.719\n",
      "Epoch 476/500, Loss: 0.472, Accuracy:0.844\n",
      "Epoch 477/500, Loss: 0.568, Accuracy:0.781\n",
      "Epoch 478/500, Loss: 0.589, Accuracy:0.781\n",
      "Epoch 479/500, Loss: 0.508, Accuracy:0.781\n",
      "Epoch 480/500, Loss: 0.402, Accuracy:0.812\n",
      "Epoch 481/500, Loss: 0.582, Accuracy:0.781\n",
      "Epoch 482/500, Loss: 0.348, Accuracy:0.906\n",
      "Epoch 483/500, Loss: 0.507, Accuracy:0.625\n",
      "Epoch 484/500, Loss: 0.550, Accuracy:0.625\n",
      "Epoch 485/500, Loss: 0.406, Accuracy:0.750\n",
      "Epoch 486/500, Loss: 0.616, Accuracy:0.719\n",
      "Epoch 487/500, Loss: 0.512, Accuracy:0.719\n",
      "Epoch 488/500, Loss: 0.604, Accuracy:0.688\n",
      "Epoch 489/500, Loss: 0.475, Accuracy:0.750\n",
      "Epoch 490/500, Loss: 0.411, Accuracy:0.719\n",
      "Epoch 491/500, Loss: 0.354, Accuracy:0.844\n",
      "Epoch 492/500, Loss: 0.434, Accuracy:0.812\n",
      "Epoch 493/500, Loss: 0.458, Accuracy:0.750\n",
      "Epoch 494/500, Loss: 0.434, Accuracy:0.781\n",
      "Epoch 495/500, Loss: 0.493, Accuracy:0.781\n",
      "Epoch 496/500, Loss: 0.486, Accuracy:0.719\n",
      "Epoch 497/500, Loss: 0.527, Accuracy:0.719\n",
      "Epoch 498/500, Loss: 0.464, Accuracy:0.688\n",
      "Epoch 499/500, Loss: 0.542, Accuracy:0.656\n",
      "Epoch 500/500, Loss: 0.335, Accuracy:0.844\n"
     ]
    }
   ],
   "source": [
    "epochs=500\n",
    "for epoch in range(epochs):\n",
    "    for inputs,labels in train_loader:\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.float()\n",
    "        # Forward Prop\n",
    "        outputs=net(inputs)\n",
    "        # net.forward() # we dont realy need to call it like that because Torch will do it automaticly when we supply argument to net, it will call forward itself\n",
    "        # Loss Calculatuion\n",
    "        loss = criterion(outputs, labels)\n",
    "        # clear gradient buffer (w <-- w-lr.fradient)\n",
    "        optimizer.zero_grad()\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Accuracy Calculation\n",
    "    output=(outputs> 0.5).float()\n",
    "    # (output == labels).sum() / output.shape[0]  # if u want calc it manually\n",
    "    accuracy=(output == labels).float().mean()\n",
    "    #print(Statistics)\n",
    "    print(\"Epoch {}/{}, Loss: {:.3f}, Accuracy:{:.3f}\".format(epoch+1, epochs,loss,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de67dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
