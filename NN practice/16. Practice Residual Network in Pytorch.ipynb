{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-16T11:51:12.044382200Z",
     "start_time": "2023-06-16T11:51:07.291825400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Hyper-parameters\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing models\n",
    "transform = tr.Compose([\n",
    "    tr.Pad(4),\n",
    "    tr.RandomHorizontalFlip(),\n",
    "    tr.RandomCrop(32),\n",
    "    tr.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/', train=True, transform=transform, download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='data/', train=False, transform=tr.ToTensor())\n",
    "\n",
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T07:25:37.450510700Z",
     "start_time": "2023-06-17T07:25:36.215162200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![resnetimage](https://user-images.githubusercontent.com/30661597/78585170-f4ac7c80-786b-11ea-8b00-8b751c65f5ca.PNG)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x.clone()\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual  # out= out + residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T05:12:52.513092500Z",
     "start_time": "2023-06-17T05:12:52.499768Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        \"\"\"\n",
    "        layers will be a list: [2,2,2]\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                                       nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T05:12:52.834927300Z",
     "start_time": "2023-06-17T05:12:52.814449600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T07:26:27.014449100Z",
     "start_time": "2023-06-17T07:26:26.987369500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[[[ 0.1620,  0.0228,  0.1398],\n           [-0.0243,  0.1452,  0.0083],\n           [-0.0067, -0.0022,  0.0309]],\n \n          [[ 0.1725, -0.0709, -0.1744],\n           [ 0.1012, -0.1385,  0.0228],\n           [-0.0006, -0.1880, -0.1295]],\n \n          [[-0.0034,  0.1325, -0.1155],\n           [ 0.0216,  0.1336, -0.0044],\n           [-0.0344, -0.0295,  0.0655]]],\n \n \n         [[[ 0.1295,  0.1287,  0.0707],\n           [-0.1076,  0.0946,  0.1167],\n           [-0.0741,  0.1881,  0.1151]],\n \n          [[ 0.1356, -0.0615, -0.1697],\n           [-0.0958, -0.1554, -0.0446],\n           [-0.1181, -0.1477,  0.1562]],\n \n          [[ 0.1369,  0.1789, -0.0420],\n           [ 0.1115, -0.1843, -0.0476],\n           [ 0.0698,  0.1048, -0.1782]]],\n \n \n         [[[-0.0315,  0.0974,  0.1536],\n           [-0.0275, -0.1304,  0.1623],\n           [-0.1417, -0.1663,  0.1347]],\n \n          [[-0.0305,  0.1530, -0.1694],\n           [-0.1170,  0.1388,  0.0998],\n           [-0.1764,  0.1448, -0.1888]],\n \n          [[-0.0310,  0.0974,  0.1654],\n           [ 0.0721,  0.1018, -0.0032],\n           [ 0.1085, -0.0231,  0.1612]]],\n \n \n         [[[ 0.0651, -0.1319, -0.0258],\n           [ 0.0046,  0.1073, -0.1494],\n           [ 0.0536, -0.0862, -0.0233]],\n \n          [[ 0.1876, -0.0780,  0.1302],\n           [ 0.1462, -0.0951, -0.1850],\n           [-0.0344,  0.0392, -0.1205]],\n \n          [[-0.0311, -0.0017, -0.0941],\n           [-0.0021,  0.1051,  0.1088],\n           [-0.1025,  0.0824, -0.0982]]],\n \n \n         [[[-0.0777, -0.0263, -0.0777],\n           [ 0.1410,  0.1756, -0.0384],\n           [-0.1893, -0.0582, -0.1317]],\n \n          [[-0.0195, -0.0451,  0.0600],\n           [ 0.0992, -0.0988, -0.1614],\n           [-0.1030,  0.1110,  0.1470]],\n \n          [[ 0.1071, -0.0158, -0.0365],\n           [-0.1300,  0.1826,  0.1542],\n           [-0.0267, -0.1288,  0.0765]]],\n \n \n         [[[ 0.0862, -0.0795,  0.0694],\n           [ 0.0389, -0.1377, -0.1878],\n           [ 0.1127,  0.1500,  0.0863]],\n \n          [[-0.1095,  0.1262, -0.0050],\n           [-0.1364,  0.1198, -0.0310],\n           [ 0.1449,  0.1409, -0.0991]],\n \n          [[-0.1058, -0.0176, -0.1421],\n           [ 0.0380, -0.0608, -0.1695],\n           [ 0.0158,  0.1231, -0.1654]]],\n \n \n         [[[ 0.0824, -0.0385, -0.1830],\n           [-0.1125,  0.0424,  0.0941],\n           [ 0.1576, -0.0308,  0.0996]],\n \n          [[ 0.0172, -0.0764, -0.1918],\n           [-0.0663,  0.0739, -0.1889],\n           [ 0.0919, -0.1668,  0.1585]],\n \n          [[ 0.0010,  0.0650, -0.0154],\n           [-0.1522, -0.0688,  0.0713],\n           [-0.0523,  0.0447,  0.0907]]],\n \n \n         [[[-0.1084,  0.1209,  0.0242],\n           [-0.0997, -0.1592, -0.1880],\n           [-0.0147,  0.1556,  0.1736]],\n \n          [[ 0.0301, -0.0657,  0.0538],\n           [-0.1545, -0.0173, -0.1592],\n           [-0.1301,  0.0040,  0.1344]],\n \n          [[ 0.0192, -0.1346,  0.1383],\n           [ 0.0912,  0.0343, -0.1672],\n           [-0.1843, -0.0221,  0.0444]]],\n \n \n         [[[ 0.0538, -0.1590,  0.1274],\n           [ 0.0356,  0.0408, -0.1340],\n           [-0.0622, -0.0399, -0.1025]],\n \n          [[ 0.1010, -0.0088,  0.1340],\n           [-0.1402, -0.1087, -0.1684],\n           [ 0.0825, -0.1384, -0.1658]],\n \n          [[ 0.0023, -0.0615, -0.1799],\n           [-0.0416,  0.1350, -0.1408],\n           [ 0.0595,  0.0175, -0.0445]]],\n \n \n         [[[-0.0771, -0.1522, -0.0451],\n           [ 0.0505,  0.1880,  0.0861],\n           [-0.1127,  0.0166, -0.0509]],\n \n          [[ 0.1058,  0.0894, -0.1245],\n           [-0.0900, -0.1085,  0.1844],\n           [ 0.0581, -0.0076, -0.1367]],\n \n          [[-0.0264, -0.0224, -0.0071],\n           [-0.0942, -0.1326, -0.0893],\n           [ 0.0864, -0.0280, -0.0826]]],\n \n \n         [[[-0.1127, -0.0833, -0.1431],\n           [ 0.1418, -0.1425, -0.1053],\n           [ 0.1664, -0.1883,  0.0679]],\n \n          [[ 0.0751, -0.1002, -0.1123],\n           [ 0.0160,  0.1100, -0.0277],\n           [-0.0038,  0.0448, -0.0669]],\n \n          [[-0.1026,  0.1011, -0.0470],\n           [ 0.0237,  0.1521, -0.0067],\n           [-0.0692,  0.0029,  0.1749]]],\n \n \n         [[[ 0.1090, -0.0566,  0.0292],\n           [ 0.0701, -0.0178, -0.1579],\n           [ 0.1708,  0.0186, -0.0068]],\n \n          [[ 0.0935,  0.0689, -0.1425],\n           [-0.0083, -0.0034,  0.1058],\n           [-0.1714,  0.0614,  0.0838]],\n \n          [[ 0.1049, -0.0064,  0.0472],\n           [ 0.0449, -0.0977,  0.1093],\n           [ 0.1802, -0.0614,  0.0064]]],\n \n \n         [[[ 0.0983,  0.1726,  0.0974],\n           [-0.1240,  0.0219, -0.1885],\n           [-0.1820, -0.1611,  0.0041]],\n \n          [[-0.0368, -0.1886, -0.1506],\n           [ 0.1034,  0.0362, -0.1024],\n           [ 0.1148,  0.1158,  0.0170]],\n \n          [[-0.1600,  0.1263, -0.1901],\n           [-0.0666,  0.1123,  0.1517],\n           [ 0.1622,  0.1087,  0.1560]]],\n \n \n         [[[ 0.0458, -0.0806, -0.0414],\n           [ 0.0231, -0.1072,  0.1156],\n           [ 0.1787,  0.1233, -0.0602]],\n \n          [[-0.0339, -0.1494,  0.0457],\n           [-0.0940,  0.1210, -0.0657],\n           [ 0.0210,  0.1753,  0.1136]],\n \n          [[ 0.1781,  0.1444,  0.1705],\n           [ 0.1871,  0.0867,  0.0623],\n           [ 0.1286,  0.0085,  0.0634]]],\n \n \n         [[[ 0.0154,  0.0581, -0.0931],\n           [-0.1446, -0.1037,  0.1870],\n           [ 0.0809, -0.0491, -0.1743]],\n \n          [[ 0.0847,  0.1225,  0.1693],\n           [-0.0562,  0.1720,  0.1294],\n           [-0.0893, -0.0425, -0.0447]],\n \n          [[ 0.1219,  0.1661, -0.0790],\n           [-0.0491, -0.0364,  0.0703],\n           [ 0.0503,  0.0598,  0.1734]]],\n \n \n         [[[-0.0778,  0.1380,  0.1265],\n           [ 0.0940,  0.0975,  0.1745],\n           [ 0.0459, -0.1551, -0.0589]],\n \n          [[ 0.0863,  0.0473, -0.1523],\n           [-0.1293, -0.0866, -0.1310],\n           [-0.0783,  0.0886,  0.0812]],\n \n          [[-0.0004,  0.0223, -0.0770],\n           [-0.0263, -0.0922,  0.0723],\n           [ 0.0609,  0.0690, -0.0046]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[ 0.0053, -0.0486,  0.0319],\n           [ 0.0430, -0.0742, -0.0800],\n           [-0.0367, -0.0489, -0.0757]],\n \n          [[ 0.0626,  0.0832,  0.0302],\n           [ 0.0040, -0.0538, -0.0186],\n           [-0.0766,  0.0201, -0.0328]],\n \n          [[-0.0536, -0.0004, -0.0541],\n           [ 0.0370, -0.0257, -0.0127],\n           [ 0.0759, -0.0018, -0.0210]],\n \n          ...,\n \n          [[ 0.0756, -0.0144,  0.0252],\n           [ 0.0245, -0.0611,  0.0056],\n           [ 0.0031,  0.0148,  0.0181]],\n \n          [[-0.0243, -0.0716,  0.0502],\n           [-0.0656,  0.0186, -0.0367],\n           [ 0.0388, -0.0176,  0.0137]],\n \n          [[ 0.0493,  0.0626, -0.0770],\n           [-0.0156, -0.0708,  0.0054],\n           [-0.0647,  0.0789, -0.0353]]],\n \n \n         [[[-0.0636, -0.0817,  0.0715],\n           [ 0.0130, -0.0689,  0.0833],\n           [ 0.0713,  0.0403, -0.0512]],\n \n          [[-0.0592,  0.0040, -0.0020],\n           [-0.0582,  0.0203,  0.0684],\n           [-0.0404, -0.0222, -0.0389]],\n \n          [[ 0.0363,  0.0686,  0.0439],\n           [ 0.0825,  0.0250,  0.0775],\n           [ 0.0029, -0.0350,  0.0377]],\n \n          ...,\n \n          [[-0.0401,  0.0632, -0.0752],\n           [ 0.0716,  0.0420, -0.0706],\n           [-0.0376,  0.0751,  0.0622]],\n \n          [[ 0.0792, -0.0287,  0.0008],\n           [-0.0786,  0.0467,  0.0644],\n           [ 0.0088,  0.0057,  0.0700]],\n \n          [[ 0.0466, -0.0246,  0.0818],\n           [ 0.0138, -0.0537,  0.0636],\n           [-0.0288, -0.0094, -0.0187]]],\n \n \n         [[[-0.0363,  0.0800, -0.0551],\n           [-0.0783,  0.0618, -0.0213],\n           [ 0.0537,  0.0550,  0.0454]],\n \n          [[ 0.0400, -0.0158, -0.0107],\n           [ 0.0302, -0.0723,  0.0307],\n           [-0.0687, -0.0830,  0.0615]],\n \n          [[-0.0771,  0.0518,  0.0538],\n           [-0.0382, -0.0019, -0.0725],\n           [ 0.0527, -0.0748,  0.0216]],\n \n          ...,\n \n          [[ 0.0676, -0.0076, -0.0653],\n           [-0.0241, -0.0538,  0.0145],\n           [ 0.0071, -0.0566, -0.0391]],\n \n          [[ 0.0547, -0.0614,  0.0484],\n           [-0.0791,  0.0001,  0.0457],\n           [-0.0435, -0.0432, -0.0750]],\n \n          [[ 0.0505, -0.0466,  0.0114],\n           [-0.0555, -0.0817, -0.0828],\n           [ 0.0173, -0.0789,  0.0004]]],\n \n \n         ...,\n \n \n         [[[ 0.0815, -0.0732, -0.0682],\n           [-0.0691,  0.0100,  0.0386],\n           [ 0.0145,  0.0815,  0.0353]],\n \n          [[-0.0513,  0.0763,  0.0132],\n           [-0.0547, -0.0148,  0.0236],\n           [ 0.0582,  0.0238,  0.0764]],\n \n          [[ 0.0205,  0.0436,  0.0097],\n           [ 0.0076, -0.0334, -0.0737],\n           [-0.0647, -0.0480, -0.0764]],\n \n          ...,\n \n          [[ 0.0069,  0.0313,  0.0693],\n           [ 0.0155,  0.0765, -0.0671],\n           [ 0.0606,  0.0656, -0.0829]],\n \n          [[ 0.0272,  0.0458,  0.0687],\n           [ 0.0683,  0.0344,  0.0673],\n           [-0.0793, -0.0469,  0.0725]],\n \n          [[ 0.0723, -0.0779,  0.0576],\n           [-0.0037,  0.0136,  0.0328],\n           [-0.0010, -0.0126, -0.0458]]],\n \n \n         [[[ 0.0729,  0.0574, -0.0486],\n           [ 0.0623, -0.0044, -0.0323],\n           [-0.0499,  0.0097,  0.0331]],\n \n          [[-0.0002, -0.0495,  0.0007],\n           [ 0.0546,  0.0774, -0.0536],\n           [-0.0448,  0.0009,  0.0160]],\n \n          [[ 0.0625,  0.0440,  0.0067],\n           [-0.0716, -0.0436,  0.0500],\n           [-0.0098,  0.0270, -0.0071]],\n \n          ...,\n \n          [[ 0.0484, -0.0570, -0.0068],\n           [-0.0064, -0.0662,  0.0713],\n           [-0.0389,  0.0305, -0.0818]],\n \n          [[ 0.0245, -0.0754, -0.0634],\n           [ 0.0182,  0.0218, -0.0765],\n           [-0.0382,  0.0362, -0.0389]],\n \n          [[-0.0806,  0.0470,  0.0464],\n           [-0.0030, -0.0762, -0.0800],\n           [-0.0708,  0.0308,  0.0278]]],\n \n \n         [[[-0.0661, -0.0055,  0.0384],\n           [-0.0055,  0.0729, -0.0700],\n           [ 0.0593,  0.0738,  0.0129]],\n \n          [[-0.0724,  0.0615, -0.0038],\n           [ 0.0084, -0.0384,  0.0056],\n           [ 0.0053,  0.0735,  0.0333]],\n \n          [[-0.0605, -0.0567, -0.0432],\n           [-0.0119, -0.0734,  0.0047],\n           [ 0.0311, -0.0296, -0.0272]],\n \n          ...,\n \n          [[ 0.0174,  0.0035,  0.0283],\n           [ 0.0113,  0.0476, -0.0232],\n           [-0.0751, -0.0113,  0.0021]],\n \n          [[ 0.0626,  0.0055, -0.0553],\n           [ 0.0152,  0.0401,  0.0245],\n           [-0.0671,  0.0488,  0.0714]],\n \n          [[-0.0423, -0.0740, -0.0543],\n           [ 0.0215,  0.0602,  0.0652],\n           [ 0.0292, -0.0308,  0.0752]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[ 0.0609,  0.0431, -0.0745],\n           [ 0.0797, -0.0417,  0.0663],\n           [ 0.0027, -0.0406, -0.0650]],\n \n          [[-0.0627, -0.0240,  0.0662],\n           [ 0.0234,  0.0064, -0.0117],\n           [-0.0764,  0.0347, -0.0447]],\n \n          [[ 0.0143, -0.0775, -0.0540],\n           [ 0.0243,  0.0626,  0.0784],\n           [ 0.0564, -0.0358, -0.0035]],\n \n          ...,\n \n          [[-0.0459, -0.0300, -0.0430],\n           [-0.0403, -0.0057, -0.0132],\n           [-0.0640, -0.0770, -0.0606]],\n \n          [[-0.0120,  0.0146, -0.0211],\n           [ 0.0135, -0.0074,  0.0289],\n           [ 0.0348,  0.0637,  0.0659]],\n \n          [[-0.0767, -0.0052,  0.0277],\n           [ 0.0128,  0.0228,  0.0099],\n           [ 0.0146,  0.0281, -0.0810]]],\n \n \n         [[[ 0.0762, -0.0459,  0.0128],\n           [-0.0129, -0.0272, -0.0241],\n           [-0.0218,  0.0361, -0.0362]],\n \n          [[-0.0338, -0.0339, -0.0092],\n           [ 0.0371,  0.0492,  0.0249],\n           [ 0.0304,  0.0130,  0.0051]],\n \n          [[ 0.0400,  0.0308, -0.0448],\n           [-0.0012,  0.0195,  0.0129],\n           [-0.0127, -0.0711, -0.0429]],\n \n          ...,\n \n          [[-0.0140,  0.0148, -0.0464],\n           [ 0.0299, -0.0737, -0.0395],\n           [-0.0158,  0.0128, -0.0420]],\n \n          [[-0.0390, -0.0199, -0.0790],\n           [ 0.0586,  0.0365, -0.0232],\n           [-0.0225, -0.0459,  0.0077]],\n \n          [[-0.0255, -0.0431, -0.0678],\n           [-0.0422, -0.0213, -0.0284],\n           [ 0.0700,  0.0306,  0.0114]]],\n \n \n         [[[-0.0260,  0.0666,  0.0533],\n           [ 0.0228,  0.0006,  0.0140],\n           [ 0.0367, -0.0433, -0.0209]],\n \n          [[ 0.0445,  0.0807, -0.0391],\n           [-0.0048,  0.0178, -0.0520],\n           [-0.0348, -0.0742,  0.0386]],\n \n          [[ 0.0666, -0.0455, -0.0796],\n           [-0.0147, -0.0109, -0.0482],\n           [-0.0591,  0.0349, -0.0227]],\n \n          ...,\n \n          [[-0.0360, -0.0213, -0.0356],\n           [-0.0236, -0.0231, -0.0332],\n           [-0.0750,  0.0331, -0.0547]],\n \n          [[-0.0722, -0.0398,  0.0234],\n           [-0.0769,  0.0458,  0.0079],\n           [ 0.0421, -0.0518,  0.0037]],\n \n          [[-0.0103, -0.0715,  0.0161],\n           [ 0.0494, -0.0180,  0.0019],\n           [ 0.0789, -0.0756,  0.0030]]],\n \n \n         ...,\n \n \n         [[[-0.0280,  0.0782, -0.0512],\n           [ 0.0730, -0.0564, -0.0592],\n           [-0.0388, -0.0139, -0.0706]],\n \n          [[-0.0345, -0.0431, -0.0010],\n           [ 0.0617,  0.0007, -0.0095],\n           [ 0.0384,  0.0114, -0.0831]],\n \n          [[ 0.0555,  0.0385, -0.0097],\n           [-0.0096,  0.0131,  0.0686],\n           [-0.0016,  0.0652,  0.0196]],\n \n          ...,\n \n          [[-0.0053, -0.0601,  0.0804],\n           [-0.0246,  0.0434, -0.0808],\n           [ 0.0458, -0.0821,  0.0228]],\n \n          [[-0.0026,  0.0297,  0.0675],\n           [-0.0161, -0.0469, -0.0337],\n           [ 0.0150, -0.0640,  0.0791]],\n \n          [[-0.0541,  0.0459,  0.0657],\n           [ 0.0269,  0.0729,  0.0478],\n           [ 0.0074, -0.0653,  0.0154]]],\n \n \n         [[[ 0.0083, -0.0088, -0.0688],\n           [ 0.0350,  0.0159,  0.0517],\n           [-0.0581,  0.0525, -0.0238]],\n \n          [[-0.0159, -0.0159, -0.0791],\n           [ 0.0672, -0.0425, -0.0769],\n           [ 0.0810, -0.0301, -0.0603]],\n \n          [[ 0.0325, -0.0186,  0.0448],\n           [-0.0649, -0.0422, -0.0649],\n           [ 0.0583, -0.0435,  0.0521]],\n \n          ...,\n \n          [[ 0.0409,  0.0821,  0.0744],\n           [ 0.0164,  0.0285, -0.0361],\n           [-0.0442, -0.0708, -0.0339]],\n \n          [[-0.0027, -0.0384, -0.0213],\n           [-0.0487,  0.0061, -0.0597],\n           [-0.0416, -0.0180,  0.0759]],\n \n          [[-0.0352,  0.0510, -0.0636],\n           [-0.0186, -0.0832,  0.0590],\n           [-0.0324, -0.0233, -0.0317]]],\n \n \n         [[[ 0.0056, -0.0276, -0.0213],\n           [ 0.0773, -0.0070,  0.0415],\n           [-0.0094,  0.0776,  0.0136]],\n \n          [[ 0.0752,  0.0223, -0.0240],\n           [ 0.0634,  0.0713,  0.0589],\n           [-0.0700, -0.0774,  0.0606]],\n \n          [[-0.0549, -0.0788,  0.0708],\n           [-0.0430,  0.0733, -0.0019],\n           [ 0.0094, -0.0046,  0.0125]],\n \n          ...,\n \n          [[-0.0755,  0.0374,  0.0516],\n           [-0.0124,  0.0557,  0.0678],\n           [-0.0549, -0.0393,  0.0771]],\n \n          [[-0.0139, -0.0188, -0.0414],\n           [ 0.0280, -0.0374, -0.0061],\n           [-0.0497,  0.0289,  0.0203]],\n \n          [[ 0.0257,  0.0340, -0.0622],\n           [-0.0783,  0.0261,  0.0456],\n           [-0.0345, -0.0636,  0.0265]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[-4.9164e-02,  5.0936e-02, -7.9228e-02],\n           [ 6.8056e-02,  6.1496e-02,  4.5379e-02],\n           [ 7.5939e-02,  7.0735e-02,  3.9056e-02]],\n \n          [[ 5.5199e-02,  4.8123e-02,  6.1869e-03],\n           [-3.2930e-02, -5.7438e-02,  7.7486e-03],\n           [ 3.1737e-02, -1.7781e-02,  2.9661e-02]],\n \n          [[ 7.5155e-02,  3.3308e-02, -2.9516e-02],\n           [ 3.3918e-02, -2.6512e-02,  7.3449e-02],\n           [ 4.7729e-02,  3.0055e-02, -4.5662e-02]],\n \n          ...,\n \n          [[-7.3403e-02, -6.2932e-02, -6.2593e-02],\n           [ 4.4088e-02,  7.2244e-03, -2.7423e-02],\n           [-5.8771e-02,  5.5895e-03, -1.7138e-02]],\n \n          [[ 7.6469e-02,  7.5581e-02, -1.8704e-02],\n           [-4.2956e-02, -2.2834e-02,  5.7012e-02],\n           [ 7.3045e-02,  6.1379e-03,  6.8423e-02]],\n \n          [[ 4.9923e-03, -8.5400e-03,  2.0417e-02],\n           [ 5.5004e-02,  2.6371e-02,  5.7274e-02],\n           [ 2.7512e-02,  4.4668e-02, -3.8027e-02]]],\n \n \n         [[[-3.9555e-02,  5.5551e-02,  6.6419e-02],\n           [-4.5310e-02, -5.7752e-02, -2.2399e-02],\n           [-6.9712e-02,  7.1911e-03, -5.9209e-02]],\n \n          [[-4.2327e-02, -3.6369e-02, -5.9254e-02],\n           [-4.8344e-02,  4.9843e-02,  1.0416e-02],\n           [-8.3187e-02, -7.6244e-02, -4.6867e-02]],\n \n          [[-5.2120e-02,  9.4169e-03, -2.4730e-02],\n           [-3.3601e-03, -7.2884e-02,  3.5073e-02],\n           [-6.3498e-02, -8.1708e-03,  1.3598e-02]],\n \n          ...,\n \n          [[-3.4810e-02, -5.0372e-02,  2.8111e-02],\n           [-1.1437e-02, -1.0714e-02, -5.7431e-02],\n           [ 1.4429e-02,  3.5915e-02, -5.4939e-02]],\n \n          [[ 4.0371e-02, -1.1867e-02, -1.5952e-05],\n           [ 6.3622e-02, -2.6001e-02,  2.9388e-02],\n           [-7.4426e-02, -7.7667e-02, -5.5975e-02]],\n \n          [[-3.7332e-02,  7.8184e-02,  1.7316e-02],\n           [-6.0919e-02, -4.8163e-02,  1.9551e-02],\n           [ 9.5383e-03, -2.8626e-02, -6.1123e-02]]],\n \n \n         [[[-6.8493e-02, -6.6474e-02,  7.6137e-02],\n           [-6.6779e-02, -6.1442e-02,  2.0874e-02],\n           [ 4.5468e-02, -5.8700e-02, -6.0480e-02]],\n \n          [[-4.4655e-03, -2.4535e-02, -4.8297e-02],\n           [-1.9947e-02, -1.0527e-02, -3.6352e-02],\n           [ 2.6774e-02, -3.5819e-02, -3.2024e-02]],\n \n          [[-1.7344e-02, -4.3441e-02,  3.6920e-02],\n           [ 1.2449e-03, -1.3240e-02, -8.3642e-03],\n           [ 6.7172e-02, -3.0490e-02,  5.4178e-02]],\n \n          ...,\n \n          [[-5.6733e-02,  7.2979e-02, -4.5356e-02],\n           [-2.4769e-02,  3.2512e-02, -3.5475e-02],\n           [ 8.1559e-02,  6.6690e-02,  1.1819e-02]],\n \n          [[ 7.0934e-02, -3.4426e-02, -1.6657e-02],\n           [-5.4324e-03, -1.1674e-03,  4.8434e-02],\n           [ 7.7907e-02, -5.5139e-02,  8.9193e-03]],\n \n          [[-3.3960e-02,  1.5909e-02, -1.8859e-02],\n           [ 2.3043e-02, -7.3754e-02,  4.8353e-02],\n           [ 1.8681e-02,  4.4306e-02, -3.6082e-02]]],\n \n \n         ...,\n \n \n         [[[ 3.4155e-02, -9.0255e-03,  6.9979e-02],\n           [-2.4113e-02,  3.5988e-02, -2.9306e-02],\n           [ 8.1560e-02, -4.3403e-02, -5.6895e-02]],\n \n          [[ 4.7248e-02, -2.1398e-02, -4.3185e-02],\n           [-5.9279e-02, -3.3424e-02, -3.7259e-02],\n           [ 4.1144e-03, -6.4322e-03, -7.6502e-02]],\n \n          [[-3.5256e-02, -7.4170e-02, -1.3137e-02],\n           [-2.7872e-02, -3.3520e-02,  7.0542e-02],\n           [-2.3512e-02,  5.5781e-02, -2.5721e-02]],\n \n          ...,\n \n          [[ 2.7277e-02, -7.3815e-02, -8.2763e-02],\n           [ 6.1705e-02, -5.2595e-02,  7.7810e-02],\n           [ 1.2412e-02, -5.0833e-03, -7.9995e-02]],\n \n          [[ 5.8613e-02,  1.5602e-02, -5.9021e-02],\n           [-5.7836e-02,  2.0136e-02, -8.0735e-02],\n           [ 7.4415e-02, -5.0637e-02,  5.1204e-02]],\n \n          [[ 6.1456e-02,  2.6289e-02,  4.3550e-02],\n           [-5.9082e-03,  2.4100e-02,  3.9397e-02],\n           [-6.6644e-02, -2.7020e-03,  6.7697e-02]]],\n \n \n         [[[-5.4635e-02,  4.1157e-02,  9.9509e-03],\n           [ 1.7191e-02,  8.2658e-02,  3.0401e-02],\n           [-7.6682e-02,  7.4740e-02,  2.5900e-02]],\n \n          [[ 6.5009e-02,  2.9048e-02,  9.0056e-03],\n           [-6.1740e-02,  4.6495e-02, -1.8021e-02],\n           [ 1.4337e-02, -4.2341e-02,  6.9134e-02]],\n \n          [[ 5.7348e-02, -7.2994e-02,  2.2129e-03],\n           [-5.0259e-02,  4.4446e-02,  2.9730e-02],\n           [-5.7185e-02,  5.8160e-02, -3.5797e-02]],\n \n          ...,\n \n          [[-2.1580e-03, -3.1138e-02, -1.4192e-02],\n           [ 1.2100e-02, -6.3064e-02,  4.5352e-02],\n           [-6.9427e-02, -6.7518e-02, -4.6563e-02]],\n \n          [[ 6.4103e-02,  5.9069e-03, -5.6665e-03],\n           [ 6.0249e-02,  1.5737e-02, -6.3875e-02],\n           [ 3.9266e-03, -3.0656e-02, -5.0388e-02]],\n \n          [[ 3.8267e-02,  1.2801e-02,  8.0383e-02],\n           [-3.2486e-02,  6.3034e-02,  3.8282e-02],\n           [-3.0312e-02, -8.8879e-03,  8.9181e-03]]],\n \n \n         [[[ 1.2816e-02,  2.8800e-02, -5.0861e-02],\n           [ 6.0997e-02,  6.5881e-02,  7.8909e-02],\n           [-4.6188e-02, -1.5764e-02, -2.8181e-02]],\n \n          [[-4.4833e-02,  6.0587e-02,  4.8604e-02],\n           [ 8.2392e-02, -1.2138e-02,  6.6571e-02],\n           [ 4.2356e-03, -4.0424e-02,  2.2196e-02]],\n \n          [[-4.9039e-02,  7.8124e-02, -1.9155e-02],\n           [-9.9185e-03,  5.9416e-02,  4.3928e-02],\n           [ 4.5980e-02,  7.1970e-02, -5.5962e-02]],\n \n          ...,\n \n          [[ 2.0273e-02, -7.9924e-02, -4.0752e-02],\n           [-3.5878e-02,  6.5527e-02, -5.0148e-02],\n           [ 3.0275e-03,  8.1049e-02, -4.9285e-02]],\n \n          [[-5.6101e-02,  1.2568e-02,  8.6216e-03],\n           [ 1.4137e-02, -5.8977e-02,  1.1808e-02],\n           [-2.9655e-03, -4.8796e-02,  7.9712e-02]],\n \n          [[-7.8873e-02,  5.6466e-02,  2.7618e-02],\n           [-4.6841e-02, -6.5823e-02,  3.7680e-02],\n           [-7.3251e-02, -4.5822e-02, -6.4165e-02]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[-0.0565, -0.0116, -0.0757],\n           [-0.0036,  0.0209,  0.0250],\n           [-0.0028, -0.0158,  0.0571]],\n \n          [[-0.0435,  0.0325,  0.0397],\n           [-0.0764,  0.0456, -0.0575],\n           [ 0.0227,  0.0341,  0.0612]],\n \n          [[ 0.0588,  0.0710, -0.0618],\n           [-0.0499, -0.0650,  0.0203],\n           [-0.0417, -0.0582,  0.0692]],\n \n          ...,\n \n          [[ 0.0784,  0.0510,  0.0183],\n           [ 0.0800,  0.0416, -0.0720],\n           [ 0.0234, -0.0486, -0.0170]],\n \n          [[ 0.0558, -0.0714, -0.0768],\n           [-0.0392,  0.0341,  0.0287],\n           [-0.0657, -0.0716,  0.0643]],\n \n          [[ 0.0438,  0.0748,  0.0762],\n           [-0.0540, -0.0613, -0.0026],\n           [ 0.0077, -0.0576, -0.0428]]],\n \n \n         [[[ 0.0379,  0.0678, -0.0751],\n           [ 0.0476, -0.0127, -0.0263],\n           [ 0.0170,  0.0628, -0.0151]],\n \n          [[ 0.0651,  0.0811, -0.0018],\n           [-0.0019,  0.0823, -0.0564],\n           [ 0.0766, -0.0813,  0.0377]],\n \n          [[-0.0303,  0.0442,  0.0624],\n           [ 0.0374,  0.0251,  0.0248],\n           [ 0.0440,  0.0569,  0.0021]],\n \n          ...,\n \n          [[-0.0513, -0.0590,  0.0179],\n           [-0.0496,  0.0056,  0.0213],\n           [-0.0219,  0.0325,  0.0146]],\n \n          [[ 0.0454, -0.0712, -0.0456],\n           [ 0.0227, -0.0508, -0.0287],\n           [ 0.0599, -0.0746, -0.0483]],\n \n          [[ 0.0559, -0.0769, -0.0307],\n           [ 0.0641,  0.0754,  0.0782],\n           [ 0.0164, -0.0040,  0.0276]]],\n \n \n         [[[ 0.0416,  0.0288, -0.0132],\n           [-0.0512,  0.0776,  0.0401],\n           [-0.0062,  0.0087, -0.0448]],\n \n          [[-0.0119,  0.0439,  0.0438],\n           [-0.0004,  0.0420,  0.0539],\n           [ 0.0293,  0.0298, -0.0353]],\n \n          [[ 0.0414, -0.0630, -0.0249],\n           [ 0.0085,  0.0385,  0.0025],\n           [-0.0565,  0.0561,  0.0025]],\n \n          ...,\n \n          [[-0.0431,  0.0811,  0.0258],\n           [ 0.0776,  0.0710,  0.0099],\n           [-0.0440,  0.0496, -0.0311]],\n \n          [[-0.0211,  0.0284, -0.0097],\n           [-0.0283,  0.0735, -0.0585],\n           [ 0.0274, -0.0144, -0.0149]],\n \n          [[-0.0009,  0.0573,  0.0771],\n           [-0.0645,  0.0411, -0.0766],\n           [-0.0348, -0.0471, -0.0081]]],\n \n \n         ...,\n \n \n         [[[ 0.0787,  0.0430, -0.0136],\n           [ 0.0802,  0.0512,  0.0248],\n           [-0.0766,  0.0513,  0.0710]],\n \n          [[ 0.0398,  0.0411,  0.0520],\n           [-0.0370, -0.0051,  0.0374],\n           [-0.0808,  0.0704,  0.0574]],\n \n          [[ 0.0698,  0.0667,  0.0802],\n           [ 0.0232, -0.0563,  0.0707],\n           [-0.0779, -0.0203,  0.0595]],\n \n          ...,\n \n          [[-0.0030,  0.0587,  0.0071],\n           [-0.0474,  0.0298,  0.0189],\n           [-0.0408,  0.0630, -0.0451]],\n \n          [[-0.0549,  0.0608, -0.0683],\n           [-0.0638,  0.0589,  0.0238],\n           [ 0.0474, -0.0119, -0.0550]],\n \n          [[-0.0573,  0.0393, -0.0235],\n           [-0.0810, -0.0193, -0.0673],\n           [-0.0821, -0.0334,  0.0798]]],\n \n \n         [[[ 0.0229, -0.0650,  0.0115],\n           [-0.0672,  0.0074, -0.0202],\n           [ 0.0018, -0.0144, -0.0570]],\n \n          [[ 0.0730, -0.0832, -0.0229],\n           [-0.0542, -0.0659,  0.0496],\n           [ 0.0142,  0.0236, -0.0654]],\n \n          [[ 0.0442, -0.0272, -0.0085],\n           [-0.0195,  0.0594,  0.0423],\n           [ 0.0029, -0.0690,  0.0090]],\n \n          ...,\n \n          [[ 0.0416, -0.0501, -0.0336],\n           [-0.0619,  0.0482, -0.0648],\n           [ 0.0712, -0.0724,  0.0414]],\n \n          [[ 0.0109, -0.0688,  0.0762],\n           [-0.0620, -0.0081,  0.0613],\n           [-0.0752, -0.0749, -0.0324]],\n \n          [[-0.0466,  0.0018,  0.0151],\n           [ 0.0814, -0.0073,  0.0095],\n           [ 0.0524,  0.0468,  0.0160]]],\n \n \n         [[[-0.0078,  0.0273, -0.0586],\n           [ 0.0405, -0.0649, -0.0431],\n           [ 0.0095, -0.0423,  0.0150]],\n \n          [[-0.0201,  0.0072, -0.0422],\n           [-0.0360, -0.0760, -0.0050],\n           [ 0.0727, -0.0335, -0.0250]],\n \n          [[-0.0655, -0.0061, -0.0784],\n           [ 0.0407, -0.0623,  0.0237],\n           [ 0.0464, -0.0724, -0.0014]],\n \n          ...,\n \n          [[-0.0170, -0.0748, -0.0379],\n           [-0.0233, -0.0479,  0.0348],\n           [ 0.0106, -0.0163, -0.0739]],\n \n          [[-0.0708, -0.0682, -0.0203],\n           [ 0.0597, -0.0291, -0.0359],\n           [-0.0013,  0.0114,  0.0182]],\n \n          [[-0.0539, -0.0716,  0.0251],\n           [ 0.0174,  0.0044,  0.0625],\n           [ 0.0130,  0.0012, -0.0808]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[-0.0155,  0.0739, -0.0817],\n           [-0.0019,  0.0194, -0.0383],\n           [ 0.0560,  0.0334, -0.0097]],\n \n          [[-0.0821,  0.0368, -0.0546],\n           [ 0.0091,  0.0425, -0.0583],\n           [ 0.0239, -0.0481, -0.0416]],\n \n          [[ 0.0389,  0.0392,  0.0096],\n           [ 0.0638,  0.0368,  0.0219],\n           [-0.0402, -0.0156, -0.0025]],\n \n          ...,\n \n          [[-0.0032,  0.0451, -0.0666],\n           [ 0.0102,  0.0817, -0.0346],\n           [ 0.0616,  0.0411,  0.0232]],\n \n          [[ 0.0726,  0.0531,  0.0308],\n           [ 0.0626, -0.0234, -0.0444],\n           [ 0.0322,  0.0798, -0.0105]],\n \n          [[-0.0329,  0.0193,  0.0564],\n           [-0.0630, -0.0032,  0.0721],\n           [ 0.0824, -0.0244,  0.0700]]],\n \n \n         [[[ 0.0798,  0.0792, -0.0754],\n           [-0.0758,  0.0601,  0.0099],\n           [-0.0557,  0.0418, -0.0440]],\n \n          [[ 0.0487,  0.0825, -0.0760],\n           [ 0.0778, -0.0195,  0.0052],\n           [ 0.0094, -0.0570, -0.0246]],\n \n          [[ 0.0029, -0.0042,  0.0664],\n           [-0.0521, -0.0003,  0.0335],\n           [ 0.0818,  0.0731,  0.0393]],\n \n          ...,\n \n          [[ 0.0728, -0.0571, -0.0406],\n           [ 0.0348,  0.0317,  0.0363],\n           [ 0.0619,  0.0351,  0.0046]],\n \n          [[-0.0152,  0.0213, -0.0136],\n           [-0.0152,  0.0228,  0.0177],\n           [ 0.0773, -0.0606, -0.0406]],\n \n          [[-0.0321,  0.0756,  0.0536],\n           [ 0.0786, -0.0351,  0.0013],\n           [-0.0813,  0.0188,  0.0736]]],\n \n \n         [[[ 0.0054, -0.0367, -0.0029],\n           [ 0.0129,  0.0518,  0.0360],\n           [-0.0252, -0.0516, -0.0163]],\n \n          [[ 0.0249,  0.0487, -0.0527],\n           [-0.0429,  0.0286,  0.0539],\n           [-0.0832,  0.0526, -0.0678]],\n \n          [[ 0.0286, -0.0394, -0.0363],\n           [-0.0117,  0.0545, -0.0568],\n           [-0.0540,  0.0328, -0.0234]],\n \n          ...,\n \n          [[-0.0070, -0.0782,  0.0161],\n           [ 0.0735, -0.0096, -0.0023],\n           [-0.0662, -0.0523,  0.0531]],\n \n          [[ 0.0279, -0.0826,  0.0469],\n           [ 0.0572,  0.0578,  0.0090],\n           [ 0.0081,  0.0123,  0.0063]],\n \n          [[ 0.0076, -0.0041, -0.0240],\n           [-0.0094,  0.0446,  0.0761],\n           [-0.0545, -0.0471, -0.0146]]],\n \n \n         ...,\n \n \n         [[[ 0.0436, -0.0694,  0.0116],\n           [ 0.0064, -0.0067,  0.0722],\n           [ 0.0357,  0.0610, -0.0070]],\n \n          [[-0.0433,  0.0281,  0.0362],\n           [-0.0727,  0.0705,  0.0316],\n           [ 0.0416,  0.0132,  0.0337]],\n \n          [[-0.0504,  0.0454, -0.0397],\n           [ 0.0765,  0.0008,  0.0018],\n           [-0.0746,  0.0479,  0.0251]],\n \n          ...,\n \n          [[-0.0257,  0.0747,  0.0182],\n           [ 0.0793,  0.0764,  0.0559],\n           [ 0.0715, -0.0799, -0.0310]],\n \n          [[-0.0676,  0.0826, -0.0245],\n           [ 0.0465,  0.0505, -0.0595],\n           [ 0.0191, -0.0181, -0.0198]],\n \n          [[ 0.0166, -0.0404, -0.0101],\n           [ 0.0315,  0.0077, -0.0533],\n           [ 0.0182, -0.0178,  0.0796]]],\n \n \n         [[[-0.0042, -0.0701, -0.0762],\n           [ 0.0461, -0.0515, -0.0567],\n           [ 0.0825,  0.0055, -0.0333]],\n \n          [[-0.0388, -0.0230,  0.0192],\n           [ 0.0156, -0.0366, -0.0699],\n           [ 0.0670, -0.0471,  0.0732]],\n \n          [[ 0.0784,  0.0610,  0.0352],\n           [-0.0580, -0.0790,  0.0544],\n           [-0.0665, -0.0646, -0.0236]],\n \n          ...,\n \n          [[ 0.0782, -0.0168,  0.0781],\n           [-0.0023, -0.0465, -0.0252],\n           [-0.0188,  0.0334, -0.0819]],\n \n          [[ 0.0823, -0.0798,  0.0026],\n           [ 0.0209, -0.0338,  0.0577],\n           [ 0.0815, -0.0123, -0.0420]],\n \n          [[ 0.0064, -0.0525, -0.0413],\n           [-0.0339,  0.0441,  0.0756],\n           [-0.0099,  0.0386,  0.0123]]],\n \n \n         [[[-0.0403,  0.0275, -0.0062],\n           [-0.0341, -0.0093, -0.0013],\n           [ 0.0799,  0.0080, -0.0550]],\n \n          [[ 0.0540,  0.0583,  0.0506],\n           [ 0.0404, -0.0738, -0.0017],\n           [ 0.0211, -0.0649, -0.0712]],\n \n          [[ 0.0259, -0.0596, -0.0039],\n           [ 0.0509,  0.0810,  0.0640],\n           [-0.0603, -0.0659, -0.0585]],\n \n          ...,\n \n          [[ 0.0015,  0.0809,  0.0637],\n           [-0.0325, -0.0169, -0.0771],\n           [ 0.0674,  0.0203, -0.0128]],\n \n          [[-0.0157, -0.0201, -0.0777],\n           [ 0.0599,  0.0773, -0.0255],\n           [ 0.0486, -0.0677, -0.0638]],\n \n          [[-0.0113, -0.0266, -0.0566],\n           [-0.0427, -0.0125, -0.0747],\n           [ 0.0496,  0.0160,  0.0096]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[[[ 3.0654e-02,  3.7696e-02,  5.3992e-02],\n           [-3.8476e-02,  5.7389e-02,  5.7287e-02],\n           [-3.5113e-04, -4.0527e-02,  5.6504e-02]],\n \n          [[ 5.8132e-02,  5.5596e-02, -6.0225e-03],\n           [ 4.5698e-02, -5.4111e-02, -1.2217e-02],\n           [ 8.0955e-03, -5.8430e-02,  3.0497e-02]],\n \n          [[-1.3412e-03, -4.4628e-02, -2.3898e-03],\n           [-9.3281e-03, -2.5625e-03, -2.0054e-02],\n           [-1.9732e-02,  1.0415e-02, -5.6657e-02]],\n \n          ...,\n \n          [[-2.6270e-02, -5.7477e-02, -4.1149e-02],\n           [-2.4372e-02, -2.0860e-02, -6.3465e-03],\n           [-4.1498e-02, -4.9623e-02, -3.3724e-02]],\n \n          [[ 2.4036e-02,  3.4758e-03,  5.6036e-02],\n           [ 2.1247e-02,  1.4173e-02, -3.1696e-02],\n           [ 2.8928e-04, -4.3372e-02,  1.3078e-02]],\n \n          [[-2.1110e-02, -4.7286e-02,  3.6436e-02],\n           [ 4.8492e-02,  2.1946e-02,  4.4532e-02],\n           [ 3.9377e-02, -5.7246e-02,  1.6007e-02]]],\n \n \n         [[[-1.7296e-02, -4.3283e-02,  4.5473e-02],\n           [-1.1528e-02, -2.2747e-02, -3.6797e-04],\n           [ 6.9074e-03, -3.6816e-02, -1.6106e-02]],\n \n          [[ 3.2079e-02, -5.3590e-02, -1.2993e-02],\n           [ 5.8277e-02,  2.5269e-02,  4.6936e-02],\n           [-1.4624e-02, -6.5028e-03,  2.1378e-02]],\n \n          [[-1.0567e-02, -4.4427e-02, -2.9238e-02],\n           [ 4.8723e-03,  5.0258e-03, -2.4904e-02],\n           [-2.9665e-02, -1.0509e-02,  3.5747e-02]],\n \n          ...,\n \n          [[ 4.8688e-04, -1.7528e-02, -1.0888e-02],\n           [-1.9134e-02,  4.5584e-02,  5.5880e-02],\n           [-4.7713e-02, -2.0838e-02, -6.4957e-03]],\n \n          [[ 4.7606e-02,  5.5547e-02, -4.7433e-02],\n           [ 4.0185e-02, -2.2114e-02, -5.4390e-02],\n           [ 4.2980e-02,  4.8320e-02, -2.9354e-02]],\n \n          [[-8.4853e-03, -5.5496e-02, -5.8442e-02],\n           [-1.8815e-02, -1.4016e-03, -1.5323e-02],\n           [-3.2939e-02, -3.4793e-02,  5.7547e-02]]],\n \n \n         [[[ 4.0861e-02,  1.6454e-02,  1.7350e-02],\n           [-3.8358e-02, -4.6841e-04,  4.8886e-02],\n           [-4.8291e-02, -4.6577e-02, -3.6922e-02]],\n \n          [[ 4.9474e-02,  3.1543e-02, -1.5395e-02],\n           [-3.5576e-02,  1.0611e-02,  2.4112e-02],\n           [-5.0338e-02, -5.1845e-02, -3.9870e-02]],\n \n          [[-5.5953e-02, -3.6206e-02, -5.7195e-02],\n           [ 1.2557e-02, -8.2520e-03, -3.8075e-02],\n           [ 4.0867e-02, -2.7814e-02,  1.6619e-02]],\n \n          ...,\n \n          [[ 5.5307e-02, -1.8301e-02,  1.7718e-02],\n           [ 4.6823e-02,  1.6557e-02, -4.6766e-02],\n           [ 2.7084e-02, -4.5582e-02, -2.8033e-02]],\n \n          [[ 2.0570e-02, -2.3812e-02, -3.6977e-02],\n           [-5.0079e-02,  5.0340e-02, -3.9023e-02],\n           [ 3.2853e-02, -4.0430e-03,  4.6893e-02]],\n \n          [[-3.4705e-02, -4.4339e-02,  7.3365e-03],\n           [ 4.1356e-02, -5.7148e-02, -5.7201e-02],\n           [-3.6482e-02, -2.8582e-02, -5.4976e-03]]],\n \n \n         ...,\n \n \n         [[[ 2.2557e-02,  8.0355e-03,  1.1456e-02],\n           [ 3.6425e-02,  4.7795e-05, -4.6591e-02],\n           [-4.7556e-02,  5.4122e-02,  3.7631e-02]],\n \n          [[ 2.6128e-02,  2.3618e-02,  1.7047e-02],\n           [ 2.0816e-02, -1.5847e-02,  3.8895e-02],\n           [ 8.5525e-03,  1.3937e-03, -4.1453e-02]],\n \n          [[-1.1171e-02, -5.4248e-02, -4.1457e-02],\n           [-5.6286e-03,  1.0733e-02, -1.7304e-02],\n           [-3.0459e-02,  2.9084e-02, -5.1335e-02]],\n \n          ...,\n \n          [[-5.7274e-02, -2.0938e-02, -4.8206e-03],\n           [ 3.4281e-02,  3.6424e-02, -1.7771e-03],\n           [-1.2932e-02,  2.1105e-02, -5.6326e-02]],\n \n          [[-5.5626e-03,  5.9437e-03, -9.2826e-03],\n           [-4.9404e-02,  3.7480e-02, -2.2970e-02],\n           [ 9.2085e-03, -3.6590e-02, -3.7744e-02]],\n \n          [[-3.0673e-02,  2.5795e-02, -1.1615e-02],\n           [ 8.9989e-03, -4.6189e-02, -5.1014e-02],\n           [ 1.9889e-02, -3.6157e-02,  5.5066e-02]]],\n \n \n         [[[ 1.0274e-03,  1.2539e-02,  2.6344e-02],\n           [-1.0833e-02,  3.4472e-02, -3.6585e-02],\n           [ 3.2598e-02, -5.4929e-02,  2.1493e-02]],\n \n          [[ 3.1800e-02, -1.3697e-03, -3.1380e-02],\n           [-3.6374e-02, -1.8562e-02,  4.4984e-02],\n           [ 1.3061e-02, -5.2692e-02, -4.1577e-02]],\n \n          [[ 4.0930e-02, -3.6023e-03,  4.0542e-02],\n           [-3.6870e-02, -2.2007e-02, -2.0469e-04],\n           [-5.3117e-02, -4.5630e-02,  4.7074e-02]],\n \n          ...,\n \n          [[-2.3998e-02,  5.7514e-02,  1.1247e-02],\n           [ 3.7294e-02, -1.1903e-02,  2.8903e-02],\n           [ 1.2135e-03, -4.3446e-02, -4.9542e-02]],\n \n          [[ 5.3871e-02, -3.8637e-02,  3.7369e-02],\n           [-1.5239e-02, -4.9915e-02, -1.9967e-02],\n           [-1.7033e-02, -5.6594e-02, -2.4830e-02]],\n \n          [[ 2.0632e-02,  2.7891e-02,  1.7105e-02],\n           [-4.1772e-02, -4.7662e-02,  7.5399e-03],\n           [-4.3811e-03,  4.8252e-02, -5.8238e-02]]],\n \n \n         [[[-5.3312e-04,  2.8658e-03, -4.7691e-02],\n           [ 2.3100e-02, -5.3502e-02,  1.5520e-03],\n           [-1.7947e-02,  9.5209e-03, -1.5692e-02]],\n \n          [[ 2.2851e-02, -5.1456e-02, -1.2912e-02],\n           [-1.0820e-02,  3.0743e-02,  3.9561e-02],\n           [ 5.8087e-02,  7.9001e-03, -3.3087e-02]],\n \n          [[-5.0699e-03, -3.9546e-02, -1.3549e-02],\n           [-5.4475e-02,  4.8173e-02, -1.5487e-02],\n           [-6.3133e-03,  5.0380e-02,  6.0864e-03]],\n \n          ...,\n \n          [[ 4.6578e-02, -1.6071e-02,  5.3463e-02],\n           [-1.2627e-02, -5.3075e-02,  4.2280e-02],\n           [-4.7686e-02,  5.3178e-02,  4.9022e-02]],\n \n          [[-1.4414e-02,  1.9164e-02, -2.2313e-02],\n           [-2.3038e-02,  1.2283e-02, -5.5777e-03],\n           [-2.8220e-02, -7.2989e-03, -1.4713e-02]],\n \n          [[-2.5316e-02, -9.1820e-03,  3.7476e-02],\n           [ 4.8002e-02,  1.2710e-02, -4.8102e-02],\n           [ 1.7296e-02, -5.6722e-02,  5.3704e-02]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[[[ 0.0824,  0.0024, -0.0594],\n           [-0.0775, -0.0142,  0.0492],\n           [ 0.0183, -0.0031,  0.0768]],\n \n          [[-0.0353,  0.0713,  0.0687],\n           [-0.0540,  0.0032, -0.0340],\n           [ 0.0532, -0.0450,  0.0824]],\n \n          [[-0.0115, -0.0553,  0.0040],\n           [ 0.0657, -0.0329, -0.0424],\n           [ 0.0349, -0.0013,  0.0136]],\n \n          ...,\n \n          [[ 0.0830,  0.0319,  0.0324],\n           [ 0.0514, -0.0312, -0.0051],\n           [-0.0766, -0.0628, -0.0721]],\n \n          [[-0.0098, -0.0479,  0.0345],\n           [ 0.0727,  0.0239, -0.0166],\n           [ 0.0106,  0.0253, -0.0258]],\n \n          [[ 0.0327,  0.0103,  0.0071],\n           [ 0.0810,  0.0759,  0.0738],\n           [ 0.0121,  0.0601,  0.0195]]],\n \n \n         [[[-0.0028,  0.0567, -0.0817],\n           [ 0.0017, -0.0408, -0.0131],\n           [-0.0468, -0.0155, -0.0775]],\n \n          [[ 0.0585,  0.0808, -0.0233],\n           [-0.0665,  0.0088,  0.0354],\n           [ 0.0111,  0.0605,  0.0538]],\n \n          [[-0.0652,  0.0609,  0.0605],\n           [-0.0089, -0.0777,  0.0407],\n           [-0.0687, -0.0232, -0.0092]],\n \n          ...,\n \n          [[-0.0518,  0.0378,  0.0721],\n           [ 0.0092, -0.0355,  0.0235],\n           [-0.0817, -0.0481,  0.0803]],\n \n          [[ 0.0702, -0.0479, -0.0033],\n           [-0.0135,  0.0411,  0.0740],\n           [-0.0552, -0.0772, -0.0037]],\n \n          [[ 0.0484,  0.0134, -0.0617],\n           [-0.0305,  0.0682, -0.0562],\n           [ 0.0563,  0.0307, -0.0354]]],\n \n \n         [[[ 0.0170, -0.0181, -0.0357],\n           [-0.0544, -0.0748, -0.0104],\n           [ 0.0002,  0.0342, -0.0236]],\n \n          [[-0.0559,  0.0574,  0.0188],\n           [ 0.0218,  0.0091, -0.0831],\n           [-0.0002,  0.0681,  0.0582]],\n \n          [[ 0.0784, -0.0450, -0.0579],\n           [-0.0062, -0.0561, -0.0201],\n           [-0.0567,  0.0721,  0.0513]],\n \n          ...,\n \n          [[-0.0466,  0.0258,  0.0204],\n           [ 0.0494, -0.0093, -0.0645],\n           [-0.0176,  0.0111, -0.0480]],\n \n          [[ 0.0544,  0.0455, -0.0495],\n           [ 0.0792,  0.0550,  0.0296],\n           [-0.0191,  0.0713,  0.0800]],\n \n          [[-0.0488,  0.0658, -0.0561],\n           [ 0.0396, -0.0397, -0.0258],\n           [-0.0655,  0.0297, -0.0543]]],\n \n \n         ...,\n \n \n         [[[-0.0651,  0.0257,  0.0165],\n           [-0.0754, -0.0328,  0.0692],\n           [ 0.0684,  0.0468, -0.0285]],\n \n          [[ 0.0088, -0.0518, -0.0679],\n           [-0.0316, -0.0564, -0.0714],\n           [-0.0824,  0.0735,  0.0198]],\n \n          [[-0.0832, -0.0025,  0.0380],\n           [ 0.0209, -0.0466, -0.0426],\n           [ 0.0112, -0.0510, -0.0570]],\n \n          ...,\n \n          [[-0.0643, -0.0283, -0.0719],\n           [ 0.0729, -0.0034, -0.0821],\n           [ 0.0357, -0.0613,  0.0020]],\n \n          [[-0.0635,  0.0185, -0.0708],\n           [ 0.0411, -0.0307,  0.0513],\n           [ 0.0639,  0.0551,  0.0196]],\n \n          [[ 0.0304, -0.0696, -0.0288],\n           [-0.0494, -0.0343,  0.0696],\n           [ 0.0685,  0.0112, -0.0163]]],\n \n \n         [[[ 0.0770, -0.0698,  0.0117],\n           [ 0.0434,  0.0075, -0.0051],\n           [ 0.0442,  0.0705, -0.0016]],\n \n          [[ 0.0437, -0.0020,  0.0414],\n           [ 0.0798,  0.0474, -0.0141],\n           [ 0.0430, -0.0530,  0.0541]],\n \n          [[ 0.0502,  0.0822, -0.0542],\n           [ 0.0783, -0.0632, -0.0517],\n           [ 0.0423, -0.0304,  0.0559]],\n \n          ...,\n \n          [[ 0.0304, -0.0254,  0.0214],\n           [-0.0663,  0.0728, -0.0620],\n           [ 0.0483,  0.0092, -0.0363]],\n \n          [[-0.0069,  0.0552,  0.0449],\n           [-0.0687, -0.0617,  0.0191],\n           [-0.0185,  0.0385, -0.0280]],\n \n          [[ 0.0091, -0.0040,  0.0265],\n           [ 0.0231, -0.0686,  0.0615],\n           [ 0.0226,  0.0545, -0.0116]]],\n \n \n         [[[ 0.0464,  0.0513,  0.0286],\n           [-0.0277, -0.0142,  0.0417],\n           [ 0.0203,  0.0382,  0.0354]],\n \n          [[-0.0160,  0.0059, -0.0807],\n           [-0.0507, -0.0327,  0.0560],\n           [-0.0758,  0.0257,  0.0521]],\n \n          [[-0.0049,  0.0639, -0.0199],\n           [ 0.0095, -0.0431, -0.0276],\n           [-0.0156,  0.0206,  0.0551]],\n \n          ...,\n \n          [[ 0.0651, -0.0833, -0.0796],\n           [ 0.0074, -0.0720, -0.0660],\n           [-0.0728, -0.0719,  0.0359]],\n \n          [[ 0.0206, -0.0621,  0.0179],\n           [-0.0619, -0.0826,  0.0432],\n           [-0.0804, -0.0590, -0.0218]],\n \n          [[-0.0816,  0.0174, -0.0099],\n           [-0.0088,  0.0754,  0.0384],\n           [-0.0381,  0.0832,  0.0738]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[[[-4.0649e-02,  2.5587e-02,  1.2062e-02],\n           [ 1.1948e-02,  1.0529e-02, -9.4534e-03],\n           [ 4.5493e-02, -4.4949e-03,  5.4819e-02]],\n \n          [[ 3.4675e-02,  1.2478e-02,  2.0815e-03],\n           [-2.3338e-02,  1.0951e-02, -1.9657e-02],\n           [ 5.3307e-02, -7.3577e-03, -2.7210e-02]],\n \n          [[-7.7562e-03, -3.7496e-02, -4.5624e-05],\n           [-3.7876e-02, -7.5212e-03, -1.9624e-02],\n           [ 5.6164e-02,  5.3970e-02, -1.7903e-02]],\n \n          ...,\n \n          [[-2.0851e-02,  3.1045e-02, -6.1520e-03],\n           [-1.9364e-02, -1.9172e-02, -2.6569e-02],\n           [ 1.0265e-02, -5.5354e-02, -1.0503e-02]],\n \n          [[ 3.2376e-02,  5.6106e-02, -5.1539e-02],\n           [ 4.3250e-02, -6.5298e-03, -1.6942e-03],\n           [ 2.8584e-02, -1.3057e-02, -5.0227e-02]],\n \n          [[-1.6835e-02, -6.2554e-03,  5.4173e-03],\n           [-6.5478e-03, -2.5896e-02,  4.7220e-02],\n           [ 4.2200e-02,  2.1959e-02, -2.7542e-02]]],\n \n \n         [[[ 3.6895e-02,  2.0509e-03, -1.5855e-02],\n           [ 3.8097e-02, -5.2693e-02, -3.3868e-02],\n           [ 2.5103e-02, -3.5412e-02, -4.2952e-02]],\n \n          [[ 5.1340e-02, -2.1092e-03, -2.0513e-02],\n           [ 2.6216e-02, -3.6617e-02, -3.3902e-02],\n           [-2.4187e-02, -3.8771e-02,  4.6133e-02]],\n \n          [[-2.1119e-03,  1.3367e-02,  3.0042e-02],\n           [-6.4569e-03,  1.9647e-02, -1.6244e-03],\n           [ 3.4726e-02,  4.7163e-02,  2.1258e-02]],\n \n          ...,\n \n          [[ 3.7592e-03, -3.3879e-02,  2.5616e-02],\n           [ 3.7853e-03, -2.0800e-02, -5.4729e-02],\n           [-5.3792e-02,  2.6785e-02,  3.7068e-02]],\n \n          [[ 1.9415e-02, -1.7967e-02, -5.7565e-02],\n           [ 4.1455e-02, -1.2261e-02, -1.2834e-02],\n           [-4.1680e-02, -3.8071e-02,  5.7492e-02]],\n \n          [[-1.5854e-02, -5.3250e-03, -1.2148e-02],\n           [ 3.6773e-02,  5.5123e-02,  3.6300e-02],\n           [ 6.6556e-03,  5.1818e-02, -3.0755e-02]]],\n \n \n         [[[ 1.2040e-02, -2.0113e-02,  5.1240e-02],\n           [ 3.6360e-02, -3.7639e-02,  3.1365e-02],\n           [ 2.9860e-02,  3.4476e-03, -1.9082e-02]],\n \n          [[-3.0829e-02, -4.4320e-02, -1.2094e-02],\n           [ 2.0635e-02,  3.3386e-02,  4.5446e-02],\n           [ 3.1924e-02,  1.9259e-02,  1.3467e-02]],\n \n          [[ 2.7852e-02,  3.9857e-03,  2.7941e-02],\n           [ 4.8390e-03, -5.7896e-02, -5.4829e-02],\n           [ 2.2492e-02, -2.0897e-02,  1.6025e-02]],\n \n          ...,\n \n          [[ 4.2878e-02,  3.0989e-02, -1.8372e-02],\n           [-8.7700e-03,  3.4805e-02,  2.8837e-02],\n           [ 1.8484e-02,  4.1006e-02,  4.9690e-02]],\n \n          [[ 4.4424e-02, -5.7253e-02,  3.9204e-02],\n           [ 1.8992e-03,  1.3332e-02,  3.6487e-02],\n           [-4.8889e-03, -2.8521e-02,  5.1805e-03]],\n \n          [[ 7.2762e-03, -4.6253e-02, -6.0253e-03],\n           [-3.9934e-02, -4.8054e-02,  3.1079e-02],\n           [ 3.1271e-02, -1.2549e-02,  1.2644e-02]]],\n \n \n         ...,\n \n \n         [[[-3.4513e-02, -8.6082e-03,  2.6379e-02],\n           [-4.2205e-03, -3.0159e-02, -5.5368e-02],\n           [ 1.8006e-02,  5.4110e-02,  1.1979e-02]],\n \n          [[-2.2233e-02,  2.1010e-02,  1.7255e-02],\n           [ 2.3987e-03, -3.5612e-02,  4.8765e-02],\n           [ 3.5838e-02,  2.6841e-02, -3.9639e-02]],\n \n          [[ 1.7813e-02,  1.4837e-02, -9.7509e-03],\n           [ 2.3616e-02,  5.0209e-02,  2.1866e-02],\n           [ 5.0820e-02, -3.9980e-02, -4.0419e-03]],\n \n          ...,\n \n          [[ 2.8018e-02, -3.8948e-02, -1.5929e-02],\n           [ 5.5640e-02,  3.1298e-02,  9.7065e-03],\n           [ 2.2955e-02, -1.4468e-02, -1.2474e-02]],\n \n          [[-3.9060e-02,  1.0869e-03,  3.0033e-02],\n           [-5.4103e-02,  4.1929e-02, -2.4740e-03],\n           [-2.3204e-02,  3.4127e-03,  5.3669e-02]],\n \n          [[-3.1072e-02, -5.3584e-02,  4.9014e-02],\n           [-5.5817e-02, -3.8119e-02, -3.8613e-02],\n           [-6.7801e-03, -1.2508e-02,  5.4119e-02]]],\n \n \n         [[[ 2.5657e-02, -4.7270e-02, -3.6942e-02],\n           [-2.9744e-02,  1.0016e-02,  1.8545e-02],\n           [-5.6196e-03, -1.2878e-03,  1.1335e-03]],\n \n          [[-3.6529e-02,  9.7188e-03, -1.6620e-03],\n           [ 5.5298e-03, -9.2538e-03,  5.4883e-02],\n           [-5.6834e-02,  5.0100e-02, -4.7997e-02]],\n \n          [[-3.8936e-02, -4.0083e-02,  1.3377e-02],\n           [ 2.5437e-03,  2.1061e-02, -2.6602e-04],\n           [ 3.3706e-03,  6.8930e-03, -8.4691e-04]],\n \n          ...,\n \n          [[-4.9261e-02,  3.1789e-02,  3.3188e-02],\n           [ 3.6578e-02, -1.4628e-02, -5.1018e-02],\n           [-5.5375e-02, -9.4956e-03,  1.9569e-02]],\n \n          [[ 1.0067e-02, -5.5280e-02, -3.9469e-02],\n           [-2.0584e-02, -7.2191e-03, -1.8692e-02],\n           [-3.4586e-02,  1.4864e-02, -2.1798e-02]],\n \n          [[ 4.3990e-02,  5.6411e-02, -2.1886e-02],\n           [-3.5170e-02,  2.9491e-02, -6.2723e-03],\n           [-4.9960e-02, -2.4810e-02, -2.5843e-03]]],\n \n \n         [[[ 5.9085e-03,  5.2757e-02,  3.6410e-02],\n           [ 9.3787e-03,  5.4143e-03, -1.6220e-02],\n           [ 3.2524e-02, -2.8645e-02, -1.9060e-02]],\n \n          [[-1.8379e-02, -4.6632e-02,  3.6402e-02],\n           [-4.7624e-02,  3.0072e-02,  2.5892e-02],\n           [-3.2704e-02,  4.1957e-02, -1.3709e-02]],\n \n          [[-1.0786e-02, -7.5319e-03,  1.4535e-02],\n           [-2.7528e-02,  3.8799e-02,  4.1755e-02],\n           [ 3.2297e-02,  5.2770e-02, -3.0336e-02]],\n \n          ...,\n \n          [[ 5.0290e-02,  4.4521e-02, -2.5672e-02],\n           [ 4.7301e-02, -3.6538e-02,  3.6810e-02],\n           [-5.5602e-02, -3.0037e-02,  2.0182e-02]],\n \n          [[-3.9729e-02, -3.7710e-02,  2.3322e-02],\n           [-2.6154e-02,  3.5037e-02,  5.4575e-03],\n           [-1.5354e-02,  2.6277e-02, -5.2584e-02]],\n \n          [[ 2.0460e-02, -4.4822e-03,  5.6655e-02],\n           [-5.8819e-02, -4.7966e-02,  3.3881e-02],\n           [ 2.8290e-02,  5.6773e-02, -3.4300e-02]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[[[ 0.0333,  0.0046,  0.0256],\n           [-0.0388, -0.0179,  0.0258],\n           [ 0.0578, -0.0064,  0.0039]],\n \n          [[ 0.0406,  0.0404, -0.0156],\n           [-0.0291,  0.0155, -0.0403],\n           [ 0.0282,  0.0075,  0.0235]],\n \n          [[-0.0158, -0.0129,  0.0018],\n           [-0.0519, -0.0575, -0.0414],\n           [ 0.0196,  0.0493, -0.0505]],\n \n          ...,\n \n          [[ 0.0175, -0.0280, -0.0543],\n           [-0.0349,  0.0226, -0.0011],\n           [ 0.0402, -0.0039, -0.0415]],\n \n          [[-0.0160, -0.0162,  0.0507],\n           [-0.0362,  0.0536,  0.0116],\n           [ 0.0400, -0.0189,  0.0232]],\n \n          [[ 0.0088, -0.0482, -0.0107],\n           [ 0.0479, -0.0566,  0.0137],\n           [ 0.0107, -0.0145, -0.0215]]],\n \n \n         [[[-0.0183, -0.0399, -0.0417],\n           [-0.0023,  0.0273,  0.0252],\n           [-0.0588,  0.0142, -0.0467]],\n \n          [[ 0.0237,  0.0564, -0.0200],\n           [ 0.0278, -0.0192, -0.0409],\n           [ 0.0358, -0.0132,  0.0292]],\n \n          [[ 0.0441, -0.0183, -0.0201],\n           [ 0.0404, -0.0412,  0.0479],\n           [ 0.0244, -0.0303, -0.0083]],\n \n          ...,\n \n          [[ 0.0480,  0.0507,  0.0574],\n           [ 0.0578, -0.0163,  0.0380],\n           [-0.0577,  0.0280,  0.0515]],\n \n          [[ 0.0142, -0.0475, -0.0379],\n           [ 0.0019, -0.0548,  0.0546],\n           [ 0.0160,  0.0461,  0.0435]],\n \n          [[ 0.0534, -0.0354, -0.0205],\n           [-0.0125, -0.0114, -0.0307],\n           [-0.0234, -0.0140,  0.0562]]],\n \n \n         [[[-0.0425, -0.0111, -0.0284],\n           [-0.0172, -0.0320, -0.0188],\n           [-0.0250,  0.0568, -0.0435]],\n \n          [[-0.0168,  0.0025, -0.0016],\n           [ 0.0491, -0.0275,  0.0273],\n           [ 0.0110,  0.0205,  0.0107]],\n \n          [[-0.0245,  0.0109,  0.0372],\n           [-0.0230, -0.0326, -0.0026],\n           [-0.0233,  0.0431,  0.0166]],\n \n          ...,\n \n          [[-0.0293, -0.0163,  0.0024],\n           [-0.0250, -0.0071,  0.0071],\n           [ 0.0050,  0.0588,  0.0521]],\n \n          [[-0.0031, -0.0531, -0.0473],\n           [ 0.0054,  0.0003, -0.0229],\n           [-0.0468, -0.0396, -0.0315]],\n \n          [[ 0.0190, -0.0306,  0.0294],\n           [ 0.0002,  0.0363, -0.0067],\n           [-0.0204, -0.0584,  0.0191]]],\n \n \n         ...,\n \n \n         [[[-0.0055, -0.0013, -0.0259],\n           [ 0.0071,  0.0486,  0.0105],\n           [ 0.0077,  0.0220, -0.0440]],\n \n          [[ 0.0301, -0.0198,  0.0402],\n           [ 0.0458, -0.0437, -0.0560],\n           [ 0.0026, -0.0172,  0.0514]],\n \n          [[-0.0006,  0.0194, -0.0279],\n           [ 0.0036, -0.0426, -0.0115],\n           [-0.0353, -0.0466,  0.0497]],\n \n          ...,\n \n          [[ 0.0510, -0.0198, -0.0416],\n           [-0.0545, -0.0173,  0.0123],\n           [ 0.0267, -0.0503,  0.0454]],\n \n          [[-0.0028,  0.0241,  0.0016],\n           [ 0.0435, -0.0357,  0.0219],\n           [-0.0170, -0.0129, -0.0124]],\n \n          [[ 0.0300, -0.0180,  0.0133],\n           [-0.0335, -0.0368,  0.0084],\n           [-0.0065,  0.0538,  0.0367]]],\n \n \n         [[[ 0.0037, -0.0222,  0.0434],\n           [-0.0379, -0.0551,  0.0493],\n           [ 0.0439, -0.0052, -0.0312]],\n \n          [[ 0.0216, -0.0450,  0.0456],\n           [ 0.0365,  0.0583,  0.0490],\n           [-0.0180, -0.0578,  0.0584]],\n \n          [[-0.0351, -0.0316,  0.0505],\n           [-0.0310,  0.0498,  0.0498],\n           [ 0.0391,  0.0432,  0.0308]],\n \n          ...,\n \n          [[ 0.0456,  0.0068, -0.0111],\n           [-0.0076, -0.0508,  0.0222],\n           [-0.0386, -0.0181, -0.0173]],\n \n          [[-0.0168, -0.0386, -0.0246],\n           [-0.0147, -0.0440, -0.0481],\n           [ 0.0243, -0.0029,  0.0585]],\n \n          [[-0.0305,  0.0075,  0.0366],\n           [ 0.0090, -0.0368,  0.0407],\n           [ 0.0313,  0.0501,  0.0342]]],\n \n \n         [[[-0.0369, -0.0318,  0.0434],\n           [ 0.0229, -0.0089, -0.0333],\n           [ 0.0191, -0.0064, -0.0522]],\n \n          [[-0.0496, -0.0017,  0.0555],\n           [ 0.0095, -0.0378, -0.0459],\n           [-0.0200,  0.0550,  0.0272]],\n \n          [[ 0.0451, -0.0239,  0.0196],\n           [-0.0478, -0.0093, -0.0400],\n           [ 0.0072,  0.0388, -0.0290]],\n \n          ...,\n \n          [[ 0.0556,  0.0101,  0.0523],\n           [-0.0358, -0.0007,  0.0226],\n           [-0.0326, -0.0369,  0.0576]],\n \n          [[ 0.0259,  0.0161,  0.0360],\n           [ 0.0443, -0.0162, -0.0391],\n           [ 0.0516, -0.0530, -0.0484]],\n \n          [[ 0.0334,  0.0425,  0.0143],\n           [ 0.0322,  0.0203,  0.0091],\n           [ 0.0101, -0.0438, -0.0478]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[[[ 0.0578,  0.0089,  0.0050],\n           [ 0.0502,  0.0552, -0.0540],\n           [ 0.0508, -0.0291,  0.0522]],\n \n          [[-0.0189, -0.0344, -0.0110],\n           [ 0.0327, -0.0481,  0.0039],\n           [-0.0340,  0.0524, -0.0435]],\n \n          [[-0.0151,  0.0234,  0.0452],\n           [-0.0230,  0.0213,  0.0266],\n           [-0.0013,  0.0264, -0.0585]],\n \n          ...,\n \n          [[-0.0003,  0.0355, -0.0181],\n           [-0.0396, -0.0572,  0.0226],\n           [ 0.0580, -0.0518,  0.0157]],\n \n          [[ 0.0506,  0.0420, -0.0057],\n           [ 0.0240,  0.0461,  0.0268],\n           [ 0.0521, -0.0525, -0.0402]],\n \n          [[-0.0486, -0.0224, -0.0527],\n           [-0.0480,  0.0494,  0.0054],\n           [-0.0374, -0.0566, -0.0292]]],\n \n \n         [[[-0.0077, -0.0425, -0.0051],\n           [-0.0180, -0.0139,  0.0308],\n           [-0.0112,  0.0143, -0.0099]],\n \n          [[ 0.0353,  0.0090, -0.0558],\n           [-0.0484, -0.0419,  0.0493],\n           [-0.0282, -0.0562, -0.0352]],\n \n          [[-0.0082, -0.0382, -0.0467],\n           [ 0.0224,  0.0525,  0.0315],\n           [ 0.0340, -0.0475,  0.0069]],\n \n          ...,\n \n          [[-0.0293,  0.0102,  0.0337],\n           [-0.0309, -0.0368, -0.0526],\n           [-0.0288,  0.0523,  0.0521]],\n \n          [[ 0.0410, -0.0124, -0.0012],\n           [ 0.0498, -0.0511,  0.0081],\n           [-0.0562,  0.0179, -0.0355]],\n \n          [[ 0.0386, -0.0509, -0.0478],\n           [-0.0164, -0.0406, -0.0098],\n           [-0.0429,  0.0384,  0.0412]]],\n \n \n         [[[ 0.0250,  0.0259,  0.0183],\n           [ 0.0499, -0.0271, -0.0442],\n           [ 0.0040,  0.0179,  0.0317]],\n \n          [[ 0.0115,  0.0146,  0.0086],\n           [-0.0174, -0.0045, -0.0462],\n           [ 0.0534, -0.0318,  0.0426]],\n \n          [[-0.0381, -0.0247, -0.0222],\n           [ 0.0101,  0.0440, -0.0202],\n           [-0.0457, -0.0038,  0.0426]],\n \n          ...,\n \n          [[ 0.0577, -0.0377,  0.0181],\n           [-0.0331, -0.0347,  0.0039],\n           [-0.0321, -0.0545, -0.0112]],\n \n          [[-0.0181, -0.0126,  0.0181],\n           [-0.0431, -0.0142,  0.0472],\n           [ 0.0563, -0.0423, -0.0200]],\n \n          [[ 0.0032,  0.0483,  0.0181],\n           [ 0.0095,  0.0293,  0.0265],\n           [-0.0125,  0.0040, -0.0212]]],\n \n \n         ...,\n \n \n         [[[-0.0499,  0.0074,  0.0110],\n           [ 0.0356, -0.0050, -0.0179],\n           [ 0.0249, -0.0585, -0.0086]],\n \n          [[-0.0094,  0.0113,  0.0474],\n           [-0.0319,  0.0150, -0.0065],\n           [-0.0012,  0.0091, -0.0043]],\n \n          [[-0.0189, -0.0276, -0.0406],\n           [ 0.0054,  0.0129,  0.0320],\n           [-0.0430,  0.0275, -0.0220]],\n \n          ...,\n \n          [[ 0.0020, -0.0243,  0.0162],\n           [ 0.0031, -0.0586, -0.0345],\n           [-0.0431,  0.0262,  0.0438]],\n \n          [[ 0.0292,  0.0285,  0.0053],\n           [ 0.0464,  0.0139, -0.0381],\n           [-0.0371,  0.0484, -0.0290]],\n \n          [[ 0.0120,  0.0405,  0.0269],\n           [-0.0526, -0.0542,  0.0219],\n           [-0.0403,  0.0482, -0.0033]]],\n \n \n         [[[ 0.0379, -0.0475,  0.0581],\n           [-0.0124,  0.0553, -0.0103],\n           [ 0.0169,  0.0126, -0.0122]],\n \n          [[ 0.0244, -0.0134, -0.0403],\n           [ 0.0501,  0.0327, -0.0057],\n           [ 0.0316, -0.0389, -0.0265]],\n \n          [[-0.0037,  0.0519, -0.0352],\n           [-0.0228,  0.0360, -0.0431],\n           [ 0.0450, -0.0327,  0.0580]],\n \n          ...,\n \n          [[ 0.0220,  0.0328, -0.0065],\n           [-0.0097, -0.0202, -0.0306],\n           [-0.0469,  0.0113, -0.0394]],\n \n          [[ 0.0546, -0.0211, -0.0036],\n           [-0.0483,  0.0376, -0.0179],\n           [ 0.0165, -0.0540,  0.0481]],\n \n          [[ 0.0075, -0.0385,  0.0469],\n           [-0.0145, -0.0401,  0.0400],\n           [ 0.0265,  0.0547, -0.0544]]],\n \n \n         [[[ 0.0552,  0.0459, -0.0025],\n           [ 0.0101, -0.0049,  0.0445],\n           [-0.0192, -0.0068, -0.0214]],\n \n          [[-0.0400,  0.0409,  0.0391],\n           [ 0.0212,  0.0327,  0.0034],\n           [ 0.0160,  0.0511, -0.0452]],\n \n          [[-0.0414,  0.0413, -0.0011],\n           [-0.0400,  0.0153, -0.0364],\n           [ 0.0399, -0.0023, -0.0076]],\n \n          ...,\n \n          [[ 0.0231,  0.0071, -0.0510],\n           [ 0.0448,  0.0478, -0.0514],\n           [ 0.0535, -0.0539,  0.0470]],\n \n          [[-0.0118, -0.0566, -0.0254],\n           [ 0.0245, -0.0021,  0.0265],\n           [-0.0281, -0.0255,  0.0366]],\n \n          [[-0.0059, -0.0232, -0.0202],\n           [-0.0327,  0.0190,  0.0306],\n           [-0.0485, -0.0176, -0.0158]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[ 2.8337e-02,  3.5270e-02,  3.0433e-02],\n           [ 4.9139e-03,  1.8059e-02,  2.9658e-02],\n           [-3.7444e-02, -1.5361e-02,  4.0178e-02]],\n \n          [[ 6.1916e-03,  3.9695e-02,  8.8587e-03],\n           [-1.2853e-02, -2.1037e-02,  2.7795e-02],\n           [-1.3899e-02,  5.1331e-03, -1.0921e-03]],\n \n          [[-1.2832e-02, -8.3122e-03, -1.5695e-02],\n           [ 2.4426e-02, -5.2607e-04,  2.6868e-02],\n           [-3.5637e-02,  5.2161e-03, -2.1233e-02]],\n \n          ...,\n \n          [[ 2.1772e-02, -2.3747e-02, -1.6550e-02],\n           [-2.1526e-02, -3.2019e-03, -1.5177e-02],\n           [-1.8024e-02, -1.6303e-02, -1.5071e-02]],\n \n          [[-1.0104e-02, -1.8339e-02, -1.3153e-03],\n           [ 2.7483e-02, -3.1376e-02, -8.8859e-03],\n           [ 2.7089e-02,  3.9402e-02, -2.2263e-02]],\n \n          [[-3.2224e-02, -3.1511e-02, -3.9681e-03],\n           [-2.5020e-02, -2.0561e-03,  3.1174e-03],\n           [ 2.9084e-02, -1.6335e-02,  1.2909e-02]]],\n \n \n         [[[ 1.2732e-02, -2.9956e-02, -3.5003e-03],\n           [ 2.3586e-02,  3.2078e-02,  6.7183e-04],\n           [-1.3639e-02, -2.5774e-02,  4.3463e-04]],\n \n          [[-3.7530e-02, -5.3937e-03,  1.0947e-02],\n           [ 3.1096e-02,  2.2382e-02, -8.7099e-03],\n           [ 3.9998e-02,  3.1804e-02,  3.3464e-02]],\n \n          [[ 2.8007e-02,  1.4704e-02, -2.0051e-03],\n           [ 3.5907e-02, -7.7660e-03, -3.2933e-02],\n           [ 4.1380e-02,  1.9921e-03, -2.9528e-02]],\n \n          ...,\n \n          [[-3.3939e-02,  2.2565e-02, -1.4364e-02],\n           [ 2.1992e-02,  2.0975e-02, -1.5174e-02],\n           [ 3.3071e-02,  3.8504e-02,  1.7267e-02]],\n \n          [[ 3.7223e-02, -3.4765e-02, -9.1053e-03],\n           [-5.2586e-03,  3.0431e-02,  5.1667e-03],\n           [ 1.9790e-02,  1.5686e-02, -1.5030e-02]],\n \n          [[ 7.9345e-03,  6.5912e-03,  4.3581e-03],\n           [ 3.4271e-04,  8.4918e-03, -2.1015e-02],\n           [-3.1571e-02, -3.6360e-02, -3.9548e-02]]],\n \n \n         [[[-1.5700e-02,  2.4622e-02, -2.9533e-02],\n           [ 3.1683e-02,  3.9156e-02,  1.8675e-02],\n           [ 2.6018e-03,  2.2092e-02,  2.6640e-02]],\n \n          [[ 6.3305e-03, -2.9075e-02,  8.4213e-03],\n           [ 2.0755e-02, -3.6724e-03,  1.7318e-02],\n           [-3.9732e-02, -2.4634e-03, -1.8926e-03]],\n \n          [[ 8.4369e-03, -9.2535e-03, -3.6718e-02],\n           [ 2.3774e-02, -2.9292e-02,  1.9840e-02],\n           [-4.1391e-03,  2.8863e-03, -9.6033e-03]],\n \n          ...,\n \n          [[-3.4732e-02,  4.1610e-02,  3.4149e-02],\n           [-1.7938e-04,  3.4866e-02, -2.8616e-02],\n           [-1.9829e-02,  1.8100e-02, -5.7876e-03]],\n \n          [[ 9.5828e-03, -3.7437e-02,  2.6193e-02],\n           [-3.5513e-02, -6.0650e-03, -3.1444e-02],\n           [ 2.0452e-02, -3.7085e-02, -3.5603e-02]],\n \n          [[ 1.9088e-02, -4.0295e-02,  3.0904e-02],\n           [-1.1607e-02, -1.1361e-03, -3.0526e-02],\n           [ 2.2802e-02,  3.6688e-02, -1.5806e-02]]],\n \n \n         ...,\n \n \n         [[[-3.2321e-02, -2.1213e-02, -2.3009e-02],\n           [-4.0430e-02, -1.0105e-02,  1.3283e-02],\n           [ 1.3477e-02,  3.6139e-02, -1.0701e-02]],\n \n          [[ 1.5536e-02,  9.5645e-03,  1.9796e-02],\n           [ 1.6843e-02,  1.8089e-02, -3.7288e-02],\n           [ 6.2430e-03,  3.2115e-03, -1.5957e-03]],\n \n          [[-3.7413e-03,  3.1503e-02,  3.4632e-02],\n           [ 2.0193e-02, -2.2639e-02,  2.5592e-02],\n           [-5.6481e-03,  1.9031e-02,  1.7268e-02]],\n \n          ...,\n \n          [[ 3.8171e-02, -3.3250e-04, -6.4613e-03],\n           [-6.7918e-03, -3.9667e-02, -9.2075e-03],\n           [ 2.6768e-02, -3.7603e-02, -3.1201e-02]],\n \n          [[ 1.1891e-02,  3.3552e-02, -4.0570e-02],\n           [ 5.6272e-03, -2.1957e-02, -6.8124e-03],\n           [ 8.8307e-03, -3.7966e-02,  2.1635e-02]],\n \n          [[-2.2137e-02, -3.9655e-02, -3.8961e-02],\n           [ 2.2977e-02, -2.8355e-02, -6.1791e-03],\n           [-3.0277e-02,  3.4437e-02,  3.1998e-02]]],\n \n \n         [[[-1.2492e-02, -1.9330e-02, -3.0969e-02],\n           [ 1.9946e-02,  1.1019e-02, -1.1957e-03],\n           [-6.1812e-03, -8.9803e-03,  3.8565e-02]],\n \n          [[ 3.5339e-03,  9.2282e-04,  2.1714e-02],\n           [-1.4592e-02, -1.4864e-02, -1.5195e-05],\n           [ 3.7537e-03,  5.8069e-03,  3.8879e-02]],\n \n          [[-3.3788e-02, -5.8717e-03, -3.6197e-02],\n           [ 1.6464e-02,  1.8203e-02,  3.2679e-02],\n           [-1.9230e-02, -6.6789e-03,  2.8421e-02]],\n \n          ...,\n \n          [[ 2.7266e-02,  3.0411e-02,  2.5198e-02],\n           [ 7.2153e-03, -2.5714e-02, -2.3414e-02],\n           [-1.3448e-02, -2.0236e-02,  9.9444e-03]],\n \n          [[-2.0890e-04,  3.9785e-02, -4.1325e-02],\n           [ 4.1518e-02, -3.5357e-02,  3.0800e-03],\n           [ 3.6304e-02, -1.8882e-02, -7.9623e-03]],\n \n          [[-2.8397e-02,  3.6215e-02, -1.9601e-02],\n           [-4.0999e-02, -1.7234e-02,  2.5216e-03],\n           [ 4.9975e-03, -9.7109e-03, -2.9509e-02]]],\n \n \n         [[[-9.6431e-03, -3.6498e-02,  1.6013e-02],\n           [ 2.0163e-02, -1.8675e-02, -9.6951e-03],\n           [ 1.4705e-02, -6.9202e-03, -3.1275e-02]],\n \n          [[ 1.4963e-02, -8.4347e-03, -1.6592e-02],\n           [-2.0479e-04,  9.9979e-03, -9.1459e-04],\n           [-3.1561e-02,  2.4861e-02, -1.3543e-02]],\n \n          [[-1.8140e-02, -8.6556e-03, -1.1403e-02],\n           [ 1.6082e-02,  2.2881e-02, -1.8160e-02],\n           [-2.3808e-02,  9.7504e-03, -2.1852e-02]],\n \n          ...,\n \n          [[-4.5095e-05, -2.3170e-03,  3.8222e-02],\n           [-1.2891e-02, -3.2514e-02, -6.7987e-03],\n           [-1.5298e-03,  3.2316e-02, -2.2013e-02]],\n \n          [[-2.9436e-02, -2.0414e-02, -9.2524e-03],\n           [ 1.5394e-02, -3.9899e-02, -1.3947e-02],\n           [ 1.1905e-02, -8.7281e-03, -3.9263e-02]],\n \n          [[ 1.2588e-02,  1.3405e-02,  2.1808e-03],\n           [-2.2377e-02,  3.6797e-02,  3.8221e-03],\n           [-3.4362e-02, -1.0505e-02,  3.0715e-02]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[-0.0241, -0.0291, -0.0277],\n           [ 0.0544,  0.0257,  0.0363],\n           [ 0.0545,  0.0025, -0.0554]],\n \n          [[-0.0101,  0.0164, -0.0461],\n           [ 0.0414, -0.0534, -0.0361],\n           [ 0.0374,  0.0095, -0.0091]],\n \n          [[ 0.0206,  0.0471, -0.0424],\n           [-0.0548, -0.0576,  0.0235],\n           [-0.0045,  0.0352,  0.0322]],\n \n          ...,\n \n          [[-0.0051, -0.0358,  0.0104],\n           [ 0.0154,  0.0460, -0.0300],\n           [ 0.0402, -0.0324, -0.0198]],\n \n          [[ 0.0294, -0.0487,  0.0302],\n           [ 0.0482,  0.0389,  0.0311],\n           [-0.0473,  0.0476,  0.0483]],\n \n          [[-0.0081, -0.0386, -0.0306],\n           [-0.0215,  0.0329, -0.0074],\n           [-0.0538,  0.0230, -0.0078]]],\n \n \n         [[[ 0.0454,  0.0352,  0.0517],\n           [-0.0280, -0.0142,  0.0448],\n           [-0.0515,  0.0455, -0.0309]],\n \n          [[ 0.0425,  0.0041,  0.0253],\n           [ 0.0403, -0.0467, -0.0350],\n           [ 0.0432,  0.0441,  0.0089]],\n \n          [[-0.0498,  0.0006, -0.0057],\n           [ 0.0324,  0.0126, -0.0385],\n           [ 0.0341, -0.0234,  0.0156]],\n \n          ...,\n \n          [[-0.0360,  0.0046,  0.0074],\n           [ 0.0464, -0.0539, -0.0094],\n           [-0.0270,  0.0234, -0.0118]],\n \n          [[ 0.0171,  0.0232, -0.0537],\n           [ 0.0195, -0.0019, -0.0507],\n           [ 0.0420,  0.0343,  0.0546]],\n \n          [[ 0.0254, -0.0457,  0.0029],\n           [-0.0055,  0.0571,  0.0448],\n           [ 0.0543, -0.0453,  0.0342]]],\n \n \n         [[[ 0.0390, -0.0404, -0.0332],\n           [-0.0154, -0.0487,  0.0505],\n           [-0.0285, -0.0109, -0.0488]],\n \n          [[ 0.0391,  0.0166,  0.0025],\n           [-0.0318, -0.0174,  0.0448],\n           [ 0.0359, -0.0162, -0.0018]],\n \n          [[ 0.0491,  0.0588, -0.0070],\n           [ 0.0372, -0.0168,  0.0363],\n           [-0.0004, -0.0214,  0.0378]],\n \n          ...,\n \n          [[ 0.0205, -0.0120, -0.0473],\n           [-0.0065, -0.0078,  0.0089],\n           [ 0.0584, -0.0579,  0.0233]],\n \n          [[ 0.0193, -0.0300,  0.0380],\n           [ 0.0260,  0.0036, -0.0186],\n           [ 0.0482,  0.0127, -0.0165]],\n \n          [[-0.0331, -0.0408, -0.0050],\n           [ 0.0190, -0.0315, -0.0106],\n           [-0.0307, -0.0486,  0.0186]]],\n \n \n         ...,\n \n \n         [[[ 0.0090, -0.0319,  0.0420],\n           [-0.0342,  0.0460, -0.0479],\n           [ 0.0231, -0.0332,  0.0324]],\n \n          [[-0.0133, -0.0390, -0.0082],\n           [-0.0419, -0.0181,  0.0223],\n           [ 0.0238, -0.0126,  0.0391]],\n \n          [[-0.0077, -0.0215,  0.0277],\n           [ 0.0277,  0.0438, -0.0008],\n           [-0.0065,  0.0027, -0.0501]],\n \n          ...,\n \n          [[ 0.0261, -0.0434, -0.0314],\n           [ 0.0571,  0.0094,  0.0332],\n           [ 0.0376, -0.0327, -0.0429]],\n \n          [[-0.0238,  0.0057,  0.0340],\n           [ 0.0266,  0.0586, -0.0123],\n           [ 0.0017,  0.0406, -0.0535]],\n \n          [[-0.0490, -0.0482,  0.0503],\n           [ 0.0322,  0.0435, -0.0219],\n           [ 0.0253, -0.0286,  0.0287]]],\n \n \n         [[[-0.0479,  0.0548,  0.0420],\n           [ 0.0082,  0.0434, -0.0217],\n           [-0.0279, -0.0585, -0.0325]],\n \n          [[ 0.0271, -0.0111,  0.0003],\n           [ 0.0156, -0.0549, -0.0445],\n           [ 0.0080, -0.0137, -0.0467]],\n \n          [[ 0.0015, -0.0427,  0.0536],\n           [ 0.0193,  0.0238, -0.0550],\n           [-0.0362,  0.0298,  0.0089]],\n \n          ...,\n \n          [[ 0.0112,  0.0163, -0.0498],\n           [-0.0351,  0.0345, -0.0404],\n           [ 0.0043,  0.0359,  0.0555]],\n \n          [[ 0.0024, -0.0537,  0.0552],\n           [-0.0558, -0.0428, -0.0195],\n           [-0.0573, -0.0126, -0.0412]],\n \n          [[ 0.0146,  0.0375,  0.0046],\n           [ 0.0419,  0.0410, -0.0229],\n           [-0.0399,  0.0355,  0.0289]]],\n \n \n         [[[ 0.0544,  0.0205,  0.0564],\n           [ 0.0445,  0.0435, -0.0418],\n           [ 0.0476, -0.0207, -0.0465]],\n \n          [[-0.0139,  0.0115,  0.0511],\n           [-0.0310, -0.0244,  0.0237],\n           [-0.0057, -0.0564, -0.0047]],\n \n          [[ 0.0319,  0.0251, -0.0015],\n           [-0.0548,  0.0526,  0.0585],\n           [ 0.0415, -0.0207, -0.0099]],\n \n          ...,\n \n          [[ 0.0424,  0.0169,  0.0118],\n           [ 0.0002, -0.0254, -0.0341],\n           [ 0.0407, -0.0195, -0.0474]],\n \n          [[-0.0361, -0.0478, -0.0383],\n           [ 0.0077, -0.0209,  0.0097],\n           [ 0.0146, -0.0283,  0.0015]],\n \n          [[ 0.0068,  0.0210, -0.0072],\n           [-0.0236,  0.0080, -0.0232],\n           [ 0.0285, -0.0399, -0.0526]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[ 0.0040,  0.0077,  0.0285],\n           [ 0.0337, -0.0370,  0.0202],\n           [-0.0098,  0.0289, -0.0192]],\n \n          [[ 0.0305,  0.0232, -0.0182],\n           [-0.0206,  0.0099, -0.0243],\n           [-0.0059, -0.0211,  0.0030]],\n \n          [[-0.0317, -0.0038,  0.0095],\n           [ 0.0363, -0.0352, -0.0309],\n           [ 0.0377, -0.0403,  0.0032]],\n \n          ...,\n \n          [[-0.0021, -0.0329,  0.0255],\n           [-0.0346,  0.0076, -0.0042],\n           [ 0.0158,  0.0145,  0.0240]],\n \n          [[-0.0069, -0.0110, -0.0224],\n           [-0.0361,  0.0209,  0.0359],\n           [ 0.0047,  0.0281,  0.0279]],\n \n          [[ 0.0074, -0.0410, -0.0350],\n           [-0.0078,  0.0339, -0.0403],\n           [ 0.0137, -0.0087,  0.0209]]],\n \n \n         [[[ 0.0027,  0.0388,  0.0270],\n           [ 0.0253,  0.0054,  0.0358],\n           [-0.0368,  0.0121, -0.0161]],\n \n          [[-0.0062,  0.0388, -0.0344],\n           [ 0.0221, -0.0414, -0.0004],\n           [-0.0290, -0.0039, -0.0358]],\n \n          [[-0.0401, -0.0388,  0.0147],\n           [ 0.0202,  0.0006,  0.0372],\n           [ 0.0403, -0.0151, -0.0405]],\n \n          ...,\n \n          [[-0.0307,  0.0029,  0.0316],\n           [-0.0228, -0.0333, -0.0280],\n           [ 0.0412,  0.0389,  0.0287]],\n \n          [[ 0.0249,  0.0078,  0.0414],\n           [ 0.0158,  0.0273,  0.0203],\n           [ 0.0264,  0.0088, -0.0081]],\n \n          [[ 0.0237, -0.0360, -0.0368],\n           [ 0.0191, -0.0111,  0.0153],\n           [-0.0275,  0.0307, -0.0383]]],\n \n \n         [[[-0.0218, -0.0037, -0.0268],\n           [ 0.0349,  0.0015,  0.0131],\n           [ 0.0051,  0.0145, -0.0296]],\n \n          [[ 0.0036, -0.0128,  0.0366],\n           [ 0.0209,  0.0126,  0.0274],\n           [-0.0329,  0.0007, -0.0202]],\n \n          [[-0.0231, -0.0247, -0.0392],\n           [ 0.0129, -0.0338, -0.0253],\n           [-0.0255, -0.0355,  0.0282]],\n \n          ...,\n \n          [[-0.0105,  0.0200, -0.0408],\n           [-0.0120,  0.0046, -0.0302],\n           [ 0.0057, -0.0206, -0.0317]],\n \n          [[-0.0278,  0.0269, -0.0044],\n           [-0.0412,  0.0189, -0.0123],\n           [-0.0125, -0.0042,  0.0234]],\n \n          [[ 0.0091,  0.0245,  0.0387],\n           [-0.0025,  0.0131,  0.0268],\n           [ 0.0048,  0.0187,  0.0340]]],\n \n \n         ...,\n \n \n         [[[ 0.0328, -0.0294, -0.0066],\n           [-0.0121,  0.0222,  0.0270],\n           [ 0.0064,  0.0040,  0.0284]],\n \n          [[-0.0037, -0.0296,  0.0046],\n           [-0.0118,  0.0415,  0.0054],\n           [-0.0179, -0.0094, -0.0100]],\n \n          [[-0.0009,  0.0392,  0.0306],\n           [ 0.0191,  0.0137,  0.0205],\n           [-0.0016, -0.0134, -0.0346]],\n \n          ...,\n \n          [[-0.0220,  0.0105,  0.0066],\n           [-0.0116,  0.0391,  0.0227],\n           [-0.0109, -0.0145,  0.0370]],\n \n          [[ 0.0161, -0.0062,  0.0161],\n           [-0.0351,  0.0156,  0.0323],\n           [ 0.0314, -0.0021, -0.0307]],\n \n          [[ 0.0333, -0.0254, -0.0177],\n           [-0.0385,  0.0230,  0.0258],\n           [ 0.0125, -0.0150, -0.0311]]],\n \n \n         [[[-0.0339,  0.0305, -0.0213],\n           [-0.0396, -0.0257,  0.0126],\n           [ 0.0285,  0.0288,  0.0225]],\n \n          [[ 0.0306, -0.0009,  0.0053],\n           [ 0.0318, -0.0246, -0.0112],\n           [ 0.0226,  0.0123,  0.0148]],\n \n          [[ 0.0081, -0.0394,  0.0109],\n           [-0.0082, -0.0379, -0.0316],\n           [ 0.0024, -0.0250,  0.0192]],\n \n          ...,\n \n          [[ 0.0303,  0.0021,  0.0379],\n           [-0.0153,  0.0138, -0.0379],\n           [ 0.0375, -0.0342,  0.0361]],\n \n          [[ 0.0334,  0.0212, -0.0269],\n           [-0.0222,  0.0054, -0.0190],\n           [ 0.0383,  0.0094, -0.0336]],\n \n          [[-0.0313,  0.0259, -0.0120],\n           [ 0.0123,  0.0212,  0.0225],\n           [ 0.0188,  0.0417, -0.0004]]],\n \n \n         [[[-0.0325, -0.0042,  0.0368],\n           [ 0.0061, -0.0191, -0.0210],\n           [-0.0042, -0.0235,  0.0065]],\n \n          [[ 0.0302, -0.0064,  0.0119],\n           [-0.0169,  0.0079,  0.0269],\n           [-0.0381,  0.0262,  0.0321]],\n \n          [[ 0.0381, -0.0092,  0.0285],\n           [-0.0241,  0.0356, -0.0236],\n           [-0.0009, -0.0236, -0.0150]],\n \n          ...,\n \n          [[-0.0070, -0.0028,  0.0379],\n           [-0.0027,  0.0126, -0.0325],\n           [-0.0110,  0.0262,  0.0367]],\n \n          [[ 0.0157, -0.0228, -0.0290],\n           [ 0.0089,  0.0295, -0.0227],\n           [ 0.0093, -0.0127,  0.0094]],\n \n          [[-0.0314,  0.0081, -0.0092],\n           [-0.0412, -0.0194, -0.0161],\n           [-0.0236, -0.0004,  0.0025]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[[[ 0.0145,  0.0012, -0.0209],\n           [-0.0217, -0.0186,  0.0290],\n           [-0.0151, -0.0355,  0.0210]],\n \n          [[-0.0272,  0.0396, -0.0164],\n           [ 0.0382,  0.0164, -0.0175],\n           [-0.0369, -0.0094,  0.0077]],\n \n          [[ 0.0407, -0.0299,  0.0294],\n           [ 0.0168,  0.0195,  0.0089],\n           [ 0.0399,  0.0157, -0.0416]],\n \n          ...,\n \n          [[-0.0012, -0.0247, -0.0211],\n           [-0.0094, -0.0105, -0.0290],\n           [-0.0291,  0.0040, -0.0227]],\n \n          [[ 0.0069,  0.0389, -0.0264],\n           [ 0.0326, -0.0220,  0.0083],\n           [ 0.0084,  0.0093,  0.0231]],\n \n          [[ 0.0343, -0.0128,  0.0133],\n           [ 0.0010,  0.0060,  0.0055],\n           [ 0.0288,  0.0356, -0.0393]]],\n \n \n         [[[-0.0109, -0.0395, -0.0174],\n           [ 0.0332,  0.0215, -0.0253],\n           [ 0.0084, -0.0358, -0.0146]],\n \n          [[-0.0203, -0.0114, -0.0046],\n           [-0.0291, -0.0277,  0.0050],\n           [-0.0094,  0.0342, -0.0262]],\n \n          [[-0.0158, -0.0242, -0.0348],\n           [-0.0174,  0.0045, -0.0276],\n           [ 0.0278, -0.0186,  0.0268]],\n \n          ...,\n \n          [[-0.0100,  0.0163,  0.0148],\n           [-0.0296, -0.0032,  0.0073],\n           [ 0.0268,  0.0320,  0.0384]],\n \n          [[-0.0261, -0.0115, -0.0203],\n           [-0.0077, -0.0076, -0.0262],\n           [-0.0003,  0.0404, -0.0227]],\n \n          [[ 0.0170,  0.0154, -0.0126],\n           [-0.0339,  0.0134,  0.0182],\n           [ 0.0068, -0.0404,  0.0286]]],\n \n \n         [[[-0.0370,  0.0264,  0.0329],\n           [ 0.0407,  0.0213, -0.0027],\n           [-0.0154, -0.0083,  0.0237]],\n \n          [[-0.0151,  0.0027,  0.0224],\n           [-0.0131, -0.0108,  0.0256],\n           [-0.0043,  0.0040,  0.0161]],\n \n          [[ 0.0025,  0.0339, -0.0034],\n           [-0.0038, -0.0295,  0.0368],\n           [-0.0232,  0.0303, -0.0235]],\n \n          ...,\n \n          [[-0.0334, -0.0285,  0.0141],\n           [ 0.0008, -0.0345, -0.0009],\n           [-0.0043,  0.0071,  0.0148]],\n \n          [[ 0.0391, -0.0337, -0.0179],\n           [ 0.0113,  0.0399,  0.0150],\n           [-0.0171, -0.0066, -0.0045]],\n \n          [[ 0.0139,  0.0092,  0.0387],\n           [ 0.0136, -0.0248,  0.0009],\n           [-0.0180,  0.0396,  0.0403]]],\n \n \n         ...,\n \n \n         [[[ 0.0249, -0.0172, -0.0058],\n           [ 0.0316,  0.0278,  0.0292],\n           [ 0.0084, -0.0284,  0.0247]],\n \n          [[ 0.0314,  0.0119, -0.0218],\n           [ 0.0109,  0.0214,  0.0323],\n           [ 0.0132, -0.0363, -0.0084]],\n \n          [[-0.0138,  0.0297, -0.0305],\n           [-0.0365, -0.0110, -0.0339],\n           [-0.0340, -0.0017, -0.0204]],\n \n          ...,\n \n          [[-0.0244, -0.0374, -0.0002],\n           [ 0.0213,  0.0171, -0.0282],\n           [ 0.0073,  0.0312,  0.0069]],\n \n          [[ 0.0018, -0.0350,  0.0316],\n           [-0.0254, -0.0063, -0.0237],\n           [-0.0100,  0.0388, -0.0041]],\n \n          [[ 0.0160, -0.0394,  0.0326],\n           [-0.0105,  0.0416, -0.0012],\n           [ 0.0105,  0.0210,  0.0069]]],\n \n \n         [[[-0.0288, -0.0363,  0.0281],\n           [ 0.0269, -0.0142, -0.0010],\n           [-0.0403, -0.0235, -0.0094]],\n \n          [[-0.0061, -0.0302,  0.0078],\n           [ 0.0369,  0.0155,  0.0061],\n           [-0.0073,  0.0403, -0.0232]],\n \n          [[-0.0321, -0.0077, -0.0084],\n           [ 0.0131,  0.0150,  0.0299],\n           [ 0.0326, -0.0404,  0.0339]],\n \n          ...,\n \n          [[ 0.0156, -0.0128,  0.0184],\n           [-0.0330,  0.0160,  0.0383],\n           [-0.0131, -0.0017,  0.0216]],\n \n          [[ 0.0331,  0.0023, -0.0045],\n           [ 0.0042, -0.0393,  0.0148],\n           [ 0.0319,  0.0149, -0.0276]],\n \n          [[-0.0190, -0.0267,  0.0279],\n           [ 0.0055, -0.0036, -0.0211],\n           [ 0.0376,  0.0093,  0.0155]]],\n \n \n         [[[-0.0339,  0.0040, -0.0392],\n           [ 0.0166,  0.0085, -0.0321],\n           [-0.0326,  0.0158, -0.0138]],\n \n          [[ 0.0193, -0.0152, -0.0038],\n           [-0.0116, -0.0205, -0.0117],\n           [ 0.0033, -0.0331, -0.0076]],\n \n          [[ 0.0127,  0.0001,  0.0407],\n           [ 0.0042,  0.0113, -0.0358],\n           [-0.0278,  0.0019,  0.0007]],\n \n          ...,\n \n          [[-0.0251,  0.0019,  0.0360],\n           [ 0.0377,  0.0208, -0.0044],\n           [ 0.0384, -0.0237,  0.0057]],\n \n          [[-0.0108,  0.0245, -0.0179],\n           [ 0.0094,  0.0207,  0.0001],\n           [ 0.0130, -0.0031,  0.0077]],\n \n          [[ 0.0357,  0.0129,  0.0185],\n           [ 0.0075,  0.0396,  0.0074],\n           [-0.0249,  0.0117, -0.0026]]]], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([[ 5.5210e-02,  9.7829e-02,  2.7488e-03, -8.6737e-02, -8.4092e-04,\n           1.2450e-01,  5.7791e-02, -1.1812e-02,  8.5654e-02,  5.1411e-02,\n          -5.5057e-02,  5.1466e-02, -5.0566e-02,  5.9268e-03,  2.6137e-02,\n          -5.0316e-02, -5.8311e-02,  1.1336e-01,  6.3686e-02, -8.4363e-02,\n           7.1823e-02,  1.0864e-01, -2.9230e-02, -8.1355e-03, -7.4188e-02,\n           1.9957e-02, -7.6406e-02,  1.0931e-01, -1.1820e-01, -6.0446e-02,\n           9.0741e-02,  1.9881e-02, -6.2861e-02,  4.0602e-02,  7.7083e-02,\n           5.9999e-02, -8.4335e-02, -4.1759e-02, -1.7270e-03,  5.3156e-02,\n          -4.1556e-02,  6.1433e-02, -1.2271e-01, -2.6337e-02, -9.4678e-02,\n          -6.9788e-03,  2.7994e-02,  4.7526e-02,  4.9794e-02, -4.2651e-02,\n           7.0409e-02,  2.5003e-02, -9.2142e-02, -1.2024e-01,  1.1128e-01,\n          -6.9197e-04,  6.6155e-02,  8.0000e-02, -2.8692e-02, -1.6699e-02,\n          -6.6870e-03, -6.6249e-02,  7.6853e-03,  1.6561e-03],\n         [ 8.6269e-02,  9.8940e-02,  1.2200e-01,  9.6854e-02,  8.6025e-02,\n          -1.1761e-01, -1.1737e-01,  1.1568e-02,  1.6198e-02,  9.3976e-03,\n          -1.1894e-01, -3.2832e-02, -4.7418e-02, -6.1798e-02,  8.2330e-02,\n           5.9022e-03, -5.4146e-02, -1.1878e-01, -1.1440e-01,  3.2247e-02,\n          -1.1554e-01, -7.8047e-02, -7.7931e-02,  1.6865e-04, -6.9754e-02,\n           1.1602e-01, -4.9187e-03, -6.8066e-02,  1.0475e-02,  3.8899e-02,\n           5.5051e-02,  1.1714e-02, -5.5243e-03, -2.6110e-03, -7.7519e-02,\n          -9.1839e-02, -5.9556e-02,  2.1761e-02, -2.9509e-02,  1.1133e-01,\n           9.5002e-02,  6.4814e-03,  1.2330e-01,  9.5975e-02,  8.8644e-02,\n           6.4880e-02,  1.2791e-02,  4.2371e-02,  5.6979e-02, -2.6584e-03,\n          -7.8879e-02, -7.7835e-02,  8.8687e-02, -9.7685e-02,  1.1917e-02,\n          -7.2266e-02, -9.7360e-02,  2.5729e-02, -8.6164e-03, -1.6742e-03,\n          -3.2419e-02,  1.0705e-01,  6.5322e-02,  1.3353e-02],\n         [ 6.3468e-02,  4.2157e-02,  9.7707e-02,  5.8698e-02, -2.6615e-02,\n           7.4666e-02, -2.4101e-02,  3.9948e-02,  4.7470e-02, -1.1549e-01,\n          -1.2085e-01,  2.4089e-02, -5.6019e-03,  9.0461e-02,  3.5216e-02,\n          -1.1403e-01, -6.9681e-02,  4.6667e-02, -1.2248e-02,  1.1752e-01,\n           8.1198e-02, -1.1706e-01,  1.0597e-01,  9.6104e-02,  9.5950e-02,\n          -4.8565e-02, -4.4887e-02, -1.1608e-01, -8.6585e-02, -9.7778e-02,\n           1.1381e-01,  8.3044e-02, -3.9822e-02, -2.3679e-02, -4.7262e-02,\n           8.4447e-02,  1.6147e-02,  1.9110e-02, -9.1636e-03, -5.5357e-02,\n           5.7038e-02, -3.9311e-02, -5.1130e-02,  3.1590e-02, -1.1148e-01,\n          -8.5074e-03,  6.5281e-02, -1.0180e-01,  4.4132e-02, -7.5952e-02,\n          -2.4437e-02,  4.2682e-03, -3.9789e-02,  1.7698e-02,  3.2245e-02,\n           1.0237e-01, -1.1631e-02,  1.0154e-01, -1.0085e-01, -5.5593e-02,\n           6.7445e-02,  5.7006e-02, -1.0191e-01,  3.4119e-02],\n         [-2.4838e-02, -1.5754e-02, -6.5472e-02, -1.8251e-02,  5.1066e-02,\n          -6.3478e-02,  3.0891e-02,  1.0608e-02, -8.7090e-02,  8.5924e-02,\n           5.9885e-03,  6.9048e-02, -1.0422e-01,  6.8173e-02, -8.2347e-02,\n           8.7536e-02, -8.7156e-02,  6.1310e-02, -4.3322e-03,  8.5455e-02,\n          -6.8057e-02, -1.1645e-01,  1.2303e-01, -1.1447e-01,  7.4991e-02,\n          -2.6063e-02,  7.3939e-02, -7.5204e-02, -4.5018e-02, -1.0053e-01,\n           1.6435e-02,  1.1547e-01,  4.4885e-02,  2.1682e-02, -6.9893e-03,\n          -1.0534e-01,  1.1711e-01,  3.9145e-02,  1.1009e-01, -1.0910e-01,\n          -1.2984e-02,  3.9087e-02, -8.0979e-02,  5.3084e-02, -9.9140e-03,\n          -1.0230e-01,  5.9834e-02, -1.7134e-02, -1.0048e-01, -8.4858e-02,\n           1.9076e-02, -1.8193e-02,  1.1874e-01,  1.0320e-01, -4.1874e-03,\n           1.6293e-02,  1.0419e-01, -5.1416e-02, -6.9841e-02,  1.1464e-01,\n           9.4416e-02, -1.5929e-02, -2.9438e-02,  2.5961e-02],\n         [-9.8735e-02,  1.2314e-01, -1.2226e-01,  6.5345e-04, -4.5539e-02,\n          -1.2293e-01,  2.1672e-03, -3.1872e-02,  8.6402e-02, -2.0896e-02,\n           5.6927e-02,  3.3246e-02, -1.1259e-02, -7.6584e-02, -5.7867e-02,\n          -5.4193e-03, -8.6622e-02, -6.9199e-03, -1.0539e-01, -1.2058e-01,\n          -2.4847e-02,  9.7081e-03, -1.0895e-01, -5.9427e-02,  3.1479e-02,\n          -2.2698e-02, -7.8161e-03,  1.1794e-01,  1.1643e-01, -6.5381e-02,\n           5.9033e-02,  1.1472e-01,  1.2251e-01, -4.4681e-02,  5.7805e-02,\n           5.8992e-02,  8.1384e-02,  3.8573e-02, -7.1342e-02,  4.2245e-02,\n           7.4955e-02,  8.9129e-03,  6.2258e-02,  8.4979e-02,  7.6283e-02,\n          -3.3610e-02,  8.5438e-02,  9.0122e-02, -7.2568e-02, -2.2489e-02,\n           4.2876e-02,  9.4226e-02,  1.1271e-01, -7.5491e-02, -4.9402e-02,\n          -1.1329e-01,  2.1360e-02, -8.6762e-02,  6.0954e-02, -9.4835e-02,\n          -3.0453e-02,  1.0265e-01,  1.6993e-02,  3.2183e-02],\n         [-1.2072e-01,  6.7452e-02,  2.7980e-02,  9.3430e-02,  5.8195e-02,\n           2.0110e-02,  7.0251e-02, -7.8226e-02,  4.3675e-02, -9.5043e-02,\n           2.4128e-02,  1.2159e-01, -3.5050e-02,  2.1757e-02, -4.8474e-02,\n           2.7984e-02, -1.0920e-01,  8.2275e-02, -5.7947e-02, -6.9690e-02,\n           5.3724e-02,  9.9005e-02, -6.7177e-02,  9.5304e-02,  4.5614e-03,\n          -1.1055e-01,  3.2765e-04, -1.0126e-01, -6.9766e-02, -1.1074e-01,\n           1.9816e-02,  8.9036e-02,  1.1538e-01, -6.8814e-02,  1.9521e-02,\n           8.2557e-02,  8.7777e-02,  2.5002e-02, -3.9982e-02,  2.6102e-02,\n          -9.2791e-02,  3.6672e-02,  4.4752e-02, -6.7734e-03, -1.1189e-01,\n           5.7044e-02, -5.4092e-02, -1.0247e-01,  1.0223e-01,  1.2261e-04,\n           2.7888e-02, -7.1429e-02,  5.0370e-02,  4.6350e-03, -8.7648e-02,\n           1.0254e-01,  2.9729e-02, -1.4177e-02, -4.2733e-03,  4.3825e-02,\n           7.4253e-02, -2.8663e-02, -3.4282e-02,  1.7385e-02],\n         [-1.1996e-01,  1.5100e-02,  8.1616e-02,  1.0409e-01, -7.0433e-02,\n          -9.2761e-02, -3.6266e-02,  5.5478e-02,  5.6075e-02, -3.3389e-02,\n           5.6271e-02, -1.5585e-02,  6.1576e-02, -6.9231e-03,  1.2468e-01,\n           2.4286e-02, -7.1608e-02, -6.0008e-03,  1.0268e-01, -9.0929e-02,\n           3.7141e-02,  6.6746e-02, -1.0113e-01, -1.1481e-02,  9.9904e-02,\n           5.8444e-02, -9.8802e-02,  3.6285e-02,  9.1722e-02, -8.2769e-02,\n          -1.3017e-02,  7.1636e-03, -2.0510e-02, -4.6194e-02, -1.0042e-01,\n           1.0504e-01,  9.4630e-02,  3.4471e-02,  2.9597e-02,  1.0544e-01,\n          -1.1166e-01,  6.1609e-02,  3.0586e-02, -1.0072e-01,  6.5069e-03,\n           4.9297e-02,  5.4376e-03,  1.2137e-01,  4.0310e-02, -7.0450e-02,\n          -1.1714e-01, -5.0076e-02,  9.6213e-02, -3.5090e-02,  1.5005e-02,\n           8.0658e-02,  1.8430e-02, -5.2927e-02, -1.2234e-01, -1.1911e-01,\n          -8.7206e-02, -9.0885e-02, -5.4706e-02,  3.9535e-02],\n         [ 7.3289e-02,  1.7449e-02,  1.8199e-02, -1.1893e-01,  7.1370e-02,\n           3.9558e-02,  9.1465e-02, -1.5273e-02, -6.5442e-02, -2.1494e-02,\n           2.6800e-02, -5.0601e-03, -3.2893e-02, -3.8355e-02,  5.4631e-02,\n          -6.3377e-02,  1.3537e-02,  5.9170e-03,  1.2230e-01,  1.0156e-01,\n           1.0951e-01,  1.0904e-01, -5.0563e-02,  3.7619e-02,  5.5278e-02,\n          -1.1211e-01,  1.1668e-02,  6.0313e-02, -6.2048e-02, -1.5187e-02,\n          -1.1268e-01,  7.3030e-02, -1.1252e-02,  1.2310e-01,  6.1998e-02,\n          -8.0504e-02, -5.0299e-02,  4.4937e-03, -3.4525e-02, -1.7573e-02,\n          -3.2462e-02, -5.3200e-02, -1.0854e-01, -1.7284e-02, -4.6419e-02,\n           5.7927e-02, -7.5737e-02, -3.8780e-02,  5.0081e-02, -2.6062e-02,\n          -5.5457e-02,  2.4915e-02, -6.1342e-02, -1.2584e-02,  6.5335e-02,\n           1.2017e-02,  3.7988e-03,  1.4850e-02,  1.1965e-01,  1.9563e-02,\n           3.5009e-02, -4.1248e-02, -8.3791e-03,  5.3152e-02],\n         [-6.1068e-02,  1.0284e-01,  1.0358e-01, -1.2462e-01,  8.6831e-03,\n          -2.5277e-02,  7.9543e-03, -6.2351e-02,  1.1552e-01,  4.4930e-02,\n          -8.6816e-02, -5.0735e-02,  8.7737e-02, -5.6788e-02, -1.0252e-01,\n          -1.0377e-01, -9.7738e-02,  7.9293e-02,  3.0326e-02, -1.0345e-01,\n           9.7486e-02, -4.7487e-02,  1.1758e-01,  1.3815e-02, -3.2639e-02,\n           2.5318e-02,  7.5222e-03, -8.8261e-02, -3.5889e-02, -6.1989e-02,\n          -5.3629e-02, -9.8200e-02,  4.2827e-02,  6.5683e-03, -8.2541e-02,\n          -1.1867e-01, -1.2272e-01, -1.2286e-01,  4.2258e-02,  1.1990e-01,\n           4.5351e-02, -9.0595e-02, -1.6714e-02,  8.3403e-03, -6.4821e-02,\n           2.7419e-02, -7.7595e-02,  7.9589e-02, -2.3821e-02,  6.7130e-02,\n          -9.5001e-02, -5.6415e-02, -6.8002e-02,  9.4308e-02,  8.1844e-03,\n          -1.0921e-01,  1.0071e-01,  8.7320e-02,  1.1040e-01,  3.3378e-03,\n           4.7054e-03, -1.1389e-01, -7.8235e-03,  5.4575e-02],\n         [ 1.0594e-01, -1.8394e-04,  3.3984e-02,  7.8789e-02,  2.0149e-02,\n          -8.6852e-02,  2.7508e-02, -1.1167e-01,  3.5767e-02, -4.6590e-02,\n          -2.2032e-02, -3.7001e-02,  8.9169e-02, -6.6498e-02, -2.7583e-02,\n           5.4401e-02,  6.8292e-02,  3.3670e-02,  5.5593e-02,  7.6303e-02,\n           8.3843e-02,  9.6720e-03,  7.4348e-02,  3.8533e-02,  1.9216e-02,\n           3.3054e-02,  1.1216e-01,  1.0951e-01, -6.2357e-02, -1.2123e-01,\n          -1.0982e-01, -3.3488e-03, -1.9340e-02,  8.0296e-02,  1.3675e-02,\n          -1.0343e-01,  1.2323e-01, -3.6348e-02,  5.5949e-02,  1.2025e-01,\n          -6.8322e-02, -3.8604e-02, -7.9782e-02, -1.2032e-01, -2.1871e-02,\n          -2.0329e-02, -4.0397e-02,  5.0785e-02, -5.2515e-03, -1.9741e-02,\n           3.7357e-02,  2.5932e-02, -7.6796e-02,  5.3971e-02, -6.0779e-03,\n           1.0648e-01, -7.1074e-02, -4.7192e-02, -1.6232e-02, -7.5574e-02,\n           3.7427e-03,  7.0625e-02,  1.2363e-02, -1.5222e-02]],\n        requires_grad=True),\n Parameter containing:\n tensor([-0.0373, -0.0579, -0.0180, -0.0407,  0.0361,  0.0099, -0.0611, -0.0120,\n         -0.0742, -0.0217], requires_grad=True)]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T07:26:28.196189200Z",
     "start_time": "2023-06-17T07:26:28.104873300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "0.001"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T07:26:29.215725700Z",
     "start_time": "2023-06-17T07:26:29.184839400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/500] Loss: 1.7244\n",
      "Epoch [1/100], Step [200/500] Loss: 1.6039\n",
      "Epoch [1/100], Step [300/500] Loss: 1.3381\n",
      "Epoch [1/100], Step [400/500] Loss: 1.0633\n",
      "Epoch [1/100], Step [500/500] Loss: 0.9804\n",
      "Epoch [2/100], Step [100/500] Loss: 1.0611\n",
      "Epoch [2/100], Step [200/500] Loss: 1.0486\n",
      "Epoch [2/100], Step [300/500] Loss: 1.0326\n",
      "Epoch [2/100], Step [400/500] Loss: 1.0638\n",
      "Epoch [2/100], Step [500/500] Loss: 0.9514\n",
      "Epoch [3/100], Step [100/500] Loss: 0.9639\n",
      "Epoch [3/100], Step [200/500] Loss: 0.9611\n",
      "Epoch [3/100], Step [300/500] Loss: 0.9702\n",
      "Epoch [3/100], Step [400/500] Loss: 0.9309\n",
      "Epoch [3/100], Step [500/500] Loss: 0.9250\n",
      "Epoch [4/100], Step [100/500] Loss: 0.7645\n",
      "Epoch [4/100], Step [200/500] Loss: 0.8019\n",
      "Epoch [4/100], Step [300/500] Loss: 0.6126\n",
      "Epoch [4/100], Step [400/500] Loss: 0.7251\n",
      "Epoch [4/100], Step [500/500] Loss: 0.6817\n",
      "Epoch [5/100], Step [100/500] Loss: 0.7422\n",
      "Epoch [5/100], Step [200/500] Loss: 0.6596\n",
      "Epoch [5/100], Step [300/500] Loss: 0.9199\n",
      "Epoch [5/100], Step [400/500] Loss: 0.8188\n",
      "Epoch [5/100], Step [500/500] Loss: 0.8154\n",
      "Epoch [6/100], Step [100/500] Loss: 0.5024\n",
      "Epoch [6/100], Step [200/500] Loss: 0.5268\n",
      "Epoch [6/100], Step [300/500] Loss: 0.5872\n",
      "Epoch [6/100], Step [400/500] Loss: 0.6053\n",
      "Epoch [6/100], Step [500/500] Loss: 0.6308\n",
      "Epoch [7/100], Step [100/500] Loss: 0.5216\n",
      "Epoch [7/100], Step [200/500] Loss: 0.5438\n",
      "Epoch [7/100], Step [300/500] Loss: 0.6579\n",
      "Epoch [7/100], Step [400/500] Loss: 0.6234\n",
      "Epoch [7/100], Step [500/500] Loss: 0.7759\n",
      "Epoch [8/100], Step [100/500] Loss: 0.5780\n",
      "Epoch [8/100], Step [200/500] Loss: 0.6252\n",
      "Epoch [8/100], Step [300/500] Loss: 0.8755\n",
      "Epoch [8/100], Step [400/500] Loss: 0.5266\n",
      "Epoch [8/100], Step [500/500] Loss: 0.4948\n",
      "Epoch [9/100], Step [100/500] Loss: 0.6535\n",
      "Epoch [9/100], Step [200/500] Loss: 0.4802\n",
      "Epoch [9/100], Step [300/500] Loss: 0.3895\n",
      "Epoch [9/100], Step [400/500] Loss: 0.6419\n",
      "Epoch [9/100], Step [500/500] Loss: 0.4760\n",
      "Epoch [10/100], Step [100/500] Loss: 0.4819\n",
      "Epoch [10/100], Step [200/500] Loss: 0.5351\n",
      "Epoch [10/100], Step [300/500] Loss: 0.4309\n",
      "Epoch [10/100], Step [400/500] Loss: 0.3934\n",
      "Epoch [10/100], Step [500/500] Loss: 0.5818\n",
      "Epoch [11/100], Step [100/500] Loss: 0.5776\n",
      "Epoch [11/100], Step [200/500] Loss: 0.5273\n",
      "Epoch [11/100], Step [300/500] Loss: 0.3965\n",
      "Epoch [11/100], Step [400/500] Loss: 0.4848\n",
      "Epoch [11/100], Step [500/500] Loss: 0.4018\n",
      "Epoch [12/100], Step [100/500] Loss: 0.5376\n",
      "Epoch [12/100], Step [200/500] Loss: 0.4508\n",
      "Epoch [12/100], Step [300/500] Loss: 0.4768\n",
      "Epoch [12/100], Step [400/500] Loss: 0.4133\n",
      "Epoch [12/100], Step [500/500] Loss: 0.4874\n",
      "Epoch [13/100], Step [100/500] Loss: 0.4520\n",
      "Epoch [13/100], Step [200/500] Loss: 0.4025\n",
      "Epoch [13/100], Step [300/500] Loss: 0.3598\n",
      "Epoch [13/100], Step [400/500] Loss: 0.3637\n",
      "Epoch [13/100], Step [500/500] Loss: 0.3557\n",
      "Epoch [14/100], Step [100/500] Loss: 0.4491\n",
      "Epoch [14/100], Step [200/500] Loss: 0.3205\n",
      "Epoch [14/100], Step [300/500] Loss: 0.4641\n",
      "Epoch [14/100], Step [400/500] Loss: 0.3018\n",
      "Epoch [14/100], Step [500/500] Loss: 0.4360\n",
      "Epoch [15/100], Step [100/500] Loss: 0.3998\n",
      "Epoch [15/100], Step [200/500] Loss: 0.4755\n",
      "Epoch [15/100], Step [300/500] Loss: 0.5612\n",
      "Epoch [15/100], Step [400/500] Loss: 0.3403\n",
      "Epoch [15/100], Step [500/500] Loss: 0.4323\n",
      "Epoch [16/100], Step [100/500] Loss: 0.3694\n",
      "Epoch [16/100], Step [200/500] Loss: 0.4905\n",
      "Epoch [16/100], Step [300/500] Loss: 0.4996\n",
      "Epoch [16/100], Step [400/500] Loss: 0.4258\n",
      "Epoch [16/100], Step [500/500] Loss: 0.4861\n",
      "Epoch [17/100], Step [100/500] Loss: 0.3131\n",
      "Epoch [17/100], Step [200/500] Loss: 0.3086\n",
      "Epoch [17/100], Step [300/500] Loss: 0.3691\n",
      "Epoch [17/100], Step [400/500] Loss: 0.3193\n",
      "Epoch [17/100], Step [500/500] Loss: 0.3361\n",
      "Epoch [18/100], Step [100/500] Loss: 0.3595\n",
      "Epoch [18/100], Step [200/500] Loss: 0.3346\n",
      "Epoch [18/100], Step [300/500] Loss: 0.3586\n",
      "Epoch [18/100], Step [400/500] Loss: 0.2836\n",
      "Epoch [18/100], Step [500/500] Loss: 0.3452\n",
      "Epoch [19/100], Step [100/500] Loss: 0.2877\n",
      "Epoch [19/100], Step [200/500] Loss: 0.4058\n",
      "Epoch [19/100], Step [300/500] Loss: 0.3509\n",
      "Epoch [19/100], Step [400/500] Loss: 0.3629\n",
      "Epoch [19/100], Step [500/500] Loss: 0.3699\n",
      "The new learning rate is 0.0005\n",
      "Epoch [20/100], Step [100/500] Loss: 0.2980\n",
      "Epoch [20/100], Step [200/500] Loss: 0.3029\n",
      "Epoch [20/100], Step [300/500] Loss: 0.4164\n",
      "Epoch [20/100], Step [400/500] Loss: 0.3449\n",
      "Epoch [20/100], Step [500/500] Loss: 0.3049\n",
      "Epoch [21/100], Step [100/500] Loss: 0.2028\n",
      "Epoch [21/100], Step [200/500] Loss: 0.2690\n",
      "Epoch [21/100], Step [300/500] Loss: 0.2810\n",
      "Epoch [21/100], Step [400/500] Loss: 0.4011\n",
      "Epoch [21/100], Step [500/500] Loss: 0.3517\n",
      "Epoch [22/100], Step [100/500] Loss: 0.2546\n",
      "Epoch [22/100], Step [200/500] Loss: 0.2425\n",
      "Epoch [22/100], Step [300/500] Loss: 0.2832\n",
      "Epoch [22/100], Step [400/500] Loss: 0.3985\n",
      "Epoch [22/100], Step [500/500] Loss: 0.3278\n",
      "Epoch [23/100], Step [100/500] Loss: 0.2776\n",
      "Epoch [23/100], Step [200/500] Loss: 0.2471\n",
      "Epoch [23/100], Step [300/500] Loss: 0.3091\n",
      "Epoch [23/100], Step [400/500] Loss: 0.3061\n",
      "Epoch [23/100], Step [500/500] Loss: 0.2065\n",
      "Epoch [24/100], Step [100/500] Loss: 0.2133\n",
      "Epoch [24/100], Step [200/500] Loss: 0.2043\n",
      "Epoch [24/100], Step [300/500] Loss: 0.3538\n",
      "Epoch [24/100], Step [400/500] Loss: 0.3175\n",
      "Epoch [24/100], Step [500/500] Loss: 0.2547\n",
      "Epoch [25/100], Step [100/500] Loss: 0.2788\n",
      "Epoch [25/100], Step [200/500] Loss: 0.1842\n",
      "Epoch [25/100], Step [300/500] Loss: 0.3320\n",
      "Epoch [25/100], Step [400/500] Loss: 0.4059\n",
      "Epoch [25/100], Step [500/500] Loss: 0.2491\n",
      "Epoch [26/100], Step [100/500] Loss: 0.1919\n",
      "Epoch [26/100], Step [200/500] Loss: 0.2382\n",
      "Epoch [26/100], Step [300/500] Loss: 0.2231\n",
      "Epoch [26/100], Step [400/500] Loss: 0.2976\n",
      "Epoch [26/100], Step [500/500] Loss: 0.2512\n",
      "Epoch [27/100], Step [100/500] Loss: 0.4310\n",
      "Epoch [27/100], Step [200/500] Loss: 0.2624\n",
      "Epoch [27/100], Step [300/500] Loss: 0.2856\n",
      "Epoch [27/100], Step [400/500] Loss: 0.1671\n",
      "Epoch [27/100], Step [500/500] Loss: 0.3507\n",
      "Epoch [28/100], Step [100/500] Loss: 0.2260\n",
      "Epoch [28/100], Step [200/500] Loss: 0.3032\n",
      "Epoch [28/100], Step [300/500] Loss: 0.1538\n",
      "Epoch [28/100], Step [400/500] Loss: 0.2465\n",
      "Epoch [28/100], Step [500/500] Loss: 0.2754\n",
      "Epoch [29/100], Step [100/500] Loss: 0.1663\n",
      "Epoch [29/100], Step [200/500] Loss: 0.1886\n",
      "Epoch [29/100], Step [300/500] Loss: 0.3100\n",
      "Epoch [29/100], Step [400/500] Loss: 0.3783\n",
      "Epoch [29/100], Step [500/500] Loss: 0.1942\n",
      "Epoch [30/100], Step [100/500] Loss: 0.1521\n",
      "Epoch [30/100], Step [200/500] Loss: 0.3360\n",
      "Epoch [30/100], Step [300/500] Loss: 0.3144\n",
      "Epoch [30/100], Step [400/500] Loss: 0.2071\n",
      "Epoch [30/100], Step [500/500] Loss: 0.2621\n",
      "Epoch [31/100], Step [100/500] Loss: 0.2640\n",
      "Epoch [31/100], Step [200/500] Loss: 0.3316\n",
      "Epoch [31/100], Step [300/500] Loss: 0.2044\n",
      "Epoch [31/100], Step [400/500] Loss: 0.2739\n",
      "Epoch [31/100], Step [500/500] Loss: 0.2513\n",
      "Epoch [32/100], Step [100/500] Loss: 0.2042\n",
      "Epoch [32/100], Step [200/500] Loss: 0.2971\n",
      "Epoch [32/100], Step [300/500] Loss: 0.2405\n",
      "Epoch [32/100], Step [400/500] Loss: 0.3161\n",
      "Epoch [32/100], Step [500/500] Loss: 0.1832\n",
      "Epoch [33/100], Step [100/500] Loss: 0.1310\n",
      "Epoch [33/100], Step [200/500] Loss: 0.2500\n",
      "Epoch [33/100], Step [300/500] Loss: 0.2506\n",
      "Epoch [33/100], Step [400/500] Loss: 0.2795\n",
      "Epoch [33/100], Step [500/500] Loss: 0.1556\n",
      "Epoch [34/100], Step [100/500] Loss: 0.1862\n",
      "Epoch [34/100], Step [200/500] Loss: 0.2796\n",
      "Epoch [34/100], Step [300/500] Loss: 0.2662\n",
      "Epoch [34/100], Step [400/500] Loss: 0.2517\n",
      "Epoch [34/100], Step [500/500] Loss: 0.2670\n",
      "Epoch [35/100], Step [100/500] Loss: 0.2506\n",
      "Epoch [35/100], Step [200/500] Loss: 0.3096\n",
      "Epoch [35/100], Step [300/500] Loss: 0.2839\n",
      "Epoch [35/100], Step [400/500] Loss: 0.1864\n",
      "Epoch [35/100], Step [500/500] Loss: 0.2236\n",
      "Epoch [36/100], Step [100/500] Loss: 0.2272\n",
      "Epoch [36/100], Step [200/500] Loss: 0.1953\n",
      "Epoch [36/100], Step [300/500] Loss: 0.2521\n",
      "Epoch [36/100], Step [400/500] Loss: 0.1615\n",
      "Epoch [36/100], Step [500/500] Loss: 0.1939\n",
      "Epoch [37/100], Step [100/500] Loss: 0.2569\n",
      "Epoch [37/100], Step [200/500] Loss: 0.1378\n",
      "Epoch [37/100], Step [300/500] Loss: 0.3271\n",
      "Epoch [37/100], Step [400/500] Loss: 0.2212\n",
      "Epoch [37/100], Step [500/500] Loss: 0.3812\n",
      "Epoch [38/100], Step [100/500] Loss: 0.2159\n",
      "Epoch [38/100], Step [200/500] Loss: 0.2113\n",
      "Epoch [38/100], Step [300/500] Loss: 0.2615\n",
      "Epoch [38/100], Step [400/500] Loss: 0.2425\n",
      "Epoch [38/100], Step [500/500] Loss: 0.2152\n",
      "Epoch [39/100], Step [100/500] Loss: 0.2511\n",
      "Epoch [39/100], Step [200/500] Loss: 0.2632\n",
      "Epoch [39/100], Step [300/500] Loss: 0.2291\n",
      "Epoch [39/100], Step [400/500] Loss: 0.2643\n",
      "Epoch [39/100], Step [500/500] Loss: 0.1711\n",
      "The new learning rate is 0.00025\n",
      "Epoch [40/100], Step [100/500] Loss: 0.2710\n",
      "Epoch [40/100], Step [200/500] Loss: 0.2965\n",
      "Epoch [40/100], Step [300/500] Loss: 0.1639\n",
      "Epoch [40/100], Step [400/500] Loss: 0.1772\n",
      "Epoch [40/100], Step [500/500] Loss: 0.2510\n",
      "Epoch [41/100], Step [100/500] Loss: 0.0840\n",
      "Epoch [41/100], Step [200/500] Loss: 0.2087\n",
      "Epoch [41/100], Step [300/500] Loss: 0.2131\n",
      "Epoch [41/100], Step [400/500] Loss: 0.1778\n",
      "Epoch [41/100], Step [500/500] Loss: 0.1743\n",
      "Epoch [42/100], Step [100/500] Loss: 0.1177\n",
      "Epoch [42/100], Step [200/500] Loss: 0.2253\n",
      "Epoch [42/100], Step [300/500] Loss: 0.2994\n",
      "Epoch [42/100], Step [400/500] Loss: 0.2278\n",
      "Epoch [42/100], Step [500/500] Loss: 0.2923\n",
      "Epoch [43/100], Step [100/500] Loss: 0.2048\n",
      "Epoch [43/100], Step [200/500] Loss: 0.1029\n",
      "Epoch [43/100], Step [300/500] Loss: 0.2560\n",
      "Epoch [43/100], Step [400/500] Loss: 0.1778\n",
      "Epoch [43/100], Step [500/500] Loss: 0.2186\n",
      "Epoch [44/100], Step [100/500] Loss: 0.1695\n",
      "Epoch [44/100], Step [200/500] Loss: 0.2375\n",
      "Epoch [44/100], Step [300/500] Loss: 0.2192\n",
      "Epoch [44/100], Step [400/500] Loss: 0.1595\n",
      "Epoch [44/100], Step [500/500] Loss: 0.1323\n",
      "Epoch [45/100], Step [100/500] Loss: 0.1691\n",
      "Epoch [45/100], Step [200/500] Loss: 0.1652\n",
      "Epoch [45/100], Step [300/500] Loss: 0.1247\n",
      "Epoch [45/100], Step [400/500] Loss: 0.1896\n",
      "Epoch [45/100], Step [500/500] Loss: 0.2013\n",
      "Epoch [46/100], Step [100/500] Loss: 0.1607\n",
      "Epoch [46/100], Step [200/500] Loss: 0.1515\n",
      "Epoch [46/100], Step [300/500] Loss: 0.1255\n",
      "Epoch [46/100], Step [400/500] Loss: 0.2889\n",
      "Epoch [46/100], Step [500/500] Loss: 0.2239\n",
      "Epoch [47/100], Step [100/500] Loss: 0.1199\n",
      "Epoch [47/100], Step [200/500] Loss: 0.2068\n",
      "Epoch [47/100], Step [300/500] Loss: 0.1994\n",
      "Epoch [47/100], Step [400/500] Loss: 0.1491\n",
      "Epoch [47/100], Step [500/500] Loss: 0.1664\n",
      "Epoch [48/100], Step [100/500] Loss: 0.1915\n",
      "Epoch [48/100], Step [200/500] Loss: 0.1487\n",
      "Epoch [48/100], Step [300/500] Loss: 0.0964\n",
      "Epoch [48/100], Step [400/500] Loss: 0.1623\n",
      "Epoch [48/100], Step [500/500] Loss: 0.2181\n",
      "Epoch [49/100], Step [100/500] Loss: 0.2247\n",
      "Epoch [49/100], Step [200/500] Loss: 0.0438\n",
      "Epoch [49/100], Step [300/500] Loss: 0.0988\n",
      "Epoch [49/100], Step [400/500] Loss: 0.2092\n",
      "Epoch [49/100], Step [500/500] Loss: 0.1942\n",
      "Epoch [50/100], Step [100/500] Loss: 0.1124\n",
      "Epoch [50/100], Step [200/500] Loss: 0.1271\n",
      "Epoch [50/100], Step [300/500] Loss: 0.1457\n",
      "Epoch [50/100], Step [400/500] Loss: 0.2972\n",
      "Epoch [50/100], Step [500/500] Loss: 0.2174\n",
      "Epoch [51/100], Step [100/500] Loss: 0.2154\n",
      "Epoch [51/100], Step [200/500] Loss: 0.2365\n",
      "Epoch [51/100], Step [300/500] Loss: 0.1886\n",
      "Epoch [51/100], Step [400/500] Loss: 0.0893\n",
      "Epoch [51/100], Step [500/500] Loss: 0.1766\n",
      "Epoch [52/100], Step [100/500] Loss: 0.1798\n",
      "Epoch [52/100], Step [200/500] Loss: 0.2064\n",
      "Epoch [52/100], Step [300/500] Loss: 0.1741\n",
      "Epoch [52/100], Step [400/500] Loss: 0.2405\n",
      "Epoch [52/100], Step [500/500] Loss: 0.2776\n",
      "Epoch [53/100], Step [100/500] Loss: 0.1247\n",
      "Epoch [53/100], Step [200/500] Loss: 0.2524\n",
      "Epoch [53/100], Step [300/500] Loss: 0.0533\n",
      "Epoch [53/100], Step [400/500] Loss: 0.2085\n",
      "Epoch [53/100], Step [500/500] Loss: 0.1659\n",
      "Epoch [54/100], Step [100/500] Loss: 0.2641\n",
      "Epoch [54/100], Step [200/500] Loss: 0.1613\n",
      "Epoch [54/100], Step [300/500] Loss: 0.1191\n",
      "Epoch [54/100], Step [400/500] Loss: 0.2167\n",
      "Epoch [54/100], Step [500/500] Loss: 0.0789\n",
      "Epoch [55/100], Step [100/500] Loss: 0.1117\n",
      "Epoch [55/100], Step [200/500] Loss: 0.1196\n",
      "Epoch [55/100], Step [300/500] Loss: 0.1802\n",
      "Epoch [55/100], Step [400/500] Loss: 0.1624\n",
      "Epoch [55/100], Step [500/500] Loss: 0.1037\n",
      "Epoch [56/100], Step [100/500] Loss: 0.1320\n",
      "Epoch [56/100], Step [200/500] Loss: 0.1224\n",
      "Epoch [56/100], Step [300/500] Loss: 0.1293\n",
      "Epoch [56/100], Step [400/500] Loss: 0.1645\n",
      "Epoch [56/100], Step [500/500] Loss: 0.2101\n",
      "Epoch [57/100], Step [100/500] Loss: 0.1747\n",
      "Epoch [57/100], Step [200/500] Loss: 0.1740\n",
      "Epoch [57/100], Step [300/500] Loss: 0.3265\n",
      "Epoch [57/100], Step [400/500] Loss: 0.2263\n",
      "Epoch [57/100], Step [500/500] Loss: 0.1512\n",
      "Epoch [58/100], Step [100/500] Loss: 0.1032\n",
      "Epoch [58/100], Step [200/500] Loss: 0.1110\n",
      "Epoch [58/100], Step [300/500] Loss: 0.1216\n",
      "Epoch [58/100], Step [400/500] Loss: 0.2709\n",
      "Epoch [58/100], Step [500/500] Loss: 0.2912\n",
      "Epoch [59/100], Step [100/500] Loss: 0.1266\n",
      "Epoch [59/100], Step [200/500] Loss: 0.0891\n",
      "Epoch [59/100], Step [300/500] Loss: 0.1998\n",
      "Epoch [59/100], Step [400/500] Loss: 0.1449\n",
      "Epoch [59/100], Step [500/500] Loss: 0.1320\n",
      "The new learning rate is 0.000125\n",
      "Epoch [60/100], Step [100/500] Loss: 0.1408\n",
      "Epoch [60/100], Step [200/500] Loss: 0.2276\n",
      "Epoch [60/100], Step [300/500] Loss: 0.0957\n",
      "Epoch [60/100], Step [400/500] Loss: 0.1779\n",
      "Epoch [60/100], Step [500/500] Loss: 0.1222\n",
      "Epoch [61/100], Step [100/500] Loss: 0.2341\n",
      "Epoch [61/100], Step [200/500] Loss: 0.0978\n",
      "Epoch [61/100], Step [300/500] Loss: 0.2084\n",
      "Epoch [61/100], Step [400/500] Loss: 0.1285\n",
      "Epoch [61/100], Step [500/500] Loss: 0.1565\n",
      "Epoch [62/100], Step [100/500] Loss: 0.1731\n",
      "Epoch [62/100], Step [200/500] Loss: 0.1676\n",
      "Epoch [62/100], Step [300/500] Loss: 0.1226\n",
      "Epoch [62/100], Step [400/500] Loss: 0.2643\n",
      "Epoch [62/100], Step [500/500] Loss: 0.1434\n",
      "Epoch [63/100], Step [100/500] Loss: 0.1914\n",
      "Epoch [63/100], Step [200/500] Loss: 0.0856\n",
      "Epoch [63/100], Step [300/500] Loss: 0.0906\n",
      "Epoch [63/100], Step [400/500] Loss: 0.1264\n",
      "Epoch [63/100], Step [500/500] Loss: 0.1056\n",
      "Epoch [64/100], Step [100/500] Loss: 0.1241\n",
      "Epoch [64/100], Step [200/500] Loss: 0.1002\n",
      "Epoch [64/100], Step [300/500] Loss: 0.1652\n",
      "Epoch [64/100], Step [400/500] Loss: 0.1358\n",
      "Epoch [64/100], Step [500/500] Loss: 0.1353\n",
      "Epoch [65/100], Step [100/500] Loss: 0.0854\n",
      "Epoch [65/100], Step [200/500] Loss: 0.1372\n",
      "Epoch [65/100], Step [300/500] Loss: 0.0833\n",
      "Epoch [65/100], Step [400/500] Loss: 0.0407\n",
      "Epoch [65/100], Step [500/500] Loss: 0.0813\n",
      "Epoch [66/100], Step [100/500] Loss: 0.1053\n",
      "Epoch [66/100], Step [200/500] Loss: 0.0822\n",
      "Epoch [66/100], Step [300/500] Loss: 0.1114\n",
      "Epoch [66/100], Step [400/500] Loss: 0.1054\n",
      "Epoch [66/100], Step [500/500] Loss: 0.2252\n",
      "Epoch [67/100], Step [100/500] Loss: 0.0870\n",
      "Epoch [67/100], Step [200/500] Loss: 0.1802\n",
      "Epoch [67/100], Step [300/500] Loss: 0.1604\n",
      "Epoch [67/100], Step [400/500] Loss: 0.1139\n",
      "Epoch [67/100], Step [500/500] Loss: 0.0960\n",
      "Epoch [68/100], Step [100/500] Loss: 0.0722\n",
      "Epoch [68/100], Step [200/500] Loss: 0.2496\n",
      "Epoch [68/100], Step [300/500] Loss: 0.1579\n",
      "Epoch [68/100], Step [400/500] Loss: 0.1262\n",
      "Epoch [68/100], Step [500/500] Loss: 0.0323\n",
      "Epoch [69/100], Step [100/500] Loss: 0.0908\n",
      "Epoch [69/100], Step [200/500] Loss: 0.1895\n",
      "Epoch [69/100], Step [300/500] Loss: 0.2113\n",
      "Epoch [69/100], Step [400/500] Loss: 0.1922\n",
      "Epoch [69/100], Step [500/500] Loss: 0.1158\n",
      "Epoch [70/100], Step [100/500] Loss: 0.1548\n",
      "Epoch [70/100], Step [200/500] Loss: 0.0765\n",
      "Epoch [70/100], Step [300/500] Loss: 0.1942\n",
      "Epoch [70/100], Step [400/500] Loss: 0.0944\n",
      "Epoch [70/100], Step [500/500] Loss: 0.1501\n",
      "Epoch [71/100], Step [100/500] Loss: 0.0952\n",
      "Epoch [71/100], Step [200/500] Loss: 0.2183\n",
      "Epoch [71/100], Step [300/500] Loss: 0.1045\n",
      "Epoch [71/100], Step [400/500] Loss: 0.1328\n",
      "Epoch [71/100], Step [500/500] Loss: 0.1453\n",
      "Epoch [72/100], Step [100/500] Loss: 0.1616\n",
      "Epoch [72/100], Step [200/500] Loss: 0.1188\n",
      "Epoch [72/100], Step [300/500] Loss: 0.0978\n",
      "Epoch [72/100], Step [400/500] Loss: 0.1684\n",
      "Epoch [72/100], Step [500/500] Loss: 0.1570\n",
      "Epoch [73/100], Step [100/500] Loss: 0.1096\n",
      "Epoch [73/100], Step [200/500] Loss: 0.1001\n",
      "Epoch [73/100], Step [300/500] Loss: 0.1823\n",
      "Epoch [73/100], Step [400/500] Loss: 0.2789\n",
      "Epoch [73/100], Step [500/500] Loss: 0.0908\n",
      "Epoch [74/100], Step [100/500] Loss: 0.1205\n",
      "Epoch [74/100], Step [200/500] Loss: 0.0563\n",
      "Epoch [74/100], Step [300/500] Loss: 0.1169\n",
      "Epoch [74/100], Step [400/500] Loss: 0.1117\n",
      "Epoch [74/100], Step [500/500] Loss: 0.1369\n",
      "Epoch [75/100], Step [100/500] Loss: 0.0459\n",
      "Epoch [75/100], Step [200/500] Loss: 0.1309\n",
      "Epoch [75/100], Step [300/500] Loss: 0.1635\n",
      "Epoch [75/100], Step [400/500] Loss: 0.1088\n",
      "Epoch [75/100], Step [500/500] Loss: 0.0906\n",
      "Epoch [76/100], Step [100/500] Loss: 0.1011\n",
      "Epoch [76/100], Step [200/500] Loss: 0.1302\n",
      "Epoch [76/100], Step [300/500] Loss: 0.1493\n",
      "Epoch [76/100], Step [400/500] Loss: 0.0347\n",
      "Epoch [76/100], Step [500/500] Loss: 0.0487\n",
      "Epoch [77/100], Step [100/500] Loss: 0.1125\n",
      "Epoch [77/100], Step [200/500] Loss: 0.1847\n",
      "Epoch [77/100], Step [300/500] Loss: 0.1583\n",
      "Epoch [77/100], Step [400/500] Loss: 0.1109\n",
      "Epoch [77/100], Step [500/500] Loss: 0.1976\n",
      "Epoch [78/100], Step [100/500] Loss: 0.1466\n",
      "Epoch [78/100], Step [200/500] Loss: 0.0797\n",
      "Epoch [78/100], Step [300/500] Loss: 0.0925\n",
      "Epoch [78/100], Step [400/500] Loss: 0.1703\n",
      "Epoch [78/100], Step [500/500] Loss: 0.1168\n",
      "Epoch [79/100], Step [100/500] Loss: 0.0650\n",
      "Epoch [79/100], Step [200/500] Loss: 0.1142\n",
      "Epoch [79/100], Step [300/500] Loss: 0.1478\n",
      "Epoch [79/100], Step [400/500] Loss: 0.1123\n",
      "Epoch [79/100], Step [500/500] Loss: 0.0402\n",
      "The new learning rate is 6.25e-05\n",
      "Epoch [80/100], Step [100/500] Loss: 0.1483\n",
      "Epoch [80/100], Step [200/500] Loss: 0.1998\n",
      "Epoch [80/100], Step [300/500] Loss: 0.2000\n",
      "Epoch [80/100], Step [400/500] Loss: 0.0671\n",
      "Epoch [80/100], Step [500/500] Loss: 0.0762\n",
      "Epoch [81/100], Step [100/500] Loss: 0.0397\n",
      "Epoch [81/100], Step [200/500] Loss: 0.1240\n",
      "Epoch [81/100], Step [300/500] Loss: 0.0792\n",
      "Epoch [81/100], Step [400/500] Loss: 0.1491\n",
      "Epoch [81/100], Step [500/500] Loss: 0.0940\n",
      "Epoch [82/100], Step [100/500] Loss: 0.1841\n",
      "Epoch [82/100], Step [200/500] Loss: 0.0928\n",
      "Epoch [82/100], Step [300/500] Loss: 0.1173\n",
      "Epoch [82/100], Step [400/500] Loss: 0.1199\n",
      "Epoch [82/100], Step [500/500] Loss: 0.0618\n",
      "Epoch [83/100], Step [100/500] Loss: 0.1201\n",
      "Epoch [83/100], Step [200/500] Loss: 0.1026\n",
      "Epoch [83/100], Step [300/500] Loss: 0.0843\n",
      "Epoch [83/100], Step [400/500] Loss: 0.0350\n",
      "Epoch [83/100], Step [500/500] Loss: 0.0946\n",
      "Epoch [84/100], Step [100/500] Loss: 0.1410\n",
      "Epoch [84/100], Step [200/500] Loss: 0.0915\n",
      "Epoch [84/100], Step [300/500] Loss: 0.1401\n",
      "Epoch [84/100], Step [400/500] Loss: 0.1089\n",
      "Epoch [84/100], Step [500/500] Loss: 0.0780\n",
      "Epoch [85/100], Step [100/500] Loss: 0.1059\n",
      "Epoch [85/100], Step [200/500] Loss: 0.1276\n",
      "Epoch [85/100], Step [300/500] Loss: 0.1587\n",
      "Epoch [85/100], Step [400/500] Loss: 0.1516\n",
      "Epoch [85/100], Step [500/500] Loss: 0.0574\n",
      "Epoch [86/100], Step [100/500] Loss: 0.1168\n",
      "Epoch [86/100], Step [200/500] Loss: 0.1508\n",
      "Epoch [86/100], Step [300/500] Loss: 0.0700\n",
      "Epoch [86/100], Step [400/500] Loss: 0.0728\n",
      "Epoch [86/100], Step [500/500] Loss: 0.0606\n",
      "Epoch [87/100], Step [100/500] Loss: 0.0647\n",
      "Epoch [87/100], Step [200/500] Loss: 0.1938\n",
      "Epoch [87/100], Step [300/500] Loss: 0.1335\n",
      "Epoch [87/100], Step [400/500] Loss: 0.0883\n",
      "Epoch [87/100], Step [500/500] Loss: 0.0724\n",
      "Epoch [88/100], Step [100/500] Loss: 0.0939\n",
      "Epoch [88/100], Step [200/500] Loss: 0.1382\n",
      "Epoch [88/100], Step [300/500] Loss: 0.1702\n",
      "Epoch [88/100], Step [400/500] Loss: 0.1006\n",
      "Epoch [88/100], Step [500/500] Loss: 0.2153\n",
      "Epoch [89/100], Step [100/500] Loss: 0.1471\n",
      "Epoch [89/100], Step [200/500] Loss: 0.1466\n",
      "Epoch [89/100], Step [300/500] Loss: 0.1104\n",
      "Epoch [89/100], Step [400/500] Loss: 0.1158\n",
      "Epoch [89/100], Step [500/500] Loss: 0.0736\n",
      "Epoch [90/100], Step [100/500] Loss: 0.0935\n",
      "Epoch [90/100], Step [200/500] Loss: 0.0618\n",
      "Epoch [90/100], Step [300/500] Loss: 0.1155\n",
      "Epoch [90/100], Step [400/500] Loss: 0.0558\n",
      "Epoch [90/100], Step [500/500] Loss: 0.0548\n",
      "Epoch [91/100], Step [100/500] Loss: 0.0991\n",
      "Epoch [91/100], Step [200/500] Loss: 0.0655\n",
      "Epoch [91/100], Step [300/500] Loss: 0.2166\n",
      "Epoch [91/100], Step [400/500] Loss: 0.1382\n",
      "Epoch [91/100], Step [500/500] Loss: 0.1663\n",
      "Epoch [92/100], Step [100/500] Loss: 0.0219\n",
      "Epoch [92/100], Step [200/500] Loss: 0.0929\n",
      "Epoch [92/100], Step [300/500] Loss: 0.0959\n",
      "Epoch [92/100], Step [400/500] Loss: 0.1687\n",
      "Epoch [92/100], Step [500/500] Loss: 0.1342\n",
      "Epoch [93/100], Step [100/500] Loss: 0.0729\n",
      "Epoch [93/100], Step [200/500] Loss: 0.0671\n",
      "Epoch [93/100], Step [300/500] Loss: 0.0827\n",
      "Epoch [93/100], Step [400/500] Loss: 0.1615\n",
      "Epoch [93/100], Step [500/500] Loss: 0.0791\n",
      "Epoch [94/100], Step [100/500] Loss: 0.1551\n",
      "Epoch [94/100], Step [200/500] Loss: 0.1247\n",
      "Epoch [94/100], Step [300/500] Loss: 0.0758\n",
      "Epoch [94/100], Step [400/500] Loss: 0.0963\n",
      "Epoch [94/100], Step [500/500] Loss: 0.1592\n",
      "Epoch [95/100], Step [100/500] Loss: 0.1206\n",
      "Epoch [95/100], Step [200/500] Loss: 0.1166\n",
      "Epoch [95/100], Step [300/500] Loss: 0.0649\n",
      "Epoch [95/100], Step [400/500] Loss: 0.0504\n",
      "Epoch [95/100], Step [500/500] Loss: 0.0683\n",
      "Epoch [96/100], Step [100/500] Loss: 0.1323\n",
      "Epoch [96/100], Step [200/500] Loss: 0.0842\n",
      "Epoch [96/100], Step [300/500] Loss: 0.0986\n",
      "Epoch [96/100], Step [400/500] Loss: 0.1329\n",
      "Epoch [96/100], Step [500/500] Loss: 0.0422\n",
      "Epoch [97/100], Step [100/500] Loss: 0.0728\n",
      "Epoch [97/100], Step [200/500] Loss: 0.0410\n",
      "Epoch [97/100], Step [300/500] Loss: 0.1225\n",
      "Epoch [97/100], Step [400/500] Loss: 0.1961\n",
      "Epoch [97/100], Step [500/500] Loss: 0.1190\n",
      "Epoch [98/100], Step [100/500] Loss: 0.0833\n",
      "Epoch [98/100], Step [200/500] Loss: 0.1782\n",
      "Epoch [98/100], Step [300/500] Loss: 0.0917\n",
      "Epoch [98/100], Step [400/500] Loss: 0.0815\n",
      "Epoch [98/100], Step [500/500] Loss: 0.1282\n",
      "Epoch [99/100], Step [100/500] Loss: 0.0599\n",
      "Epoch [99/100], Step [200/500] Loss: 0.1427\n",
      "Epoch [99/100], Step [300/500] Loss: 0.1380\n",
      "Epoch [99/100], Step [400/500] Loss: 0.0949\n",
      "Epoch [99/100], Step [500/500] Loss: 0.0583\n",
      "The new learning rate is 3.125e-05\n",
      "Epoch [100/100], Step [100/500] Loss: 0.1213\n",
      "Epoch [100/100], Step [200/500] Loss: 0.1220\n",
      "Epoch [100/100], Step [300/500] Loss: 0.1067\n",
      "Epoch [100/100], Step [400/500] Loss: 0.1006\n",
      "Epoch [100/100], Step [500/500] Loss: 0.0611\n"
     ]
    }
   ],
   "source": [
    "decay = 0\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Decay the learning rate every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        decay+=1\n",
    "        optimizer.param_groups[0]['lr'] = learning_rate * (0.5**decay)\n",
    "        print(\"The new learning rate is {}\".format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T10:13:47.018997400Z",
     "start_time": "2023-06-17T07:26:30.728999900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 88.38%\n"
     ]
    }
   ],
   "source": [
    "# TEst the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted ==labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: {}%'.format(100*correct/total))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T10:13:53.881551600Z",
     "start_time": "2023-06-17T10:13:47.039729200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../Trained_models/Rnn-SIFAR10 model/rnn_sifar_10')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T06:06:04.802718400Z",
     "start_time": "2023-06-18T06:06:04.762655400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T10:13:53.929067300Z",
     "start_time": "2023-06-17T10:13:53.899067700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
